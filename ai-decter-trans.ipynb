{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T17:02:44.463591Z",
     "iopub.status.busy": "2025-07-13T17:02:44.463299Z",
     "iopub.status.idle": "2025-07-13T17:04:03.963591Z",
     "shell.execute_reply": "2025-07-13T17:04:03.962909Z",
     "shell.execute_reply.started": "2025-07-13T17:02:44.463569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt_tab: <urlopen error [SSL:\n",
      "[nltk_data]     UNEXPECTED_EOF_WHILE_READING] EOF occurred in\n",
      "[nltk_data]     violation of protocol (_ssl.c:1000)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK data download completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced AI Text Detection Pipeline\n",
    "-----------------------------------\n",
    "核心改进:\n",
    "1. 更好的LLM选择（T5/GPT-2用于重写）\n",
    "2. 多维度特征提取\n",
    "3. 更智能的扰动策略\n",
    "4. 集成学习方法\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 尝试导入，如果失败则提示安装\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI library imports\n",
    "import asyncio\n",
    "import openai  # Good for catching error types like openai.RateLimitError\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import tiktoken\n",
    "\n",
    "# 基础库\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# 可视化库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, time, hashlib, json, random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 文本处理\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 文本扰动库\n",
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "except ImportError:\n",
    "    print(\"nlpaug not found, installing...\")\n",
    "    os.system(\"pip install nlpaug\")\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Transformers 和 sentence-transformers\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# NLTK数据下载\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading required NLTK data...\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "print(\"NLTK data download completed.\")\n",
    "import logging\n",
    "\n",
    "# 配置日志\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DetectionConfig:\n",
    "    \"\"\"检测配置类\"\"\"\n",
    "\n",
    "    # 模型选择\n",
    "    revision_model: str = \"t5-small\"\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    api_key: Optional[str] = field(default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"))\n",
    "    base_url: Optional[str] = field(\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_BASE_URL\")\n",
    "    )\n",
    "\n",
    "    # 扰动参数\n",
    "    perturbation_rate: float = 0.15\n",
    "    # 【修正】使用field(default_factory=...)来避免可变默认参数问题\n",
    "    perturbation_methods: List[str] = field(\n",
    "        default_factory=lambda: [\"synonym\", \"contextual\"]\n",
    "    )\n",
    "\n",
    "    # 检测参数\n",
    "    similarity_threshold: float = 0.95  # 建议使用0.85作为更稳健的默认值\n",
    "    use_ml_classifier: bool = True\n",
    "\n",
    "    # 批处理\n",
    "    batch_size: int = 16\n",
    "    max_length: int = 512\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.perturbation_methods is None:\n",
    "            self.perturbation_methods = [\"synonym\", \"contextual\", \"backtranslation\"]\n",
    "\n",
    "\n",
    "class EnhancedTextPerturber:\n",
    "    \"\"\"增强的文本扰动器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self._init_augmenters()\n",
    "\n",
    "    def _init_augmenters(self):\n",
    "        \"\"\"初始化各种扰动器\"\"\"\n",
    "        self.augmenters = {\n",
    "            \"synonym\": naw.SynonymAug(\n",
    "                aug_src=\"wordnet\", aug_p=self.config.perturbation_rate\n",
    "            ),\n",
    "            \"contextual\": naw.ContextualWordEmbsAug(\n",
    "                model_path=\"distilbert-base-uncased\",\n",
    "                action=\"substitute\",\n",
    "                aug_p=self.config.perturbation_rate,\n",
    "            ),\n",
    "            \"random_swap\": naw.RandomWordAug(\n",
    "                action=\"swap\", aug_p=self.config.perturbation_rate\n",
    "            ),\n",
    "            \"spelling\": naw.SpellingAug(aug_p=self.config.perturbation_rate),\n",
    "        }\n",
    "\n",
    "    def perturb(self, text: str, method: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        智能扰动策略\n",
    "\n",
    "        Args:\n",
    "            text: 原始文本\n",
    "            method: 指定方法，None则随机选择\n",
    "        Returns:\n",
    "            扰动后的文本\n",
    "        \"\"\"\n",
    "        if method is None:\n",
    "            method = np.random.choice(self.config.perturbation_methods)\n",
    "\n",
    "        try:\n",
    "            if method == \"backtranslation\":\n",
    "                return self._backtranslate(text)\n",
    "            elif method in self.augmenters:\n",
    "                augmented = self.augmenters[method].augment(text)\n",
    "                return augmented[0] if isinstance(augmented, list) else augmented\n",
    "            else:\n",
    "                # 混合扰动\n",
    "                return self._mixed_perturbation(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Perturbation failed: {e}\")\n",
    "            return self._simple_perturb(text)\n",
    "\n",
    "    def _mixed_perturbation(self, text: str) -> str:\n",
    "        \"\"\"混合多种扰动方法\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        perturbed_sentences = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            # 随机选择扰动方法\n",
    "            method = np.random.choice(list(self.augmenters.keys()))\n",
    "            try:\n",
    "                perturbed = self.augmenters[method].augment(sent)\n",
    "                perturbed_sent = (\n",
    "                    perturbed[0] if isinstance(perturbed, list) else perturbed\n",
    "                )\n",
    "                perturbed_sentences.append(perturbed_sent)\n",
    "            except:\n",
    "                perturbed_sentences.append(sent)\n",
    "\n",
    "        return \" \".join(perturbed_sentences)\n",
    "\n",
    "    def _simple_perturb(self, text: str) -> str:\n",
    "        \"\"\"简单扰动作为后备\"\"\"\n",
    "        words = text.split()\n",
    "\n",
    "        # 随机替换15%的词\n",
    "        num_changes = max(1, int(len(words) * self.config.perturbation_rate))\n",
    "        indices = np.random.choice(\n",
    "            range(len(words)), size=min(num_changes, len(words)), replace=False\n",
    "        )\n",
    "\n",
    "        for idx in indices:\n",
    "            # 简单的字符级修改\n",
    "            word = words[idx]\n",
    "            if len(word) > 3:\n",
    "                words[idx] = word[:-1] + np.random.choice(list(\"aeiou\"))\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def _backtranslate(self, text: str) -> str:\n",
    "        \"\"\"反向翻译（需要翻译API）\"\"\"\n",
    "        # 这里只是示例，实际需要调用翻译API\n",
    "        return text\n",
    "\n",
    "\n",
    "class EnhancedLLMReviser:\n",
    "    \"\"\"增强的LLM重写器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.client = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._init_reviser()\n",
    "\n",
    "    def _init_model(self):\n",
    "        model_name = self.config.revision_model\n",
    "        \"\"\"初始化重写模型\"\"\"\n",
    "        if self.config.revision_model.startswith(\"t5\"):\n",
    "            # T5模型更适合文本重写任务\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.config.revision_model)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                self.config.revision_model\n",
    "            )\n",
    "        elif self.config.revision_model == \"gpt2\":\n",
    "            # GPT-2作为备选\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # 初始化OpenAI API客户端\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"请在DetectionConfig中提供api_key或设置OPENAI_API_KEY环境变量\"\n",
    "                )\n",
    "\n",
    "            self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\"\n",
    "            )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _init_reviser(self):\n",
    "        \"\"\"根据配置初始化重写模型或API客户端\"\"\"\n",
    "        model_name = self.config.revision_model\n",
    "\n",
    "        if model_name.startswith(\"t5\"):\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local T5 model loaded: {model_name}\")\n",
    "\n",
    "        elif model_name == \"gpt2\":\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local GPT-2 model loaded: {model_name}\")\n",
    "\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # 初始化OpenAI API客户端\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"请在DetectionConfig中提供api_key或设置OPENAI_API_KEY环境变量\"\n",
    "                )\n",
    "\n",
    "            # self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.client = AsyncOpenAI(\n",
    "                api_key=api_key, base_url=self.config.base_url\n",
    "            )  # Using Asynchronous Clients\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\"\n",
    "            )\n",
    "\n",
    "    # def revise(self, text: str, cache: Dict[str, str]) -> str:\n",
    "    async def revise(\n",
    "        self, original_text: str, perturbed_text: str, cache: Dict[str, str]\n",
    "    ) -> str:  # change to async def\n",
    "        \"\"\"\n",
    "        使用LLM重写文本\n",
    "\n",
    "        Args:\n",
    "            original_text: 待重写文本\n",
    "            cache: 缓存字典\n",
    "        Returns:\n",
    "            重写后的文本\n",
    "        \"\"\"\n",
    "        # 生成缓存键\n",
    "        cache_key = hashlib.md5(\n",
    "            f\"{original_text}_{self.config.revision_model}\".encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        # Check if the result for the original text is already in the cache\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "\n",
    "        # print(f\"\\norigin text: {original_text}\")\n",
    "\n",
    "        try:\n",
    "            model_name = self.config.revision_model\n",
    "            revised = \"\"\n",
    "            if model_name.startswith(\"t5\") or model_name == \"gpt2\":\n",
    "                if model_name.startswith(\"t5\"):\n",
    "                    revised = self._revise_with_t5(perturbed_text)\n",
    "                    # print(f\"t5 rewrite: {revised}\")\n",
    "                else:  # gpt2\n",
    "                    revised = self._revise_with_gpt2(perturbed_text)\n",
    "                    # print(f\"gpt2 rewrite: {revised}\")\n",
    "            elif model_name.startswith(\"gpt-\"):\n",
    "                # api rewrite\n",
    "                # revised = self._revise_with_api(perturbed_text)\n",
    "                revised = await self._revise_with_api(perturbed_text)  # change to await\n",
    "                # print(f\"gpt3.5 rewrite: {revised}\")\n",
    "            else:\n",
    "                revised = self._rule_based_revision(perturbed_text)\n",
    "\n",
    "            cache[cache_key] = revised\n",
    "            return revised\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Revision failed with model {self.config.revision_model}: {e}\")\n",
    "            fallback_revision = self._rule_based_revision(perturbed_text)\n",
    "            cache[cache_key] = fallback_revision\n",
    "            return fallback_revision\n",
    "\n",
    "    def _revise_with_t5(self, text: str) -> str:\n",
    "        \"\"\"使用T5模型重写\"\"\"\n",
    "        # T5需要特定的提示格式\n",
    "        prompt = f\"paraphrase: {text}\"\n",
    "\n",
    "        inputs = self.tokenizer.encode(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,  # 对输入进行截断的最大长度\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # 使用束搜索 (Beam Search) 以获得更高质量和更稳定的输出\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                # --- 解码策略修改 ---\n",
    "                num_beams=4,  # 修改：设置束的宽度，4或5是常用值\n",
    "                early_stopping=True,  # 新增：当所有束都完成时提前停止\n",
    "                # --- 长度控制优化 ---\n",
    "                max_length=256,  # 修改：为输出设置一个固定的最大长度\n",
    "                min_length=40,  # 新增：设置一个合理的最小长度，防止输出过短\n",
    "                # --- 质量优化 ---\n",
    "                repetition_penalty=1.2,  # 新增：轻微惩罚重复，提升多样性\n",
    "            )\n",
    "\n",
    "            # 注意：使用 beam search 时，不需要 do_sample, temperature, top_p\n",
    "\n",
    "        revised = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return revised\n",
    "\n",
    "    def _revise_with_gpt2(self, text: str) -> str:\n",
    "        \"\"\"使用GPT-2模型重写\"\"\"\n",
    "        try:\n",
    "            prompt_templates = [\n",
    "                f\"Paraphrase the following text while keeping the same meaning: {text}\\n\\nParaphrased version:\",\n",
    "                f\"Rewrite this sentence in a different way: {text}\\n\\nRewritten:\",\n",
    "                f\"Express this differently: {text}\\n\\nAlternative expression:\",\n",
    "            ]\n",
    "            prompt = np.random.choice(prompt_templates)\n",
    "            inputs = self.tokenizer.encode(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,  # Ensure padding is enabled to handle the mask\n",
    "            )\n",
    "            input_ids = inputs.input_ids.to(self.device)\n",
    "            attention_mask = inputs.attention_mask.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,  # Pass input_ids explicitly\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=min(len(input_ids[0]) + 100, 1024),\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    # early_stopping=True,\n",
    "                )\n",
    "\n",
    "            #\n",
    "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            rewritten = \"\"\n",
    "            for marker in [\n",
    "                \"Paraphrased version:\",\n",
    "                \"Rewritten:\",\n",
    "                \"Alternative expression:\",\n",
    "            ]:\n",
    "                if marker in full_text:\n",
    "                    rewritten = full_text.split(marker)[-1].strip()\n",
    "                    break\n",
    "\n",
    "            if not rewritten:\n",
    "                rewritten = full_text[len(prompt) :].strip()\n",
    "\n",
    "            if \"\\n\" in rewritten:\n",
    "                rewritten = rewritten.split(\"\\n\")[0]\n",
    "            if \".\" in rewritten:\n",
    "                rewritten = rewritten.split(\".\")[0] + \".\"\n",
    "\n",
    "            return rewritten if rewritten else text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  修复版重写失败: {e}\")\n",
    "            return text\n",
    "\n",
    "    # def _revise_with_api(self, text: str) -> str:\n",
    "    async def _revise_with_api(self, text: str) -> str:  # change to async def\n",
    "        \"\"\"Use OpenAI API with better prompting strategy\"\"\"\n",
    "        if not self.client:\n",
    "            raise ValueError(\"OpenAI client not initialized.\")\n",
    "\n",
    "        # Better prompt that encourages standardization\n",
    "        system_prompt = \"\"\"You are a professional editor. Your task is to rewrite the given text to make it more formal, standardized, and academic in style. \n",
    "\n",
    "Key guidelines:\n",
    "- Replace casual expressions with formal language\n",
    "- Use precise technical terms where appropriate\n",
    "- Maintain professional tone throughout\n",
    "- Standardize sentence structure\n",
    "- Remove colloquialisms and personal expressions\n",
    "- Keep the core meaning but express it in a more refined way\n",
    "\n",
    "The more casual or informal the original text, the more changes you should make.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Please rewrite this text in a formal, academic style:\\n\\n{text}\"\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = await self.client.chat.completions.create(  # change to await\n",
    "                    model=self.config.revision_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    ],\n",
    "                    temperature=0.3,  # Lower temperature for more consistent output\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"API error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    await asyncio.time.sleep(\n",
    "                        self.retry_delay\n",
    "                    )  # Using asynchronous sleep\n",
    "                else:\n",
    "                    return text\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _rule_based_revision(self, text: str) -> str:\n",
    "        \"\"\"基于规则的重写\"\"\"\n",
    "        import re\n",
    "\n",
    "        # 句子重排\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) > 2:\n",
    "            # 随机调整句子顺序（保持逻辑性）\n",
    "            sentences = self._reorder_sentences(sentences)\n",
    "\n",
    "        # 同义词替换\n",
    "        revised_sentences = []\n",
    "        for sent in sentences:\n",
    "            # 简单的同义词映射\n",
    "            synonyms = {\n",
    "                \"important\": \"significant\",\n",
    "                \"however\": \"nevertheless\",\n",
    "                \"therefore\": \"thus\",\n",
    "                \"because\": \"since\",\n",
    "                \"many\": \"numerous\",\n",
    "                \"show\": \"demonstrate\",\n",
    "            }\n",
    "\n",
    "            for word, syn in synonyms.items():\n",
    "                sent = re.sub(r\"\\b\" + word + r\"\\b\", syn, sent, flags=re.IGNORECASE)\n",
    "\n",
    "            revised_sentences.append(sent)\n",
    "\n",
    "        return \" \".join(revised_sentences)\n",
    "\n",
    "    def _reorder_sentences(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"智能句子重排\"\"\"\n",
    "        # 保持第一句和最后一句\n",
    "        if len(sentences) <= 2:\n",
    "            return sentences\n",
    "\n",
    "        first = sentences[0]\n",
    "        last = sentences[-1]\n",
    "        middle = sentences[1:-1]\n",
    "\n",
    "        # 打乱中间句子\n",
    "        np.random.shuffle(middle)\n",
    "\n",
    "        return [first] + middle + [last]\n",
    "\n",
    "\n",
    "class MultiFeatureExtractor:\n",
    "    \"\"\"多维度特征提取器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.sentence_model = SentenceTransformer(config.embedding_model)\n",
    "        self._init_linguistic_features()\n",
    "\n",
    "    def _init_linguistic_features(self):\n",
    "        \"\"\"初始化语言学特征提取\"\"\"\n",
    "        try:\n",
    "            nltk.download(\"punkt\", quiet=True)\n",
    "            nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "            nltk.download(\"stopwords\", quiet=True)\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "        except:\n",
    "            self.stop_words = set()\n",
    "\n",
    "    def extract_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        提取多维度特征\n",
    "\n",
    "        Returns:\n",
    "            特征字典\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 1. 语义相似度\n",
    "        features[\"semantic_similarity\"] = self._compute_semantic_similarity(\n",
    "            original, revised\n",
    "        )\n",
    "\n",
    "        # 2. 词汇级特征\n",
    "        features.update(self._extract_lexical_features(original, revised))\n",
    "\n",
    "        # 3. 句法特征\n",
    "        features.update(self._extract_syntactic_features(original, revised))\n",
    "\n",
    "        # 4. 风格特征\n",
    "        features.update(self._extract_stylistic_features(original, revised))\n",
    "\n",
    "        # 5. 编辑距离特征\n",
    "        features.update(self._extract_edit_features(original, revised))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _compute_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"计算语义相似度\"\"\"\n",
    "        emb1 = self.sentence_model.encode(text1)\n",
    "        emb2 = self.sentence_model.encode(text2)\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        return float(cosine_similarity([emb1], [emb2])[0, 0])\n",
    "\n",
    "    def _extract_lexical_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"提取词汇级特征\"\"\"\n",
    "        orig_words = set(word_tokenize(original.lower()))\n",
    "        rev_words = set(word_tokenize(revised.lower()))\n",
    "\n",
    "        # 词汇重叠率\n",
    "        overlap = len(orig_words & rev_words)\n",
    "        total = len(orig_words | rev_words)\n",
    "\n",
    "        features = {\n",
    "            \"word_overlap_ratio\": overlap / total if total > 0 else 0,\n",
    "            \"vocabulary_change_ratio\": (\n",
    "                len(orig_words ^ rev_words) / total if total > 0 else 0\n",
    "            ),\n",
    "            \"unique_words_ratio\": (\n",
    "                len(rev_words - orig_words) / len(rev_words) if rev_words else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # 停用词比例变化\n",
    "        orig_stop = len([w for w in orig_words if w in self.stop_words])\n",
    "        rev_stop = len([w for w in rev_words if w in self.stop_words])\n",
    "\n",
    "        features[\"stopword_ratio_change\"] = abs(\n",
    "            (orig_stop / len(orig_words) if orig_words else 0)\n",
    "            - (rev_stop / len(rev_words) if rev_words else 0)\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_syntactic_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"提取句法特征\"\"\"\n",
    "        orig_sents = sent_tokenize(original)\n",
    "        rev_sents = sent_tokenize(revised)\n",
    "\n",
    "        features = {\n",
    "            \"sentence_count_ratio\": (\n",
    "                len(rev_sents) / len(orig_sents) if orig_sents else 1\n",
    "            ),\n",
    "            \"avg_sentence_length_change\": (\n",
    "                abs(\n",
    "                    np.mean([len(word_tokenize(s)) for s in orig_sents])\n",
    "                    - np.mean([len(word_tokenize(s)) for s in rev_sents])\n",
    "                )\n",
    "                if orig_sents and rev_sents\n",
    "                else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # POS标签分布变化\n",
    "        try:\n",
    "            orig_pos = nltk.pos_tag(word_tokenize(original))\n",
    "            rev_pos = nltk.pos_tag(word_tokenize(revised))\n",
    "\n",
    "            # 计算主要词性比例变化\n",
    "            for pos_type in [\"NN\", \"VB\", \"JJ\", \"RB\"]:  # 名词、动词、形容词、副词\n",
    "                orig_count = sum(1 for _, pos in orig_pos if pos.startswith(pos_type))\n",
    "                rev_count = sum(1 for _, pos in rev_pos if pos.startswith(pos_type))\n",
    "\n",
    "                features[f\"pos_{pos_type}_ratio_change\"] = abs(\n",
    "                    (orig_count / len(orig_pos) if orig_pos else 0)\n",
    "                    - (rev_count / len(rev_pos) if rev_pos else 0)\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_stylistic_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"提取风格特征\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 标点符号使用变化\n",
    "        orig_punct = sum(1 for c in original if c in \".,!?;:\")\n",
    "        rev_punct = sum(1 for c in revised if c in \".,!?;:\")\n",
    "\n",
    "        features[\"punctuation_ratio_change\"] = abs(\n",
    "            (orig_punct / len(original) if original else 0)\n",
    "            - (rev_punct / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # 大写字母比例变化\n",
    "        orig_caps = sum(1 for c in original if c.isupper())\n",
    "        rev_caps = sum(1 for c in revised if c.isupper())\n",
    "\n",
    "        features[\"capitalization_ratio_change\"] = abs(\n",
    "            (orig_caps / len(original) if original else 0)\n",
    "            - (rev_caps / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # 平均词长变化\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "\n",
    "        features[\"avg_word_length_change\"] = (\n",
    "            abs(\n",
    "                np.mean([len(w) for w in orig_words])\n",
    "                - np.mean([len(w) for w in rev_words])\n",
    "            )\n",
    "            if orig_words and rev_words\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_edit_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"提取编辑距离相关特征\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "\n",
    "        # 字符级相似度\n",
    "        char_similarity = SequenceMatcher(None, original, revised).ratio()\n",
    "\n",
    "        # 词级相似度\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "        word_similarity = SequenceMatcher(None, orig_words, rev_words).ratio()\n",
    "\n",
    "        features = {\n",
    "            \"char_level_similarity\": char_similarity,\n",
    "            \"word_level_similarity\": word_similarity,\n",
    "            \"length_ratio\": len(revised) / len(original) if original else 1,\n",
    "        }\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class AITextDetector:\n",
    "    \"\"\"主检测器类\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig = None):\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.perturber = EnhancedTextPerturber(self.config)\n",
    "        self.reviser = EnhancedLLMReviser(self.config)\n",
    "        self.feature_extractor = MultiFeatureExtractor(self.config)\n",
    "        self.classifier = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cache = {}\n",
    "        self.api_concurrency_limit = asyncio.Semaphore(10)\n",
    "\n",
    "    # In the AITextDetector class\n",
    "\n",
    "    async def detect_batch(\n",
    "        self, texts: List[str], labels: Optional[List[int]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Asynchronously detects text in a batch, with robust caching and parallel API calls.\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        all_scores = []\n",
    "\n",
    "        if not texts:\n",
    "            # Handle the case where an empty list is passed\n",
    "            return {\n",
    "                \"predictions\": np.array([]),\n",
    "                \"probabilities\": np.array([]),\n",
    "                \"features\": np.array([]),\n",
    "                \"similarity_scores\": np.array([]),\n",
    "            }\n",
    "\n",
    "        print(\"Processing texts...\")\n",
    "\n",
    "        # Step 1: Perform synchronous, CPU-bound perturbation first.\n",
    "        perturbed_texts = [\n",
    "            self.perturber.perturb(text) for text in tqdm(texts, desc=\"Perturbing\")\n",
    "        ]\n",
    "\n",
    "        # Step 2: Define a helper function to manage semaphore and call the new revise method.\n",
    "        # This helper will be used to create our list of concurrent tasks.\n",
    "        async def revise_with_limit(original_text, perturbed_text):\n",
    "            async with self.api_concurrency_limit:\n",
    "                # Call the updated revise function with both original and perturbed text\n",
    "                return await self.reviser.revise(\n",
    "                    original_text, perturbed_text, self.cache\n",
    "                )\n",
    "\n",
    "        # Step 3: Create a list of NEW coroutine tasks.\n",
    "        # Each task is a call to our helper function with a pair of original and perturbed texts.\n",
    "        print(f\"Revising texts with {self.reviser.config.revision_model}...\")\n",
    "        revision_tasks = [\n",
    "            revise_with_limit(orig, pert) for orig, pert in zip(texts, perturbed_texts)\n",
    "        ]\n",
    "\n",
    "        # Step 4: Concurrently run all revision tasks using asyncio.gather.\n",
    "        # This preserves the order of the results and is wrapped in tqdm for a progress bar.\n",
    "        all_revised_texts = await asyncio.gather(\n",
    "            *tqdm(revision_tasks, desc=\"Revising\", total=len(revision_tasks))\n",
    "        )\n",
    "\n",
    "        # Step 5: Process the results. This part is synchronous again.\n",
    "        print(\"Extracting features...\")\n",
    "        for i, original_text in enumerate(texts):\n",
    "            # The order is preserved, so we can safely pair original and revised texts.\n",
    "            revised_text = all_revised_texts[i]\n",
    "            features = self.feature_extractor.extract_features(\n",
    "                original_text, revised_text\n",
    "            )\n",
    "            all_features.append(features)\n",
    "            all_scores.append(features.get(\"semantic_similarity\", 0.5))\n",
    "\n",
    "        # --- The rest of the function remains the same ---\n",
    "        feature_matrix = pd.DataFrame(all_features).fillna(0).values\n",
    "\n",
    "        if self.config.use_ml_classifier:\n",
    "            if labels is not None and self.classifier is None:\n",
    "                self._train_classifier(feature_matrix, labels)\n",
    "\n",
    "            if self.classifier is not None:\n",
    "                feature_matrix_scaled = self.scaler.transform(feature_matrix)\n",
    "                predictions = self.classifier.predict(feature_matrix_scaled)\n",
    "                probabilities = self.classifier.predict_proba(feature_matrix_scaled)[\n",
    "                    :, 1\n",
    "                ]\n",
    "            else:\n",
    "                # Fallback if classifier isn't trained\n",
    "                predictions = (\n",
    "                    np.array(all_scores) > self.config.similarity_threshold\n",
    "                ).astype(int)\n",
    "                probabilities = np.array(all_scores)\n",
    "        else:\n",
    "            # Logic for when the ML classifier is turned off\n",
    "            predictions = (\n",
    "                np.array(all_scores) > self.config.similarity_threshold\n",
    "            ).astype(int)\n",
    "            probabilities = np.array(all_scores)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": probabilities,\n",
    "            \"features\": feature_matrix,\n",
    "            \"similarity_scores\": np.array(all_scores),\n",
    "        }\n",
    "\n",
    "    def _train_classifier(self, features: np.ndarray, labels: List[int]):\n",
    "        \"\"\"训练机器学习分类器\"\"\"\n",
    "        print(\"Training classifier...\")\n",
    "\n",
    "        # 标准化特征\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "\n",
    "        # 使用随机森林\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        self.classifier.fit(features_scaled, labels)\n",
    "\n",
    "        # 特征重要性\n",
    "        feature_names = list(pd.DataFrame(features).columns)\n",
    "        importances = self.classifier.feature_importances_\n",
    "\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        for feat, imp in sorted(\n",
    "            zip(feature_names, importances), key=lambda x: x[1], reverse=True\n",
    "        )[:10]:\n",
    "            print(f\"  {feat}: {imp:.4f}\")\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        model_data = {\n",
    "            \"classifier\": self.classifier,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"config\": self.config,\n",
    "            \"cache\": self.cache,\n",
    "        }\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.classifier = model_data[\"classifier\"]\n",
    "        self.scaler = model_data[\"scaler\"]\n",
    "        self.config = model_data[\"config\"]\n",
    "        self.cache = model_data[\"cache\"]\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "# 数据集加载函数\n",
    "class CustomDataLoader:\n",
    "    \"\"\"数据加载器类\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"./data\"):\n",
    "        \"\"\"\n",
    "        初始化数据加载器\n",
    "\n",
    "        Args:\n",
    "            data_dir: 数据文件默认目录\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "            logger.info(f\"Created data directory: {data_dir}\")\n",
    "\n",
    "    def load_data(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        读取带有 'text' 与 'label' 列的数据集，并返回 DataFrame。\n",
    "        支持金融数据集格式和通用格式。\n",
    "\n",
    "        Args:\n",
    "            path: 数据文件路径、'finance' 表示使用金融数据集、'sample' 表示创建示例数据\n",
    "        Returns:\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "        \"\"\"\n",
    "        if path == \"finance\":\n",
    "            return self.load_finance_dataset()\n",
    "        if path == \"sample\":\n",
    "            return self.create_sample_dataset()\n",
    "        elif path == \"test\":\n",
    "            return self.load_origin_dataset()\n",
    "\n",
    "        # 如果是相对路径，添加数据目录前缀\n",
    "        if not os.path.isabs(path) and not os.path.exists(path):\n",
    "            path = os.path.join(self.data_dir, path)\n",
    "\n",
    "        try:\n",
    "            if path.endswith(\".csv\"):\n",
    "                df = pd.read_csv(path)\n",
    "            elif path.endswith(\".jsonl\"):\n",
    "                df = pd.read_json(path, lines=True)\n",
    "            elif path.endswith(\".json\"):\n",
    "                df = pd.read_json(path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {path}\")\n",
    "\n",
    "            # 确保有必要的列\n",
    "            if \"text\" not in df.columns:\n",
    "                raise ValueError(\"Missing 'text' column in dataset\")\n",
    "            if \"label\" not in df.columns:\n",
    "                raise ValueError(\"Missing 'label' column in dataset\")\n",
    "\n",
    "            # 如果没有id列，自动生成\n",
    "            if \"id\" not in df.columns:\n",
    "                df[\"id\"] = range(len(df))\n",
    "\n",
    "            # 数据清洗\n",
    "            df = df.dropna(subset=[\"text\", \"label\"])\n",
    "            df = df[df[\"text\"].str.len() > 10]  # 过滤太短的文本\n",
    "\n",
    "            logger.info(f\"Loaded {len(df)} samples from {path}\")\n",
    "            logger.info(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "            return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File {path} not found. Creating sample dataset...\")\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "    def find_finance_data_files(self) -> Dict[str, str]:\n",
    "        \"\"\"查找金融数据集文件\"\"\"\n",
    "        logger.info(\"Searching for finance data files...\")\n",
    "\n",
    "        # 本地环境路径\n",
    "        possible_paths = [\n",
    "            self.data_dir,\n",
    "            os.path.join(self.data_dir, \"finance\"),\n",
    "            os.path.join(self.data_dir, \"finance-dataset\"),\n",
    "            \"./finance_data\",\n",
    "            \".\",\n",
    "            \"./data\",\n",
    "        ]\n",
    "\n",
    "        files_found = {}\n",
    "\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                logger.debug(f\"Checking path: {path}\")\n",
    "                for root, dirs, files in os.walk(path):\n",
    "                    for file in files:\n",
    "                        if \"revised_human_finance\" in file or \"revised_human\" in file:\n",
    "                            files_found[\"revised_human\"] = os.path.join(root, file)\n",
    "                        elif (\n",
    "                            \"revised_chatgpt_finance\" in file\n",
    "                            or \"revised_chatgpt\" in file\n",
    "                        ):\n",
    "                            files_found[\"revised_chatgpt\"] = os.path.join(root, file)\n",
    "                        elif \"finance.jsonl\" in file:\n",
    "                            files_found[\"original\"] = os.path.join(root, file)\n",
    "\n",
    "        return files_found\n",
    "\n",
    "    def load_finance_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        加载金融数据集 - 使用修订后的文本作为数据\n",
    "        修复了多行JSON解析问题\n",
    "\n",
    "        Returns:\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "            其中text是修订后的文本，label: 0=human, 1=chatgpt\n",
    "        \"\"\"\n",
    "        files = self.find_finance_data_files()\n",
    "\n",
    "        logger.info(\"Loading finance datasets...\")\n",
    "        logger.info(f\"Found files: {list(files.keys())}\")\n",
    "\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        # 1. 加载修订后的人类文本（每行一个JSON对象）\n",
    "        if \"revised_human\" in files:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Loading revised human texts from: {files['revised_human']}\"\n",
    "                )\n",
    "                with open(files[\"revised_human\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line_no, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            try:\n",
    "                                # 每行是一个独立的JSON对象\n",
    "                                item = json.loads(line)\n",
    "                                for idx, text in item.items():\n",
    "                                    # 清理文本\n",
    "                                    cleaned_text = (\n",
    "                                        text.strip()\n",
    "                                        .replace(\"\\\\n\\\\n\", \" \")\n",
    "                                        .replace(\"\\\\n\", \" \")\n",
    "                                    )\n",
    "                                    if len(cleaned_text) > 20:\n",
    "                                        all_texts.append(cleaned_text)\n",
    "                                        all_labels.append(0)  # 0 = human\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                logger.warning(f\"  Line {line_no} parse error: {e}\")\n",
    "                logger.info(\n",
    "                    f\"  Loaded {len([l for l in all_labels if l == 0])} human texts\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading revised human texts: {e}\")\n",
    "\n",
    "        # 2. 加载修订后的ChatGPT文本（每行一个JSON对象）\n",
    "        if \"revised_chatgpt\" in files:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Loading revised ChatGPT texts from: {files['revised_chatgpt']}\"\n",
    "                )\n",
    "                with open(files[\"revised_chatgpt\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line_no, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            try:\n",
    "                                # 每行是一个独立的JSON对象\n",
    "                                item = json.loads(line)\n",
    "                                for idx, text in item.items():\n",
    "                                    # 清理文本\n",
    "                                    cleaned_text = (\n",
    "                                        text.strip()\n",
    "                                        .replace(\"\\\\n\\\\n\", \" \")\n",
    "                                        .replace(\"\\\\n\", \" \")\n",
    "                                    )\n",
    "                                    if len(cleaned_text) > 20:\n",
    "                                        all_texts.append(cleaned_text)\n",
    "                                        all_labels.append(1)  # 1 = chatgpt\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                logger.warning(f\"  Line {line_no} parse error: {e}\")\n",
    "                logger.info(\n",
    "                    f\"  Loaded {len([l for l in all_labels if l == 1])} ChatGPT texts\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading revised ChatGPT texts: {e}\")\n",
    "\n",
    "        # 4. 创建DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            {\"id\": range(len(all_texts)), \"text\": all_texts, \"label\": all_labels}\n",
    "        )\n",
    "\n",
    "        # 打乱数据\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        df[\"id\"] = range(len(df))\n",
    "\n",
    "        logger.info(f\"\\n=== Dataset Summary ===\")\n",
    "        logger.info(f\"Total samples: {len(df)}\")\n",
    "        logger.info(f\"Human texts: {len(df[df['label'] == 0])}\")\n",
    "        logger.info(f\"ChatGPT texts: {len(df[df['label'] == 1])}\")\n",
    "\n",
    "        # 显示示例\n",
    "        if len(df) > 0:\n",
    "            logger.info(\"\\nSample texts:\")\n",
    "            for label in [0, 1]:\n",
    "                label_name = \"Human\" if label == 0 else \"ChatGPT\"\n",
    "                samples = df[df[\"label\"] == label]\n",
    "                if len(samples) > 0:\n",
    "                    sample_text = samples.iloc[0][\"text\"]\n",
    "                    logger.info(f\"[{label_name}]: {sample_text[:100]}...\")\n",
    "\n",
    "        return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "    def create_sample_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        创建示例数据集用于测试\n",
    "\n",
    "        Returns:\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "        \"\"\"\n",
    "        sample_data = {\n",
    "            \"id\": range(6),\n",
    "            \"text\": [\n",
    "                \"There is most likely an error in the WSJ's data.  Yahoo! Finance reports the PE on the Russell 2000 to be 15 as of 83111 and SP 500 PE to be 13 (about the same as WSJ). Good catch, though!  E-mail WSJ, perhaps they will be grateful.\",\n",
    "                \"I know this question has a lot of answers already, but I feel the answers are phrased either strongly against, or mildly for, co-signing. What it amounts down to is that this is a personal choice. You cannot receive reliable information as to whether or not co-signing this loan is a good move due to lack of information. The person involved is going to know the person they would be co-signing for, and the people on this site will only have their own personal preferences of experiences to draw from. You know if they are reliable, if they will be able to pay off the loan without need for the banks to come after you.  This site can offer general theories, but I think it should be kept in mind that this is wholly a personal decision for the person involved, and them alone to make based on the facts that they know and we do not.\",\n",
    "                \"I think the best investment strategy is to diversify your portfolio across different asset classes.\",\n",
    "                \"Historical price-to-earnings (PE) ratios for small-cap and large-cap stocks can vary significantly over time and may not be directly comparable due to the different characteristics of these two categories of stocks.Small-cap stocks, which are defined as stocks with a market capitalization of less than $2 billion, tend to be riskier and more volatile than large-cap stocks, which have a market capitalization of $10 billion or more. As a result, investors may be willing to pay a higher price for the potential growth opportunities offered by small-cap stocks, which can lead to higher PE ratios.On the other hand, large-cap stocks tend to be more established and stable, with a longer track record of earnings and revenue growth. As a result, these stocks may trade at lower PE ratios, as investors may be less willing to pay a premium for their growth potential.It is important to note that PE ratios are just one factor to consider when evaluating a stock and should not be used in isolation. Other factors, such as the company's financial health, industry trends, and macroeconomic conditions, can also impact a stock's PE ratio.\",\n",
    "                \"Co-signing a personal loan for a friend or family member can be a risky proposition. When you co-sign a loan, you are agreeing to be responsible for the loan if the borrower is unable to make the payments. This means that if your friend or family member defaults on the loan, you will be on the hook for the remaining balance.There are a few things to consider before co-signing a personal loan for someone:Do you trust the borrower to make the payments on time and in full? If you are not confident that the borrower will be able to make the payments, it may not be a good idea to co-sign the loan.Can you afford to make the payments if the borrower defaults? If you are unable to make the payments, co-signing the loan could put your own financial stability at risk.What is the purpose of the loan? If the borrower is using the loan for a risky or questionable venture, it may not be worth the risk to co-sign.Is there another way for the borrower to get the loan without a co-signer? If the borrower has a good credit score and is able to qualify for a loan on their own, it may not be necessary for you to co-sign.In general, it is important to carefully consider the risks and potential consequences before co-signing a loan for someone. If you do decide to co-sign, it is a good idea to have a conversation with the borrower about their plans for making the loan payments and to have a clear understanding of your responsibilities as a co-signer.\",\n",
    "                \"The optimal approach to risk management involves careful assessment of market conditions.\",\n",
    "            ],\n",
    "            \"label\": [0, 0, 0, 1, 1, 1],  # 0=human, 1=AI\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        logger.info(f\"Created sample dataset with {len(df)} examples\")\n",
    "        return df\n",
    "\n",
    "    def load_origin_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Load finance dataset from JSON Lines format\"\"\"\n",
    "        files = self.find_finance_data_files()\n",
    "\n",
    "        logger.info(\"Loading finance datasets...\")\n",
    "        logger.info(f\"Found files: {list(files.keys())}\")\n",
    "\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if \"original\" in files:\n",
    "            finance_path = files[\"original\"]\n",
    "            logger.info(f\"Loading from: {finance_path}\")\n",
    "\n",
    "            try:\n",
    "                with open(finance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    line_count = 0\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            line_count += 1\n",
    "\n",
    "                            # Extract human answers\n",
    "                            human_answers = data.get(\"human_answers\", [])\n",
    "                            if (\n",
    "                                human_answers\n",
    "                                and len(human_answers) > 0\n",
    "                                and human_answers[0]\n",
    "                            ):\n",
    "                                all_texts.append(human_answers[0])\n",
    "                                all_labels.append(0)  # 0 for human\n",
    "\n",
    "                            # Extract ChatGPT answers\n",
    "                            chatgpt_answers = data.get(\"chatgpt_answers\", [])\n",
    "                            if (\n",
    "                                chatgpt_answers\n",
    "                                and len(chatgpt_answers) > 0\n",
    "                                and chatgpt_answers[0]\n",
    "                            ):\n",
    "                                all_texts.append(chatgpt_answers[0])\n",
    "                                all_labels.append(1)  # 1 for ChatGPT\n",
    "\n",
    "                            # Log progress every 1000 lines\n",
    "                            if line_num % 1000 == 0:\n",
    "                                logger.info(\n",
    "                                    f\"Processed {line_num} lines, extracted {len(all_texts)} texts\"\n",
    "                                )\n",
    "\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.error(f\"Error parsing line {line_num}: {e}\")\n",
    "                            logger.error(f\"Line content: {line[:100]}...\")\n",
    "                            continue\n",
    "\n",
    "                logger.info(f\"Successfully loaded {line_count} JSON objects\")\n",
    "                logger.info(f\"Total texts extracted: {len(all_texts)}\")\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found: {finance_path}\")\n",
    "                return self.create_sample_dataset()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading finance data: {e}\")\n",
    "                return self.create_sample_dataset()\n",
    "        else:\n",
    "            logger.warning(\"No finance data file found. Creating sample dataset...\")\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "        if len(all_texts) == 0:\n",
    "            logger.warning(\n",
    "                \"No texts extracted from finance data. Creating sample dataset...\"\n",
    "            )\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            {\"id\": range(len(all_texts)), \"text\": all_texts, \"label\": all_labels}\n",
    "        )\n",
    "\n",
    "        # Shuffle data\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        df[\"id\"] = range(len(df))\n",
    "\n",
    "        # Print summary\n",
    "        logger.info(f\"\\n=== Dataset Summary ===\")\n",
    "        logger.info(f\"Total samples: {len(df)}\")\n",
    "        logger.info(f\"Human texts: {len(df[df['label'] == 0])}\")\n",
    "        logger.info(f\"ChatGPT texts: {len(df[df['label'] == 1])}\")\n",
    "\n",
    "        # Show balance\n",
    "        balance = len(df[df[\"label\"] == 0]) / len(df) * 100\n",
    "        logger.info(\n",
    "            f\"Dataset balance: {balance:.1f}% human, {100-balance:.1f}% ChatGPT\"\n",
    "        )\n",
    "\n",
    "        # Display samples\n",
    "        if len(df) > 0:\n",
    "            logger.info(\"\\nSample texts:\")\n",
    "            for label in [0, 1]:\n",
    "                label_name = \"Human\" if label == 0 else \"ChatGPT\"\n",
    "                samples = df[df[\"label\"] == label]\n",
    "                if len(samples) > 0:\n",
    "                    sample_text = samples.iloc[0][\"text\"]\n",
    "                    logger.info(f\"\\n[{label_name}]: {sample_text[:200]}...\")\n",
    "\n",
    "        return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "    def save_dataset(self, df: pd.DataFrame, filename: str, format: str = \"csv\"):\n",
    "        \"\"\"\n",
    "        保存数据集到文件\n",
    "\n",
    "        Args:\n",
    "            df: 要保存的DataFrame\n",
    "            filename: 文件名（不含扩展名）\n",
    "            format: 文件格式 ('csv', 'json', 'jsonl')\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.data_dir, f\"{filename}.{format}\")\n",
    "\n",
    "        if format == \"csv\":\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif format == \"json\":\n",
    "            df.to_json(filepath, orient=\"records\", indent=2)\n",
    "        elif format == \"jsonl\":\n",
    "            df.to_json(filepath, orient=\"records\", lines=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "        logger.info(f\"Saved dataset to {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_data(path)\n",
    "\n",
    "\n",
    "def load_finance_dataset() -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_finance_dataset()\n",
    "\n",
    "\n",
    "def load_origin_dataset() -> pd.DataFrame:\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_origin_dataset\n",
    "\n",
    "\n",
    "def create_sample_dataset() -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.create_sample_dataset()\n",
    "\n",
    "\n",
    "def evaluate_detector(\n",
    "    detector: AITextDetector, texts: List[str], labels: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    评估检测器性能\n",
    "    \"\"\"\n",
    "    results = detector.detect_batch(texts, labels)\n",
    "\n",
    "    predictions = results[\"predictions\"]\n",
    "    probabilities = results[\"probabilities\"]\n",
    "\n",
    "    # 计算指标\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"auc\": roc_auc_score(labels, probabilities),\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predictions, target_names=[\"Human\", \"AI\"]))\n",
    "\n",
    "    # 分析相似度分布\n",
    "    human_scores = results[\"similarity_scores\"][np.array(labels) == 0]\n",
    "    ai_scores = results[\"similarity_scores\"][np.array(labels) == 1]\n",
    "\n",
    "    print(f\"\\nSimilarity Score Distribution:\")\n",
    "    print(\n",
    "        f\"Human texts - Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\"\n",
    "    )\n",
    "    print(f\"AI texts - Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def plot_roc_curves(results: Dict):\n",
    "    \"\"\"绘制所有实验的ROC曲线\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        fpr, tpr, _ = roc_curve(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        auc = roc_auc_score(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curves(results: Dict):\n",
    "    \"\"\"绘制所有实验的Precision-Recall曲线\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        plt.plot(recall, precision, label=f\"{name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(results: Dict):\n",
    "    \"\"\"绘制所有实验的混淆矩阵\"\"\"\n",
    "    n_results = len(results)\n",
    "    fig, axes = plt.subplots(1, n_results, figsize=(6 * n_results, 5))\n",
    "    if n_results == 1:\n",
    "        axes = [axes]  # Make it iterable\n",
    "    for ax, (name, result_data) in zip(axes, results.items()):\n",
    "        cm = confusion_matrix(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"]\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            ax=ax,\n",
    "            cbar=False,\n",
    "            xticklabels=[\"Human\", \"AI\"],\n",
    "            yticklabels=[\"Human\", \"AI\"],\n",
    "        )\n",
    "        ax.set_title(f\"Confusion Matrix: {name}\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_classification_metrics(results: Dict):\n",
    "    \"\"\"将关键分类指标（P, R, F1）绘制成条形图进行比较\"\"\"\n",
    "    metrics_data = []\n",
    "    for name, result_data in results.items():\n",
    "        report = classification_report(\n",
    "            result_data[\"test_labels\"],\n",
    "            result_data[\"test_results\"][\"predictions\"],\n",
    "            target_names=[\"Human\", \"AI\"],\n",
    "            output_dict=True,\n",
    "        )\n",
    "        # 只关注AI类别的指标\n",
    "        ai_metrics = report[\"AI\"]\n",
    "        metrics_data.append(\n",
    "            {\n",
    "                \"Configuration\": name,\n",
    "                \"Precision\": ai_metrics[\"precision\"],\n",
    "                \"Recall\": ai_metrics[\"recall\"],\n",
    "                \"F1-Score\": ai_metrics[\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df_melted = df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(data=df_melted, x=\"Configuration\", y=\"Score\", hue=\"Metric\")\n",
    "    plt.title('Classification Metrics for \"AI\" Class')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_optimal_threshold(labels: List[int], scores: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Find the optimal classification threshold using the ROC curve.\n",
    "\n",
    "    The optimal threshold is the one that maximizes the difference\n",
    "    between the true positive rate (TPR) and the false positive rate (FPR).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "\n",
    "    # Calculate the optimal threshold\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    print(\n",
    "        f\"\\nFound optimal similarity threshold from training data: {optimal_threshold:.4f}\"\n",
    "    )\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "# 主执行函数\n",
    "async def main(\n",
    "    data_path: str = \"finance\",\n",
    "    max_samples: Optional[int] = None,\n",
    "    use_cache: bool = True,\n",
    "    config: Optional[DetectionConfig] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    主执行函数 - 使用增强版检测器\n",
    "\n",
    "    Args:\n",
    "        data_path: 数据文件路径或 'finance'/'sample'\n",
    "        max_samples: 限制处理的样本数量（用于快速测试）\n",
    "        use_cache: 是否使用缓存\n",
    "        config: 检测器配置\n",
    "    \"\"\"\n",
    "\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    print(\"=== Enhanced AI Text Detection ===\")\n",
    "    print(\n",
    "        f\"Method: Multi-feature extraction with {config.revision_model if config else 'default model'}\"\n",
    "    )\n",
    "\n",
    "    # 1. 加载数据\n",
    "    logger.info(f\"Loading data from: {data_path}\")\n",
    "    loader = CustomDataLoader()\n",
    "    df = loader.load_data(data_path)\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Using first {max_samples} samples for testing\")\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    # 2. 数据分割（70% 训练，30% 测试）\n",
    "    split_idx = int(len(texts) * 0.6)\n",
    "    train_texts, test_texts = texts[:split_idx], texts[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "    print(f\"\\nDataset split: {len(train_texts)} training, {len(test_texts)} testing\")\n",
    "\n",
    "    # 3. 创建检测器\n",
    "    if config is None:\n",
    "        config = DetectionConfig(\n",
    "            revision_model=\"t5-small\",  # 或 \"gpt2\" t5-small\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        )\n",
    "\n",
    "    detector = AITextDetector(config)\n",
    "\n",
    "    # 4. 加载缓存\n",
    "    cache_path = f\"enhanced_cache_{config.revision_model}.json\"\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        with open(cache_path, \"r\") as f:\n",
    "            detector.cache = json.load(f)\n",
    "        print(f\"Loaded cache with {len(detector.cache)} entries\")\n",
    "\n",
    "    # 5. 训练阶段\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    print(\n",
    "        \"Train labels distribution:\", pd.Series(train_labels).value_counts().to_dict()\n",
    "    )\n",
    "    train_results = await detector.detect_batch(train_texts, train_labels)\n",
    "    # If not using the ML classifier, find the best threshold from the training run\n",
    "    if not config.use_ml_classifier:\n",
    "        optimal_threshold = find_optimal_threshold(\n",
    "            train_labels, train_results[\"similarity_scores\"]\n",
    "        )\n",
    "        # Update the detector's config with the new, optimal threshold\n",
    "        detector.config.similarity_threshold = optimal_threshold\n",
    "\n",
    "    # 6. 测试阶段\n",
    "    print(\"\\n=== Testing Phase ===\")\n",
    "    test_results = await detector.detect_batch(test_texts)\n",
    "\n",
    "    # 7. 评估结果\n",
    "    test_predictions = test_results[\"predictions\"]\n",
    "    test_probabilities = test_results[\"probabilities\"]\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    auc = roc_auc_score(test_labels, test_probabilities)\n",
    "\n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # 详细报告\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            test_labels, test_predictions, target_names=[\"Human\", \"AI\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"            Human    AI\")\n",
    "    print(f\"Actual Human  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"       AI     {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "    # 特征分析\n",
    "    features_df = test_results[\"features\"]\n",
    "    similarity_scores = test_results[\"similarity_scores\"]\n",
    "\n",
    "    # 分组统计\n",
    "    human_scores = similarity_scores[np.array(test_labels) == 0]\n",
    "    ai_scores = similarity_scores[np.array(test_labels) == 1]\n",
    "\n",
    "    print(f\"\\n=== Similarity Score Analysis ===\")\n",
    "    print(f\"Human texts:\")\n",
    "    print(f\"  Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(human_scores):.3f}, Max: {np.max(human_scores):.3f}\")\n",
    "\n",
    "    print(f\"AI texts:\")\n",
    "    print(f\"  Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(ai_scores):.3f}, Max: {np.max(ai_scores):.3f}\")\n",
    "\n",
    "    # 显示错误分类的例子\n",
    "    print(\"\\n=== Misclassified Examples ===\")\n",
    "    misclassified_idx = np.where(test_predictions != test_labels)[0]\n",
    "\n",
    "    for i, idx in enumerate(misclassified_idx[:50]):  # 显示前5个\n",
    "        true_label = \"AI\" if test_labels[idx] == 1 else \"Human\"\n",
    "        pred_label = \"AI\" if test_predictions[idx] == 1 else \"Human\"\n",
    "        text = (\n",
    "            test_texts[idx][:100] + \"...\"\n",
    "            if len(test_texts[idx]) > 100\n",
    "            else test_texts[idx]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "        print(\n",
    "            f\"Similarity: {similarity_scores[idx]:.3f}, Probability: {test_probabilities[idx]:.3f}\"\n",
    "        )\n",
    "\n",
    "    # 8. 保存缓存和模型\n",
    "    if use_cache:\n",
    "        with open(cache_path, \"w\") as f:\n",
    "            json.dump(detector.cache, f)\n",
    "        print(f\"\\nCache saved to {cache_path}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = f\"enhanced_detector_{config.revision_model}.pkl\"\n",
    "    detector.save_model(model_path)\n",
    "\n",
    "    return {\n",
    "        \"detector\": detector,\n",
    "        \"test_results\": test_results,\n",
    "        \"test_labels\": test_labels,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    "\n",
    "\n",
    "# 实验对比函数\n",
    "async def run_comparison_experiment(\n",
    "    data_path: str = \"finance\", max_samples: Optional[int] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    运行对比实验，比较不同配置的效果\n",
    "    \"\"\"\n",
    "    print(\"=== Comparison Experiment ===\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    configurations = {\n",
    "        # \"t5 with Allmini&perturbation\": DetectionConfig(\n",
    "        #     revision_model=\"t5-small\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0.15,\n",
    "        #     use_ml_classifier=False,\n",
    "        # ),\n",
    "        # \"GPT-2 with AllMini&perturbation\": DetectionConfig(\n",
    "        #     revision_model=\"gpt2\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0,\n",
    "        #     use_ml_classifier=False,\n",
    "        # ),\n",
    "        # \"t5 with Allmini\": DetectionConfig(\n",
    "        #     revision_model=\"t5-small\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0.15,\n",
    "        #     use_ml_classifier=False,\n",
    "        # ),\n",
    "        # \"GPT-2 with AllMini\": DetectionConfig(\n",
    "        #     revision_model=\"gpt2\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0,\n",
    "        #     use_ml_classifier=False,\n",
    "        # ),\n",
    "        # \"t5 with Allmini&feature&perturbation\": DetectionConfig(\n",
    "        #     revision_model=\"t5-small\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0.15,\n",
    "        #     use_ml_classifier=True,\n",
    "        # ),\n",
    "        # \"GPT-2 with AllMini&feature&perturbation\": DetectionConfig(\n",
    "        #     revision_model=\"gpt2\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0.15,\n",
    "        #     use_ml_classifier=True,\n",
    "        # ),\n",
    "        \"gpt3.5 with Bert model\": DetectionConfig(\n",
    "            revision_model=\"gpt-3.5-turbo\",\n",
    "            embedding_model=\"bert-base-uncased\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        ),\n",
    "        \"gpt3.5 with RoBEATa model\": DetectionConfig(\n",
    "            revision_model=\"gpt-3.5-turbo\",\n",
    "            embedding_model=\"roberta-base\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        ),\n",
    "        \"gpt3.5 with paraphrase model\": DetectionConfig(\n",
    "            revision_model=\"gpt-3.5-turbo\",\n",
    "            embedding_model=\"paraphrase-MiniLM-L6-v2\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        ),\n",
    "        \"gpt3.5 with funed model\": DetectionConfig(\n",
    "            revision_model=\"gpt-3.5-turbo\",  # t5-small,gpt-3.5-turbo,gpt2\n",
    "            embedding_model=\"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        ),\n",
    "        # \"High_Perturb\": DetectionConfig(\n",
    "        #     revision_model=\"t5-small\",\n",
    "        #     embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        #     perturbation_rate=0.25,\n",
    "        #     perturbation_methods=[\"synonym\", \"contextual\", \"random_swap\"],\n",
    "        #     use_ml_classifier=True,\n",
    "        # ),\n",
    "    }\n",
    "\n",
    "    # 比较结果\n",
    "    # Run experiments\n",
    "    for name, config in configurations.items():\n",
    "        print(f\"\\n{'='*20} Running: {name} {'='*20}\")\n",
    "        try:\n",
    "            results[name] = await main(\n",
    "                data_path=data_path,\n",
    "                max_samples=max_samples,\n",
    "                use_cache=True,\n",
    "                config=config,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"!!!!!! Experiment '{name}' failed: {e} !!!!!!\")\n",
    "\n",
    "    # Compare results textually\n",
    "    print(\"\\n\\n=== Final Comparison Summary ===\")\n",
    "    print(f\"{'Configuration':<20} {'Accuracy':<10} {'AUC':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for name, result_data in results.items():\n",
    "        print(\n",
    "            f\"{name:<20} {result_data['accuracy']:<10.4f} {result_data['auc']:<10.4f}\"\n",
    "        )\n",
    "\n",
    "    # Visualize results\n",
    "    print(\"\\n\\nGenerating visualizations...\")\n",
    "    if results:\n",
    "        plot_roc_curves(results)\n",
    "        plot_precision_recall_curves(results)\n",
    "        plot_confusion_matrices(results)\n",
    "        plot_classification_metrics(results)\n",
    "    else:\n",
    "        print(\"No successful experiments to visualize.\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:04:03.965596Z",
     "iopub.status.busy": "2025-07-13T17:04:03.965019Z",
     "iopub.status.idle": "2025-07-13T17:04:03.993284Z",
     "shell.execute_reply": "2025-07-13T17:04:03.992562Z",
     "shell.execute_reply.started": "2025-07-13T17:04:03.965578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "双路径融合模型实现\n",
    "结合Transformer深度特征和统计特征进行AI文本检测\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DualPathFusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    双路径融合模型\n",
    "    路径A: Transformer提取深度语义特征\n",
    "    路径B: 统计特征（相似度等）\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_name: str = \"distilbert-base-uncased\",\n",
    "        num_statistical_features: int = 13,\n",
    "        hidden_size: int = 256,\n",
    "        dropout_rate: float = 0.3,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            transformer_name: 预训练Transformer模型名称\n",
    "            num_statistical_features: 统计特征的数量\n",
    "            hidden_size: MLP隐藏层大小\n",
    "            dropout_rate: Dropout率\n",
    "        \"\"\"\n",
    "        super(DualPathFusionModel, self).__init__()\n",
    "\n",
    "        # 路径A: Transformer\n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        self.transformer_dim = self.transformer.config.hidden_size\n",
    "\n",
    "        # 冻结Transformer的前几层\n",
    "        for param in self.transformer.base_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # 路径B直接使用输入的统计特征\n",
    "\n",
    "        # 特征融合后的分类头\n",
    "        fusion_dim = self.transformer_dim + num_statistical_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size // 2, 2),  # 二分类\n",
    "        )\n",
    "\n",
    "        # 特征归一化层\n",
    "        self.feature_norm = nn.BatchNorm1d(num_statistical_features)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        statistical_features: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        Args:\n",
    "            input_ids: Transformer输入的token IDs [batch_size, seq_len]\n",
    "            attention_mask: 注意力掩码 [batch_size, seq_len]\n",
    "            statistical_features: 统计特征 [batch_size, num_features]\n",
    "\n",
    "        Returns:\n",
    "            logits: 分类logits [batch_size, 2]\n",
    "        \"\"\"\n",
    "        # 路径A: 通过Transformer提取深度语义特征\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # 取[CLS] token的输出作为句子表示\n",
    "        cls_embedding = transformer_outputs.last_hidden_state[\n",
    "            :, 0, :\n",
    "        ]  # [batch_size, transformer_dim]\n",
    "\n",
    "        # 路径B: 归一化统计特征\n",
    "        normalized_features = self.feature_norm(\n",
    "            statistical_features\n",
    "        )  # [batch_size, num_features]\n",
    "\n",
    "        # 特征融合：拼接两种特征\n",
    "        fused_features = torch.cat([cls_embedding, normalized_features], dim=1)\n",
    "\n",
    "        # 通过分类头得到最终预测\n",
    "        logits = self.classifier(fused_features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TextDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AI文本检测数据集\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        statistical_features: np.ndarray,\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: 原始文本列表\n",
    "            statistical_features: 统计特征矩阵 [n_samples, n_features]\n",
    "            labels: 标签列表 (0=human, 1=AI)\n",
    "            tokenizer: Transformer分词器\n",
    "            max_length: 最大序列长度\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.statistical_features = statistical_features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取文本和标签\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        features = self.statistical_features[idx]\n",
    "\n",
    "        # 使用tokenizer编码文本\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"statistical_features\": torch.tensor(features, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "class DualPathDetector:\n",
    "    \"\"\"\n",
    "    使用双路径融合模型的检测器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased\", device: str = None):\n",
    "        \"\"\"\n",
    "        初始化检测器\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        # 初始化tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # 初始化模型（稍后会根据特征数量调整）\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        print(f\"Dual Path Detector initialized on {self.device}\")\n",
    "\n",
    "    def prepare_data(\n",
    "        self, texts: List[str], features_df: pd.DataFrame, labels: List[int]\n",
    "    ) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        准备训练和验证数据\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            features_df: 包含统计特征的DataFrame\n",
    "            labels: 标签列表\n",
    "\n",
    "        Returns:\n",
    "            train_dataset, val_dataset\n",
    "        \"\"\"\n",
    "        # 标准化特征\n",
    "        features_array = features_df.values\n",
    "        features_normalized = self.scaler.fit_transform(features_array)\n",
    "\n",
    "        # 分割数据\n",
    "        X_train, X_val, y_train, y_val, feat_train, feat_val = train_test_split(\n",
    "            texts,\n",
    "            labels,\n",
    "            features_normalized,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=labels,\n",
    "        )\n",
    "\n",
    "        # 创建数据集\n",
    "        train_dataset = TextDetectionDataset(\n",
    "            X_train, feat_train, y_train, self.tokenizer\n",
    "        )\n",
    "        val_dataset = TextDetectionDataset(X_val, feat_val, y_val, self.tokenizer)\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        num_epochs: int = 10,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 2e-5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        训练模型\n",
    "        \"\"\"\n",
    "        # 如果模型还未初始化，根据特征数量初始化\n",
    "        if self.model is None:\n",
    "            num_features = train_dataset[0][\"statistical_features\"].shape[0]\n",
    "            self.model = DualPathFusionModel(\n",
    "                transformer_name=self.model_name, num_statistical_features=num_features\n",
    "            ).to(self.device)\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # 优化器和损失函数\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # 学习率调度器\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", patience=2, factor=0.5\n",
    "        )\n",
    "\n",
    "        # 训练循环\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # 训练阶段\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            for batch in train_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                features = batch[\"statistical_features\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 前向传播\n",
    "                logits = self.model(input_ids, attention_mask, features)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # 反向传播\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # 统计\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                train_bar.set_postfix(\n",
    "                    {\n",
    "                        \"loss\": f\"{loss.item():.4f}\",\n",
    "                        \"acc\": f\"{100.*train_correct/train_total:.2f}%\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # 验证阶段\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(\n",
    "                    val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"\n",
    "                ):\n",
    "                    input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                    features = batch[\"statistical_features\"].to(self.device)\n",
    "                    labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                    logits = self.model(input_ids, attention_mask, features)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(logits.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # 计算平均损失和准确率\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = 100.0 * train_correct / train_total\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            # 学习率调度\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            # 保存最佳模型\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.save_model(\"best_dual_path_model.pt\")\n",
    "                print(\"  ✓ Saved best model\")\n",
    "\n",
    "    def predict(\n",
    "        self, texts: List[str], features_df: pd.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        预测新文本\n",
    "\n",
    "        Returns:\n",
    "            predictions: 预测标签\n",
    "            probabilities: 预测概率\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # 标准化特征\n",
    "        features_normalized = self.scaler.transform(features_df.values)\n",
    "\n",
    "        # 创建数据集和加载器\n",
    "        dataset = TextDetectionDataset(\n",
    "            texts, features_normalized, [0] * len(texts), self.tokenizer  # 假标签\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                features = batch[\"statistical_features\"].to(self.device)\n",
    "\n",
    "                logits = self.model(input_ids, attention_mask, features)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())  # AI类的概率\n",
    "\n",
    "        return np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"scaler\": self.scaler,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"num_features\": self.model.classifier[0].in_features\n",
    "                - self.model.transformer_dim,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "\n",
    "        # 重新初始化模型\n",
    "        self.model = DualPathFusionModel(\n",
    "            transformer_name=checkpoint[\"model_name\"],\n",
    "            num_statistical_features=checkpoint[\"num_features\"],\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.scaler = checkpoint[\"scaler\"]\n",
    "        self.model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:13:03.033426Z",
     "iopub.status.busy": "2025-07-13T17:13:03.033092Z",
     "iopub.status.idle": "2025-07-13T17:13:09.757501Z",
     "shell.execute_reply": "2025-07-13T17:13:09.756335Z",
     "shell.execute_reply.started": "2025-07-13T17:13:03.033399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging setup complete. All output will be saved to ./result\\output.log\n",
      "=== Transformer Model Comparison Experiment ===\n",
      "\n",
      "========================= Running Experiment: GPT-2 =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.v_lin.bias, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.attention.out_lin.weight, vocab_layer_norm.weight, distilbert.embeddings.position_embeddings.weight, vocab_projector.weight, vocab_projector.bias, distilbert.embeddings.word_embeddings.weight, vocab_layer_norm.bias, distilbert.transformer.layer.*.output_layer_norm.weight, vocab_transform.weight, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.transformer.layer.*.output_layer_norm.bias, distilbert.transformer.layer.*.attention.v_lin.weight, distilbert.transformer.layer.*.sa_layer_norm.bias, distilbert.transformer.layer.*.attention.q_lin.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.ffn.lin*.bias, vocab_transform.bias, distilbert.transformer.layer.*.attention.k_lin.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized for model: gpt-3.5-turbo\n",
      "Using custom base URL: https://api.chatanywhere.tech/v1\n",
      "Use pytorch device_name: cuda:0\n",
      "Load pretrained SentenceTransformer: gpt2\n",
      "No sentence-transformers model found with name gpt2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: h.*.attn.c_proj.bias, ln_f.bias, h.*.attn.c_proj.weight, h.*.mlp.c_fc.bias, h.*.ln_*.weight, wte.weight, h.*.mlp.c_proj.bias, ln_f.weight, h.*.mlp.c_proj.weight, h.*.ln_*.bias, wpe.weight, h.*.attn.c_attn.weight, h.*.mlp.c_fc.weight, h.*.attn.c_attn.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual Path Detector initialized on cuda\n",
      "Enhanced AI Detector initialized with Dual Path Model: gpt2\n",
      "Successfully loaded cache with 7844 entries from enhanced_cache_gpt-3.5-turbo.json\n",
      "Searching for finance data files...\n",
      "Loading finance datasets...\n",
      "Found files: ['revised_chatgpt', 'revised_human', 'original']\n",
      "Loading from: ./data\\finance.jsonl\n",
      "Processed 1000 lines, extracted 2000 texts\n",
      "Processed 2000 lines, extracted 4000 texts\n",
      "Processed 3000 lines, extracted 6000 texts\n",
      "Successfully loaded 3933 JSON objects\n",
      "Total texts extracted: 7866\n",
      "\n",
      "=== Dataset Summary ===\n",
      "Total samples: 7866\n",
      "Human texts: 3933\n",
      "ChatGPT texts: 3933\n",
      "Dataset balance: 50.0% human, 50.0% ChatGPT\n",
      "\n",
      "Sample texts:\n",
      "\n",
      "[Human]: In the United States you can't, because the average millennial in the United States has no opportunity to save money. Either you get a college education, then you will be burdened with a student loan....\n",
      "\n",
      "[ChatGPT]: To determine your take-home pay if you claim 3 exemptions, you will need to calculate your federal income tax withholding and any other applicable deductions.First, you will need to determine your tax...\n",
      "Data prepared: 70 training, 30 testing.\n",
      "=== Training Dual Path Model ===\n",
      "1. Extracting features...\n",
      "Extracting statistical features...\n",
      "Processing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing: 100%|██████████| 70/70 [00:20<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Revising texts with gpt-3.5-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Revising: 100%|██████████| 70/70 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0be582d048647269aed497e81327c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "!!!!!! Experiment 'GPT-2' FAILED: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`. !!!!!!\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2085707846.py\", line 273, in run_transformer_comparison\n",
      "    results[name] = await main_workflow_for_comparison(\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2085707846.py\", line 197, in main_workflow_for_comparison\n",
      "    await detector.train_dual_path_model(train_df=train_df, num_epochs=5, batch_size=16)\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2085707846.py\", line 55, in train_dual_path_model\n",
      "    features_df = await self.extract_all_features(texts)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2085707846.py\", line 34, in extract_all_features\n",
      "    results = await self.base_detector.detect_batch(texts)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2939228537.py\", line 821, in detect_batch\n",
      "    features = self.feature_extractor.extract_features(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2939228537.py\", line 595, in extract_features\n",
      "    features[\"semantic_similarity\"] = self._compute_semantic_similarity(\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\suzehao\\AppData\\Local\\Temp\\ipykernel_19516\\2939228537.py\", line 615, in _compute_semantic_similarity\n",
      "    emb1 = self.sentence_model.encode(text1)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 1020, in encode\n",
      "    features = self.tokenize(sentences_batch, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\", line 1570, in tokenize\n",
      "    return self[0].tokenize(texts, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py\", line 495, in tokenize\n",
      "    self.tokenizer(\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2855, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2943, in _call_one\n",
      "    return self.batch_encode_plus(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 3135, in batch_encode_plus\n",
      "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
      "                                                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"f:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py\", line 2751, in _get_padding_truncation_strategies\n",
      "    raise ValueError(\n",
      "ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
      "\n",
      "\n",
      "\n",
      "========================= Final Comparison Summary =========================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 308\u001b[39m\n\u001b[32m    304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m run_transformer_comparison(data_path=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m, max_samples=\u001b[32m100\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 290\u001b[39m, in \u001b[36mrun_transformer_comparison\u001b[39m\u001b[34m(data_path, max_samples)\u001b[39m\n\u001b[32m    285\u001b[39m logging.info(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m25\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m Final Comparison Summary \u001b[39m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m25\u001b[39m)\n\u001b[32m    286\u001b[39m summary_df = pd.DataFrame([\n\u001b[32m    287\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mConfiguration\u001b[39m\u001b[33m\"\u001b[39m: name, \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: res[\u001b[33m\"\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mAUC\u001b[39m\u001b[33m\"\u001b[39m: res[\u001b[33m\"\u001b[39m\u001b[33mauc\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mF1-Score\u001b[39m\u001b[33m\"\u001b[39m: res[\u001b[33m\"\u001b[39m\u001b[33mf1_score\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m    288\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, res \u001b[38;5;129;01min\u001b[39;00m results.items()\n\u001b[32m    289\u001b[39m ])\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m summary_df[\u001b[33m'\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43msummary_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAccuracy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.map(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m'\u001b[39m.format)\n\u001b[32m    291\u001b[39m summary_df[\u001b[33m'\u001b[39m\u001b[33mAUC\u001b[39m\u001b[33m'\u001b[39m] = summary_df[\u001b[33m'\u001b[39m\u001b[33mAUC\u001b[39m\u001b[33m'\u001b[39m].map(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m'\u001b[39m.format)\n\u001b[32m    292\u001b[39m summary_df[\u001b[33m'\u001b[39m\u001b[33mF1-Score\u001b[39m\u001b[33m'\u001b[39m] = summary_df[\u001b[33m'\u001b[39m\u001b[33mF1-Score\u001b[39m\u001b[33m'\u001b[39m].map(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[33m'\u001b[39m.format)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\hk\\gs\\program\\LLM-generated-text-detection-main\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'Accuracy'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "将双路径融合模型集成到现有的AI文本检测流程\n",
    "\"\"\"\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "\n",
    "class EnhancedAIDetector:\n",
    "    \"\"\"\n",
    "    MODIFIED AI Detector: Integrates the dual-path model.\n",
    "    The __init__ method is updated to accept a model name for the dual-path detector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[DetectionConfig] = None, dual_path_model_name: str = \"distilbert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: Configuration for the base detector (feature extraction).\n",
    "            dual_path_model_name: The name of the transformer to use in the DualPathFusionModel.\n",
    "        \"\"\"\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.base_detector = AITextDetector(self.config)\n",
    "        # Pass the dynamic model name to the dual-path detector\n",
    "        self.dual_path_detector = DualPathDetector(model_name=dual_path_model_name)\n",
    "        print(f\"Enhanced AI Detector initialized with Dual Path Model: {dual_path_model_name}\")\n",
    "\n",
    "    # --- All other methods of this class (like extract_all_features, etc.) remain the same ---\n",
    "    # (For brevity, the other methods are omitted here but should be kept in your code)\n",
    "    async def extract_all_features(self, texts: List[str]) -> pd.DataFrame:\n",
    "        # This method should remain as is\n",
    "        print(\"Extracting statistical features...\")\n",
    "        results = await self.base_detector.detect_batch(texts)\n",
    "        features_array = results['features']\n",
    "        # ... rest of the feature processing logic\n",
    "        # (Assuming the logic from your notebook is here)\n",
    "        all_possible_features = [\n",
    "            'semantic_similarity', 'word_overlap_ratio', 'vocabulary_change_ratio',\n",
    "            'unique_words_ratio', 'stopword_ratio_change', 'sentence_count_ratio',\n",
    "            'avg_sentence_length_change', 'pos_NN_ratio_change', 'pos_VB_ratio_change',\n",
    "            'pos_JJ_ratio_change', 'pos_RB_ratio_change', 'punctuation_ratio_change',\n",
    "            'capitalization_ratio_change', 'avg_word_length_change', 'char_level_similarity',\n",
    "            'word_level_similarity', 'length_ratio'\n",
    "        ]\n",
    "        core_features = all_possible_features[:13] # Example, adjust as needed\n",
    "        features_df = pd.DataFrame(features_array[:, :13], columns=core_features)\n",
    "        return features_df\n",
    "\n",
    "    async def train_dual_path_model(self, train_df: pd.DataFrame, num_epochs: int, batch_size: int):\n",
    "        print(\"=== Training Dual Path Model ===\")\n",
    "        texts = train_df[\"text\"].tolist()\n",
    "        labels = train_df[\"label\"].tolist()\n",
    "        print(\"1. Extracting features...\")\n",
    "        features_df = await self.extract_all_features(texts)\n",
    "        print(\"2. Preparing datasets...\")\n",
    "        train_dataset, val_dataset = self.dual_path_detector.prepare_data(texts, features_df, labels)\n",
    "        print(\"3. Training model...\")\n",
    "        self.dual_path_detector.train(train_dataset, val_dataset, num_epochs=num_epochs, batch_size=batch_size)\n",
    "        print(\"\\n Training completed!\")\n",
    "\n",
    "    async def detect_with_dual_path(self, texts: List[str]) -> Dict:\n",
    "        features_df = await self.extract_all_features(texts)\n",
    "        predictions, probabilities = self.dual_path_detector.predict(texts, features_df)\n",
    "        return {\"predictions\": predictions, \"probabilities\": probabilities, \"features\": features_df}\n",
    "\n",
    "def setup_logging(output_dir: str = \"./result\"):\n",
    "    \"\"\"\n",
    "    Sets up logging to save all print output to a file and also show it on the console.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(output_dir, 'output.log')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear existing handlers to avoid duplicate logs in interactive environments\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "    # Create file handler to write to a log file (mode='w' overwrites the file each run)\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    # Create console handler to also print to the console\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_formatter = logging.Formatter('%(message)s') # Keep console output clean\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    logging.info(f\"Logging setup complete. All output will be saved to {log_file_path}\")\n",
    "\n",
    "def plot_roc_curves(results: Dict, output_dir: str = \"./result\"):\n",
    "    \"\"\"绘制所有实验的ROC曲线并保存\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        fpr, tpr, _ = roc_curve(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        auc = roc_auc_score(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"roc_curves_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(results: Dict, output_dir: str = \"./result\"):\n",
    "    \"\"\"绘制所有实验的Precision-Recall曲线并保存\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        plt.plot(recall, precision, label=f\"{name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"precision_recall_curves_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(results: Dict, output_dir: str = \"./result\"):\n",
    "    \"\"\"绘制所有实验的混淆矩阵并保存\"\"\"\n",
    "    n_results = len(results)\n",
    "    fig, axes = plt.subplots(1, n_results, figsize=(6 * n_results, 5), squeeze=False)\n",
    "    for ax, (name, result_data) in zip(axes.flatten(), results.items()):\n",
    "        cm = confusion_matrix(result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"])\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False, xticklabels=[\"Human\", \"AI\"], yticklabels=[\"Human\", \"AI\"])\n",
    "        ax.set_title(f\"Confusion Matrix: {name}\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrices_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_classification_metrics(results: Dict, output_dir: str = \"./result\"):\n",
    "    \"\"\"将关键分类指标（P, R, F1）绘制成条形图进行比较并保存\"\"\"\n",
    "    metrics_data = []\n",
    "    for name, result_data in results.items():\n",
    "        report = classification_report(result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"], target_names=[\"Human\", \"AI\"], output_dict=True)\n",
    "        ai_metrics = report[\"AI\"]\n",
    "        metrics_data.append({\"Configuration\": name, \"Precision\": ai_metrics[\"precision\"], \"Recall\": ai_metrics[\"recall\"], \"F1-Score\": ai_metrics[\"f1-score\"]})\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df_melted = df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(data=df_melted, x=\"Configuration\", y=\"Score\", hue=\"Metric\")\n",
    "    plt.title('Classification Metrics for \"AI\" Class')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.savefig(os.path.join(output_dir, \"classification_metrics_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "async def main_workflow_for_comparison(\n",
    "    data_path: str,\n",
    "    max_samples: Optional[int],\n",
    "    use_cache: bool,\n",
    "    config: DetectionConfig,\n",
    "    dual_path_model_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    MODIFIED Main Workflow: This function now uses logging instead of print.\n",
    "    \"\"\"\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    detector = EnhancedAIDetector(config=config, dual_path_model_name=dual_path_model_name)\n",
    "\n",
    "    cache_path = f\"enhanced_cache_{config.revision_model}.json\"\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, \"r\") as f:\n",
    "                detector.base_detector.cache = json.load(f)\n",
    "            logging.info(f\"Successfully loaded cache with {len(detector.base_detector.cache)} entries from {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not load cache from {cache_path}: {e}\")\n",
    "    elif use_cache:\n",
    "        logging.info(f\"Cache file not found at {cache_path}. A new cache will be created.\")\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    df_full = loader.load_data(data_path)\n",
    "    df = df_full.head(max_samples) if max_samples else df_full\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df['label'])\n",
    "    logging.info(f\"Data prepared: {len(train_df)} training, {len(test_df)} testing.\")\n",
    "\n",
    "    await detector.train_dual_path_model(train_df=train_df, num_epochs=5, batch_size=16)\n",
    "\n",
    "    test_texts = test_df[\"text\"].tolist()\n",
    "    test_labels = test_df[\"label\"].tolist()\n",
    "    test_results = await detector.detect_with_dual_path(test_texts)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, test_results['predictions'])\n",
    "    auc = roc_auc_score(test_labels, test_results['probabilities'])\n",
    "    report_dict = classification_report(test_labels, test_results['predictions'], output_dict=True)\n",
    "    f1_score = report_dict['weighted avg']['f1-score']\n",
    "\n",
    "    logging.info(f\"\\n--- Test Results for {dual_path_model_name} ---\")\n",
    "    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logging.info(f\"AUC: {auc:.4f}\")\n",
    "    logging.info(f\"Weighted F1-Score: {f1_score:.4f}\")\n",
    "    logging.info(\"\\nClassification Report:\")\n",
    "    logging.info(f\"\\n{classification_report(test_labels, test_results['predictions'], target_names=['Human', 'AI'], digits=4)}\")\n",
    "\n",
    "    return {\"detector\": detector, \"test_results\": test_results, \"test_labels\": test_labels, \"accuracy\": accuracy, \"auc\": auc, \"f1_score\": f1_score}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def run_transformer_comparison(data_path: str = \"finance\", max_samples: Optional[int] = 1000):\n",
    "    \"\"\"\n",
    "    Runs a comparison experiment between different transformer models for both\n",
    "    feature extraction and deep feature representation.\n",
    "    \"\"\"\n",
    "    setup_logging() # Setup logging to file and console\n",
    "    logging.info(\"=== Transformer Model Comparison Experiment ===\")\n",
    "    results = {}\n",
    "\n",
    "    # Define the combinations of models you want to test\n",
    "    configurations = {\n",
    "        \"BERT\": {\n",
    "            \"embedding_model\": \"bert-base-uncased\",\n",
    "            \"transformer_name\": \"bert-base-uncased\"\n",
    "        },\n",
    "        \"DistilBERT\": {\n",
    "            \"embedding_model\": \"distilbert-base-uncased\",\n",
    "            \"transformer_name\": \"distilbert-base-uncased\"\n",
    "        },\n",
    "        \"RoBERTa\": {\n",
    "            \"embedding_model\": \"roberta-base\",\n",
    "            \"transformer_name\": \"roberta-base\"\n",
    "        },\n",
    "        \"paraphrase-MiniLM-L6-v2\":{\n",
    "            \"embedding_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "            \"transformer_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "        },\n",
    "        \"fine-tune model\":{\n",
    "            \"embedding_model\": \"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "            \"transformer_name\": \"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "        },\n",
    "        # \"T5\": {\n",
    "        #     \"embedding_model\": \"t5-small\",\n",
    "        #     \"transformer_name\": \"t5-small\"\n",
    "        # },\n",
    "        # \"GPT-2\": {\n",
    "        #     \"embedding_model\": \"gpt2\",\n",
    "        #     \"transformer_name\": \"gpt2\"\n",
    "        # }\n",
    "        \n",
    "    }\n",
    "\n",
    "    # --- Run experiments for each configuration ---\n",
    "    for name, model_config in configurations.items():\n",
    "        logging.info(f\"\\n{'='*25} Running Experiment: {name} {'='*25}\")\n",
    "        try:\n",
    "            detection_config = DetectionConfig(\n",
    "                revision_model=\"gpt-3.5-turbo\",\n",
    "                embedding_model=model_config[\"embedding_model\"],\n",
    "                perturbation_rate=0.15,\n",
    "                use_ml_classifier=True,\n",
    "            )\n",
    "            \n",
    "            results[name] = await main_workflow_for_comparison(\n",
    "                data_path=data_path,\n",
    "                max_samples=max_samples,\n",
    "                use_cache=True,\n",
    "                config=detection_config,\n",
    "                dual_path_model_name=model_config[\"transformer_name\"]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            logging.error(f\"\\n!!!!!! Experiment '{name}' FAILED: {e} !!!!!!\")\n",
    "            logging.error(traceback.format_exc())\n",
    "\n",
    "    logging.info(\"\\n\\n\" + \"=\"*25 + \" Final Comparison Summary \" + \"=\"*25)\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\"Configuration\": name, \"Accuracy\": res[\"accuracy\"], \"AUC\": res[\"auc\"], \"F1-Score\": res[\"f1_score\"]}\n",
    "        for name, res in results.items()\n",
    "    ])\n",
    "    summary_df['Accuracy'] = summary_df['Accuracy'].map('{:.4f}'.format)\n",
    "    summary_df['AUC'] = summary_df['AUC'].map('{:.4f}'.format)\n",
    "    summary_df['F1-Score'] = summary_df['F1-Score'].map('{:.4f}'.format)\n",
    "    logging.info(f\"\\n{summary_df.to_string(index=False)}\")\n",
    "\n",
    "    logging.info(\"\\n\\nGenerating visualizations...\")\n",
    "    if results:\n",
    "        plot_roc_curves(results)\n",
    "        plot_precision_recall_curves(results)\n",
    "        plot_confusion_matrices(results)\n",
    "        plot_classification_metrics(results)\n",
    "    else:\n",
    "        logging.warning(\"No successful experiments were completed to visualize.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    await run_transformer_comparison(data_path=\"test\", max_samples=100)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7745407,
     "sourceId": 12289600,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
