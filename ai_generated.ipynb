{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-15T13:43:15.659723Z",
     "iopub.status.busy": "2025-07-15T13:43:15.659432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n",
      "NLTK data download completed.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enhanced AI Text Detection Pipeline\n",
    "-----------------------------------\n",
    "核心改进:\n",
    "1. 更好的LLM选择（T5/GPT-2用于重写）\n",
    "2. 多维度特征提取\n",
    "3. 更智能的扰动策略\n",
    "4. 集成学习方法\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# 尝试导入，如果失败则提示安装\n",
    "\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# OpenAI library imports\n",
    "import openai  # Good for catching error types like openai.RateLimitError\n",
    "from openai import OpenAI  # ✅ **This is the missing line you need to add.**\n",
    "import tiktoken\n",
    "\n",
    "# 基础库\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, time, hashlib, json, random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "# Transformers\n",
    "from transformers import (\n",
    "        T5ForConditionalGeneration, T5Tokenizer,\n",
    "        GPT2LMHeadModel, GPT2Tokenizer,\n",
    "        AutoTokenizer, AutoModel\n",
    "    )\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# 文本处理\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 文本扰动库\n",
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "except ImportError:\n",
    "    print(\"nlpaug not found, installing...\")\n",
    "    os.system(\"pip install nlpaug\")\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Transformers 和 sentence-transformers\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "# NLTK数据下载\n",
    "try:\n",
    "    import nltk\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"NLTK data download completed.\")\n",
    "except Exception as e:\n",
    "    print(f\"NLTK download warning: {e}\")\n",
    "    print(\"Some augmentation features may not work properly.\")\n",
    "@dataclass\n",
    "class DetectionConfig:\n",
    "    \"\"\"检测配置类\"\"\"\n",
    "    # 模型选择\n",
    "    revision_model: str = \"t5-small\"\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "    \n",
    "    # 【修正】为OpenAI API添加配置字段\n",
    "    api_key: Optional[str] = None\n",
    "    base_url: Optional[str] = None\n",
    "    \n",
    "    # 扰动参数\n",
    "    perturbation_rate: float = 0.15\n",
    "    # 【修正】使用field(default_factory=...)来避免可变默认参数问题\n",
    "    perturbation_methods: List[str] = field(default_factory=lambda: [\"synonym\", \"contextual\"])\n",
    "    \n",
    "    # 检测参数\n",
    "    similarity_threshold: float = 0.95 # 建议使用0.85作为更稳健的默认值\n",
    "    use_ml_classifier: bool = True\n",
    "    \n",
    "    # 批处理\n",
    "    batch_size: int = 16\n",
    "    max_length: int = 512\n",
    "\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.perturbation_methods is None:\n",
    "            self.perturbation_methods = [\"synonym\", \"contextual\", \"backtranslation\"]\n",
    "\n",
    "class EnhancedTextPerturber:\n",
    "    \"\"\"增强的文本扰动器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self._init_augmenters()\n",
    "\n",
    "    def _init_augmenters(self):\n",
    "        \"\"\"初始化各种扰动器\"\"\"\n",
    "        self.augmenters = {\n",
    "            \"synonym\": naw.SynonymAug(aug_src='wordnet', aug_p=self.config.perturbation_rate),\n",
    "            \"contextual\": naw.ContextualWordEmbsAug(\n",
    "                model_path='distilbert-base-uncased',\n",
    "                action=\"substitute\",\n",
    "                aug_p=self.config.perturbation_rate\n",
    "            ),\n",
    "            \"random_swap\": naw.RandomWordAug(action=\"swap\", aug_p=self.config.perturbation_rate),\n",
    "            \"spelling\": naw.SpellingAug(aug_p=self.config.perturbation_rate),\n",
    "        }\n",
    "\n",
    "    def perturb(self, text: str, method: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        智能扰动策略\n",
    "\n",
    "        Args:\n",
    "            text: 原始文本\n",
    "            method: 指定方法，None则随机选择\n",
    "        Returns:\n",
    "            扰动后的文本\n",
    "        \"\"\"\n",
    "        if method is None:\n",
    "            method = np.random.choice(self.config.perturbation_methods)\n",
    "\n",
    "        try:\n",
    "            if method == \"backtranslation\":\n",
    "                return self._backtranslate(text)\n",
    "            elif method in self.augmenters:\n",
    "                augmented = self.augmenters[method].augment(text)\n",
    "                return augmented[0] if isinstance(augmented, list) else augmented\n",
    "            else:\n",
    "                # 混合扰动\n",
    "                return self._mixed_perturbation(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Perturbation failed: {e}\")\n",
    "            return self._simple_perturb(text)\n",
    "\n",
    "    def _mixed_perturbation(self, text: str) -> str:\n",
    "        \"\"\"混合多种扰动方法\"\"\"\n",
    "        sentences = sent_tokenize(text)\n",
    "        perturbed_sentences = []\n",
    "\n",
    "        for sent in sentences:\n",
    "            # 随机选择扰动方法\n",
    "            method = np.random.choice(list(self.augmenters.keys()))\n",
    "            try:\n",
    "                perturbed = self.augmenters[method].augment(sent)\n",
    "                perturbed_sent = perturbed[0] if isinstance(perturbed, list) else perturbed\n",
    "                perturbed_sentences.append(perturbed_sent)\n",
    "            except:\n",
    "                perturbed_sentences.append(sent)\n",
    "\n",
    "        return ' '.join(perturbed_sentences)\n",
    "\n",
    "    def _simple_perturb(self, text: str) -> str:\n",
    "        \"\"\"简单扰动作为后备\"\"\"\n",
    "        words = text.split()\n",
    "\n",
    "        # 随机替换15%的词\n",
    "        num_changes = max(1, int(len(words) * self.config.perturbation_rate))\n",
    "        indices = np.random.choice(range(len(words)), size=min(num_changes, len(words)), replace=False)\n",
    "\n",
    "        for idx in indices:\n",
    "            # 简单的字符级修改\n",
    "            word = words[idx]\n",
    "            if len(word) > 3:\n",
    "                words[idx] = word[:-1] + np.random.choice(list('aeiou'))\n",
    "\n",
    "        return ' '.join(words)\n",
    "\n",
    "    def _backtranslate(self, text: str) -> str:\n",
    "        \"\"\"反向翻译（需要翻译API）\"\"\"\n",
    "        # 这里只是示例，实际需要调用翻译API\n",
    "        return text\n",
    "\n",
    "class EnhancedLLMReviser:\n",
    "    \"\"\"增强的LLM重写器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.client = None \n",
    "        self.model = None \n",
    "        self.tokenizer = None \n",
    "        self._init_reviser()\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"初始化重写模型\"\"\"\n",
    "        model_name = self.config.revision_model\n",
    "\n",
    "        if self.config.revision_model.startswith(\"t5\"):\n",
    "            # T5模型更适合文本重写任务\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.config.revision_model)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(self.config.revision_model)\n",
    "        elif self.config.revision_model == \"gpt2\":\n",
    "            # GPT-2作为备选\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # 【整合】初始化OpenAI API客户端\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"请在DetectionConfig中提供api_key或设置OPENAI_API_KEY环境变量\")\n",
    "            \n",
    "            self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\")\n",
    "\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _init_reviser(self):\n",
    "        \"\"\"根据配置初始化重写模型或API客户端\"\"\"\n",
    "        model_name = self.config.revision_model\n",
    "        \n",
    "        if model_name.startswith(\"t5\"):\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local T5 model loaded: {model_name}\")\n",
    "            \n",
    "        elif model_name == \"gpt2\":\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local GPT-2 model loaded: {model_name}\")\n",
    "\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # 【整合】初始化OpenAI API客户端\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\"请在DetectionConfig中提供api_key或设置OPENAI_API_KEY环境变量\")\n",
    "            \n",
    "            self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\")\n",
    "\n",
    "    \n",
    "    def revise(self, text: str, cache: Dict[str, str]) -> str:\n",
    "        \"\"\"\n",
    "        使用LLM重写文本\n",
    "        \n",
    "        Args:\n",
    "            text: 待重写文本\n",
    "            cache: 缓存字典\n",
    "        Returns:\n",
    "            重写后的文本\n",
    "        \"\"\"\n",
    "        # 生成缓存键\n",
    "        cache_key = hashlib.md5(f\"{text}_{self.config.revision_model}\".encode()).hexdigest()\n",
    "        # print(f\"\\norigin text: {text}\")\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            model_name = self.config.revision_model\n",
    "            revised = \"\"\n",
    "            if model_name.startswith(\"t5\"):\n",
    "                revised = self._revise_with_t5(text)\n",
    "                # print(f\"t5 rewrite: {revised}\")\n",
    "            elif model_name == \"gpt2\":\n",
    "                revised = self._revise_with_gpt2(text)\n",
    "                # print(f\"gpt2 rewrite: {revised}\")\n",
    "            elif model_name.startswith(\"gpt-\"):\n",
    "                #api rewrite\n",
    "                revised = self._revise_with_api(text)\n",
    "                # print(f\"gpt3.5 rewrite: {revised}\")\n",
    "            else:\n",
    "                revised = self._rule_based_revision(text)\n",
    "            \n",
    "            cache[cache_key] = revised\n",
    "            return revised\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Revision failed with model {self.config.revision_model}: {e}\")\n",
    "            revised = self._rule_based_revision(text)\n",
    "            cache[cache_key] = revised\n",
    "            return revised\n",
    "\n",
    "    def _revise_with_t5(self, text: str) -> str:\n",
    "        \"\"\"使用T5模型重写 - 优化版\"\"\"\n",
    "        # T5需要特定的提示格式\n",
    "        prompt = f\"paraphrase: {text}\"\n",
    "    \n",
    "        inputs = self.tokenizer.encode(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,  # 对输入进行截断的最大长度\n",
    "            truncation=True\n",
    "        ).to(self.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            # 使用束搜索 (Beam Search) 以获得更高质量和更稳定的输出\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                # --- 解码策略修改 ---\n",
    "                num_beams=4,              # 修改：设置束的宽度，4或5是常用值\n",
    "                early_stopping=True,      # 新增：当所有束都完成时提前停止\n",
    "    \n",
    "                # --- 长度控制优化 ---\n",
    "                max_length=256,           # 修改：为输出设置一个固定的最大长度\n",
    "                min_length=40,            # 新增：设置一个合理的最小长度，防止输出过短\n",
    "    \n",
    "                # --- 质量优化 ---\n",
    "                repetition_penalty=1.2    # 新增：轻微惩罚重复，提升多样性\n",
    "            )\n",
    "            \n",
    "            # 注意：使用 beam search 时，不需要 do_sample, temperature, top_p\n",
    "    \n",
    "        revised = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return revised\n",
    "\n",
    "    def _revise_with_gpt2(self, text: str) -> str:\n",
    "        \"\"\"使用GPT-2模型重写\"\"\"\n",
    "        try:\n",
    "            prompt_templates = [\n",
    "                f\"Paraphrase the following text while keeping the same meaning: {text}\\n\\nParaphrased version:\",\n",
    "                f\"Rewrite this sentence in a different way: {text}\\n\\nRewritten:\",\n",
    "                f\"Express this differently: {text}\\n\\nAlternative expression:\"\n",
    "            ]\n",
    "            prompt = np.random.choice(prompt_templates)\n",
    "            inputs = self.tokenizer.encode(\n",
    "                prompt, return_tensors=\"pt\", max_length=512, truncation=True\n",
    "            ).to(self.device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    inputs,\n",
    "                    max_length=min(len(inputs[0]) + 100, 1024),\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "    \n",
    "            \n",
    "            # 【已修正】使用 self.tokenizer\n",
    "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "            # ... (后面的提取和清理逻辑不变) ...\n",
    "            rewritten = \"\"\n",
    "            for marker in [\"Paraphrased version:\", \"Rewritten:\", \"Alternative expression:\"]:\n",
    "                if marker in full_text:\n",
    "                    rewritten = full_text.split(marker)[-1].strip()\n",
    "                    break\n",
    "            \n",
    "            if not rewritten:\n",
    "                rewritten = full_text[len(prompt):].strip()\n",
    "    \n",
    "            if '\\n' in rewritten:\n",
    "                rewritten = rewritten.split('\\n')[0]\n",
    "            if '.' in rewritten:\n",
    "                rewritten = rewritten.split('.')[0] + '.'\n",
    "    \n",
    "            return rewritten if rewritten else text\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"  修复版重写失败: {e}\")\n",
    "            return text\n",
    "\n",
    "    def _revise_with_api(self, text: str, temperature: float = 0.7, revision_style: str = \"standard\") -> str:\n",
    "        \"\"\"使用OpenAI API进行文本重写，包含重试和错误处理\"\"\"\n",
    "        if not self.client:\n",
    "            raise ValueError(\"OpenAI client not initialized.\")\n",
    "            \n",
    "        system_prompt = \"Please rewrite the following text while maintaining its meaning:\"\n",
    "        \n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.config.revision_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "                \n",
    "            except openai.RateLimitError:\n",
    "                print(f\"Rate limit exceeded, waiting {self.retry_delay}s...\")\n",
    "                time.sleep(self.retry_delay)\n",
    "                self.retry_delay *= 2\n",
    "            except openai.APIError as e:\n",
    "                print(f\"API error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(self.retry_delay)\n",
    "                else:\n",
    "                    return text # 返回原文作为后备\n",
    "            except Exception as e:\n",
    "                print(f\"Unexpected API error: {e}\")\n",
    "                return text # 返回原文作为后备\n",
    "        return text # 所有重试失败后返回原文\n",
    "\n",
    "    def _rule_based_revision(self, text: str) -> str:\n",
    "        \"\"\"基于规则的重写\"\"\"\n",
    "        import re\n",
    "\n",
    "        # 句子重排\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) > 2:\n",
    "            # 随机调整句子顺序（保持逻辑性）\n",
    "            sentences = self._reorder_sentences(sentences)\n",
    "\n",
    "        # 同义词替换\n",
    "        revised_sentences = []\n",
    "        for sent in sentences:\n",
    "            # 简单的同义词映射\n",
    "            synonyms = {\n",
    "                \"important\": \"significant\",\n",
    "                \"however\": \"nevertheless\",\n",
    "                \"therefore\": \"thus\",\n",
    "                \"because\": \"since\",\n",
    "                \"many\": \"numerous\",\n",
    "                \"show\": \"demonstrate\"\n",
    "            }\n",
    "\n",
    "            for word, syn in synonyms.items():\n",
    "                sent = re.sub(r'\\b' + word + r'\\b', syn, sent, flags=re.IGNORECASE)\n",
    "\n",
    "            revised_sentences.append(sent)\n",
    "\n",
    "        return ' '.join(revised_sentences)\n",
    "\n",
    "    def _reorder_sentences(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"智能句子重排\"\"\"\n",
    "        # 保持第一句和最后一句\n",
    "        if len(sentences) <= 2:\n",
    "            return sentences\n",
    "\n",
    "        first = sentences[0]\n",
    "        last = sentences[-1]\n",
    "        middle = sentences[1:-1]\n",
    "\n",
    "        # 打乱中间句子\n",
    "        np.random.shuffle(middle)\n",
    "\n",
    "        return [first] + middle + [last]\n",
    "\n",
    "class MultiFeatureExtractor:\n",
    "    \"\"\"多维度特征提取器\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.sentence_model = SentenceTransformer(config.embedding_model)\n",
    "        self._init_linguistic_features()\n",
    "\n",
    "    def _init_linguistic_features(self):\n",
    "        \"\"\"初始化语言学特征提取\"\"\"\n",
    "        try:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except:\n",
    "            self.stop_words = set()\n",
    "\n",
    "    def extract_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        提取多维度特征\n",
    "\n",
    "        Returns:\n",
    "            特征字典\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 1. 语义相似度\n",
    "        features['semantic_similarity'] = self._compute_semantic_similarity(original, revised)\n",
    "\n",
    "        # 2. 词汇级特征\n",
    "        features.update(self._extract_lexical_features(original, revised))\n",
    "\n",
    "        # 3. 句法特征\n",
    "        features.update(self._extract_syntactic_features(original, revised))\n",
    "\n",
    "        # 4. 风格特征\n",
    "        features.update(self._extract_stylistic_features(original, revised))\n",
    "\n",
    "        # 5. 编辑距离特征\n",
    "        features.update(self._extract_edit_features(original, revised))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _compute_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"计算语义相似度\"\"\"\n",
    "        emb1 = self.sentence_model.encode(text1)\n",
    "        emb2 = self.sentence_model.encode(text2)\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        return float(cosine_similarity([emb1], [emb2])[0, 0])\n",
    "\n",
    "    def _extract_lexical_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"提取词汇级特征\"\"\"\n",
    "        orig_words = set(word_tokenize(original.lower()))\n",
    "        rev_words = set(word_tokenize(revised.lower()))\n",
    "\n",
    "        # 词汇重叠率\n",
    "        overlap = len(orig_words & rev_words)\n",
    "        total = len(orig_words | rev_words)\n",
    "\n",
    "        features = {\n",
    "            'word_overlap_ratio': overlap / total if total > 0 else 0,\n",
    "            'vocabulary_change_ratio': len(orig_words ^ rev_words) / total if total > 0 else 0,\n",
    "            'unique_words_ratio': len(rev_words - orig_words) / len(rev_words) if rev_words else 0\n",
    "        }\n",
    "\n",
    "        # 停用词比例变化\n",
    "        orig_stop = len([w for w in orig_words if w in self.stop_words])\n",
    "        rev_stop = len([w for w in rev_words if w in self.stop_words])\n",
    "\n",
    "        features['stopword_ratio_change'] = abs(\n",
    "            (orig_stop / len(orig_words) if orig_words else 0) -\n",
    "            (rev_stop / len(rev_words) if rev_words else 0)\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_syntactic_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"提取句法特征\"\"\"\n",
    "        orig_sents = sent_tokenize(original)\n",
    "        rev_sents = sent_tokenize(revised)\n",
    "\n",
    "        features = {\n",
    "            'sentence_count_ratio': len(rev_sents) / len(orig_sents) if orig_sents else 1,\n",
    "            'avg_sentence_length_change': abs(\n",
    "                np.mean([len(word_tokenize(s)) for s in orig_sents]) -\n",
    "                np.mean([len(word_tokenize(s)) for s in rev_sents])\n",
    "            ) if orig_sents and rev_sents else 0\n",
    "        }\n",
    "\n",
    "        # POS标签分布变化\n",
    "        try:\n",
    "            orig_pos = nltk.pos_tag(word_tokenize(original))\n",
    "            rev_pos = nltk.pos_tag(word_tokenize(revised))\n",
    "\n",
    "            # 计算主要词性比例变化\n",
    "            for pos_type in ['NN', 'VB', 'JJ', 'RB']:  # 名词、动词、形容词、副词\n",
    "                orig_count = sum(1 for _, pos in orig_pos if pos.startswith(pos_type))\n",
    "                rev_count = sum(1 for _, pos in rev_pos if pos.startswith(pos_type))\n",
    "\n",
    "                features[f'pos_{pos_type}_ratio_change'] = abs(\n",
    "                    (orig_count / len(orig_pos) if orig_pos else 0) -\n",
    "                    (rev_count / len(rev_pos) if rev_pos else 0)\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_stylistic_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"提取风格特征\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 标点符号使用变化\n",
    "        orig_punct = sum(1 for c in original if c in '.,!?;:')\n",
    "        rev_punct = sum(1 for c in revised if c in '.,!?;:')\n",
    "\n",
    "        features['punctuation_ratio_change'] = abs(\n",
    "            (orig_punct / len(original) if original else 0) -\n",
    "            (rev_punct / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # 大写字母比例变化\n",
    "        orig_caps = sum(1 for c in original if c.isupper())\n",
    "        rev_caps = sum(1 for c in revised if c.isupper())\n",
    "\n",
    "        features['capitalization_ratio_change'] = abs(\n",
    "            (orig_caps / len(original) if original else 0) -\n",
    "            (rev_caps / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # 平均词长变化\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "\n",
    "        features['avg_word_length_change'] = abs(\n",
    "            np.mean([len(w) for w in orig_words]) -\n",
    "            np.mean([len(w) for w in rev_words])\n",
    "        ) if orig_words and rev_words else 0\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_edit_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"提取编辑距离相关特征\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "\n",
    "        # 字符级相似度\n",
    "        char_similarity = SequenceMatcher(None, original, revised).ratio()\n",
    "\n",
    "        # 词级相似度\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "        word_similarity = SequenceMatcher(None, orig_words, rev_words).ratio()\n",
    "\n",
    "        features = {\n",
    "            'char_level_similarity': char_similarity,\n",
    "            'word_level_similarity': word_similarity,\n",
    "            'length_ratio': len(revised) / len(original) if original else 1\n",
    "        }\n",
    "\n",
    "        return features\n",
    "\n",
    "class AITextDetector:\n",
    "    \"\"\"主检测器类\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig = None):\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.perturber = EnhancedTextPerturber(self.config)\n",
    "        self.reviser = EnhancedLLMReviser(self.config)\n",
    "        self.feature_extractor = MultiFeatureExtractor(self.config)\n",
    "        self.classifier = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cache = {}\n",
    "\n",
    "    def detect_batch(self, texts: List[str],\n",
    "                     labels: Optional[List[int]] = None) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        批量检测文本\n",
    "\n",
    "        Args:\n",
    "            texts: 文本列表\n",
    "            labels: 真实标签（可选，用于训练）\n",
    "        Returns:\n",
    "            包含预测结果和特征的字典\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        all_scores = []\n",
    "\n",
    "        print(\"Processing texts...\")\n",
    "        for i, text in enumerate(tqdm(texts)):\n",
    "            try:\n",
    "                # 1. 扰动\n",
    "                perturbed = self.perturber.perturb(text)\n",
    "\n",
    "                # 2. LLM重写\n",
    "                revised = self.reviser.revise(perturbed, self.cache)\n",
    "\n",
    "                # 3. 特征提取\n",
    "                features = self.feature_extractor.extract_features(text, revised)\n",
    "                all_features.append(features)\n",
    "\n",
    "                # 4. 主要相似度分数\n",
    "                all_scores.append(features['semantic_similarity'])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing text {i}: {e}\")\n",
    "                # 使用默认值\n",
    "                default_features = {key: 0.5 for key in ['semantic_similarity', 'word_overlap_ratio',\n",
    "                                                         'vocabulary_change_ratio', 'unique_words_ratio',\n",
    "                                                         'stopword_ratio_change', 'sentence_count_ratio',\n",
    "                                                         'avg_sentence_length_change', 'punctuation_ratio_change',\n",
    "                                                         'capitalization_ratio_change', 'avg_word_length_change',\n",
    "                                                         'char_level_similarity', 'word_level_similarity',\n",
    "                                                         'length_ratio']}\n",
    "                all_features.append(default_features)\n",
    "                all_scores.append(0.5)\n",
    "\n",
    "        # 转换为特征矩阵\n",
    "        feature_matrix = pd.DataFrame(all_features).fillna(0).values\n",
    "\n",
    "        # 如果使用ML分类器\n",
    "        if self.config.use_ml_classifier:\n",
    "            if labels is not None and self.classifier is None:\n",
    "                # 训练分类器\n",
    "                self._train_classifier(feature_matrix, labels)\n",
    "\n",
    "            if self.classifier is not None:\n",
    "                # 标准化特征\n",
    "                feature_matrix_scaled = self.scaler.transform(feature_matrix)\n",
    "                # 预测\n",
    "                predictions = self.classifier.predict(feature_matrix_scaled)\n",
    "                probabilities = self.classifier.predict_proba(feature_matrix_scaled)[:, 1]\n",
    "            else:\n",
    "                # 使用阈值方法\n",
    "                predictions = (np.array(all_scores) > self.config.similarity_threshold).astype(int)\n",
    "                probabilities = np.array(all_scores)\n",
    "        else:\n",
    "            # 仅使用阈值\n",
    "            predictions = (np.array(all_scores) > self.config.similarity_threshold).astype(int)\n",
    "            probabilities = np.array(all_scores)\n",
    "\n",
    "        return {\n",
    "            'predictions': predictions,\n",
    "            'probabilities': probabilities,\n",
    "            'features': feature_matrix,\n",
    "            'similarity_scores': np.array(all_scores)\n",
    "        }\n",
    "    \n",
    "    def _train_classifier(self, features: np.ndarray, labels: List[int]):\n",
    "        \"\"\"训练机器学习分类器\"\"\"\n",
    "        print(\"Training classifier...\")\n",
    "\n",
    "        # 标准化特征\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "\n",
    "        # 使用随机森林\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        self.classifier.fit(features_scaled, labels)\n",
    "\n",
    "        # 特征重要性\n",
    "        feature_names = list(pd.DataFrame(features).columns)\n",
    "        importances = self.classifier.feature_importances_\n",
    "\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        for feat, imp in sorted(zip(feature_names, importances),\n",
    "                               key=lambda x: x[1], reverse=True)[:10]:\n",
    "            print(f\"  {feat}: {imp:.4f}\")\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"保存模型\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        model_data = {\n",
    "            'classifier': self.classifier,\n",
    "            'scaler': self.scaler,\n",
    "            'config': self.config,\n",
    "            'cache': self.cache\n",
    "        }\n",
    "\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"加载模型\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        with open(path, 'rb') as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.classifier = model_data['classifier']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.config = model_data['config']\n",
    "        self.cache = model_data['cache']\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "        \n",
    "        \n",
    "# ------------------------------------------------------------\n",
    "# 数据集加载函数（从骨架代码整合）\n",
    "# ------------------------------------------------------------\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    读取带有 'text' 与 'label' 列的数据集，并返回 DataFrame。\n",
    "    支持金融数据集格式和通用格式。\n",
    "\n",
    "    Args:\n",
    "        path: 数据文件路径、'finance' 表示使用金融数据集、'sample' 表示创建示例数据\n",
    "    Returns:\n",
    "        df: columns = ['id', 'text', 'label']\n",
    "    \"\"\"\n",
    "    if path == 'finance':\n",
    "        return load_finance_dataset()\n",
    "    elif path == 'sample':\n",
    "        return create_sample_dataset()\n",
    "\n",
    "    try:\n",
    "        if path.endswith('.csv'):\n",
    "            df = pd.read_csv(path)\n",
    "        elif path.endswith('.jsonl'):\n",
    "            df = pd.read_json(path, lines=True)\n",
    "        elif path.endswith('.json'):\n",
    "            df = pd.read_json(path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file format: {path}\")\n",
    "\n",
    "        # 确保有必要的列\n",
    "        if 'text' not in df.columns:\n",
    "            raise ValueError(\"Missing 'text' column in dataset\")\n",
    "        if 'label' not in df.columns:\n",
    "            raise ValueError(\"Missing 'label' column in dataset\")\n",
    "\n",
    "        # 如果没有id列，自动生成\n",
    "        if 'id' not in df.columns:\n",
    "            df['id'] = range(len(df))\n",
    "\n",
    "        # 数据清洗\n",
    "        df = df.dropna(subset=['text', 'label'])\n",
    "        df = df[df['text'].str.len() > 10]  # 过滤太短的文本\n",
    "\n",
    "        print(f\"Loaded {len(df)} samples from {path}\")\n",
    "        print(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "        return df[['id', 'text', 'label']]\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {path} not found. Creating sample dataset...\")\n",
    "        return create_sample_dataset()\n",
    "\n",
    "\n",
    "def find_finance_data_files():\n",
    "    \"\"\"查找金融数据集文件\"\"\"\n",
    "    print(\"Searching for finance data files...\")\n",
    "\n",
    "    # Kaggle环境路径\n",
    "    possible_paths = [\n",
    "        \"/kaggle/input/finance-dataset\",\n",
    "        \"/kaggle/input/finance-data\",\n",
    "        \"/kaggle/input/human-chatgpt-finance\",\n",
    "        \"/kaggle/input\",\n",
    "        \"./data\",  # 本地路径\n",
    "        \".\"\n",
    "    ]\n",
    "\n",
    "    files_found = {}\n",
    "\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"Checking path: {path}\")\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                for file in files:\n",
    "                    if 'revised_human_finance' in file or 'revised_human' in file:\n",
    "                        files_found['revised_human'] = os.path.join(root, file)\n",
    "                    elif 'revised_chatgpt_finance' in file or 'revised_chatgpt' in file:\n",
    "                        files_found['revised_chatgpt'] = os.path.join(root, file)\n",
    "                    elif 'finance.jsonl' in file:\n",
    "                        files_found['original'] = os.path.join(root, file)\n",
    "\n",
    "    return files_found\n",
    "def load_finance_dataset() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    加载金融数据集 - 使用修订后的文本作为数据\n",
    "    修复了多行JSON解析问题\n",
    "    \n",
    "    Returns:\n",
    "        df: columns = ['id', 'text', 'label']\n",
    "        其中text是修订后的文本，label: 0=human, 1=chatgpt\n",
    "    \"\"\"\n",
    "    files = find_finance_data_files()\n",
    "    \n",
    "    print(\"Loading finance datasets...\")\n",
    "    print(f\"Found files: {list(files.keys())}\")\n",
    "    \n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 1. 加载修订后的人类文本（每行一个JSON对象）\n",
    "    if 'revised_human' in files:\n",
    "        try:\n",
    "            print(f\"Loading revised human texts...\")\n",
    "            with open(files['revised_human'], 'r', encoding='utf-8') as f:\n",
    "                for line_no, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        try:\n",
    "                            # 每行是一个独立的JSON对象\n",
    "                            item = json.loads(line)\n",
    "                            for idx, text in item.items():\n",
    "                                # 清理文本\n",
    "                                cleaned_text = text.strip().replace('\\\\n\\\\n', ' ').replace('\\\\n', ' ')\n",
    "                                if len(cleaned_text) > 20:\n",
    "                                    all_texts.append(cleaned_text)\n",
    "                                    all_labels.append(0)  # 0 = human\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"  Line {line_no} parse error: {e}\")\n",
    "            print(f\"  Loaded {len([l for l in all_labels if l == 0])} human texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading revised human texts: {e}\")\n",
    "    \n",
    "    # 2. 加载修订后的ChatGPT文本（每行一个JSON对象）\n",
    "    if 'revised_chatgpt' in files:\n",
    "        try:\n",
    "            print(f\"Loading revised ChatGPT texts...\")\n",
    "            with open(files['revised_chatgpt'], 'r', encoding='utf-8') as f:\n",
    "                for line_no, line in enumerate(f, 1):\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        try:\n",
    "                            # 每行是一个独立的JSON对象\n",
    "                            item = json.loads(line)\n",
    "                            for idx, text in item.items():\n",
    "                                # 清理文本\n",
    "                                cleaned_text = text.strip().replace('\\\\n\\\\n', ' ').replace('\\\\n', ' ')\n",
    "                                if len(cleaned_text) > 20:\n",
    "                                    all_texts.append(cleaned_text)\n",
    "                                    all_labels.append(1)  # 1 = chatgpt\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"  Line {line_no} parse error: {e}\")\n",
    "            print(f\"  Loaded {len([l for l in all_labels if l == 1])} ChatGPT texts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading revised ChatGPT texts: {e}\")\n",
    "    \n",
    "    # 3. 如果没有数据，尝试原始文件或创建示例\n",
    "    if len(all_texts) == 0:\n",
    "        if 'original' in files:\n",
    "            print(\"\\nNo revised texts found. Loading from original finance.jsonl...\")\n",
    "            try:\n",
    "                with open(files['original'], encoding=\"utf-8\") as f:\n",
    "                    for row in f:\n",
    "                        if row.strip():\n",
    "                            data = json.loads(row)\n",
    "                            # 人类答案\n",
    "                            human_answers = data.get(\"human_answers\", [])\n",
    "                            if human_answers and human_answers[0]:\n",
    "                                all_texts.append(human_answers[0])\n",
    "                                all_labels.append(0)\n",
    "                            # ChatGPT答案\n",
    "                            chatgpt_answers = data.get(\"chatgpt_answers\", [])\n",
    "                            if chatgpt_answers and chatgpt_answers[0]:\n",
    "                                all_texts.append(chatgpt_answers[0])\n",
    "                                all_labels.append(1)\n",
    "                print(f\"  Loaded {len(all_texts)} texts from original file\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading original file: {e}\")\n",
    "        \n",
    "        if len(all_texts) == 0:\n",
    "            print(\"Creating sample dataset...\")\n",
    "            return create_sample_dataset()\n",
    "    \n",
    "    # 4. 创建DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'id': range(len(all_texts)),\n",
    "        'text': all_texts,\n",
    "        'label': all_labels\n",
    "    })\n",
    "    \n",
    "    # 打乱数据\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    df['id'] = range(len(df))\n",
    "    \n",
    "    print(f\"\\n=== Dataset Summary ===\")\n",
    "    print(f\"Total samples: {len(df)}\")\n",
    "    print(f\"Human texts: {len(df[df['label'] == 0])}\")\n",
    "    print(f\"ChatGPT texts: {len(df[df['label'] == 1])}\")\n",
    "    \n",
    "    # 显示示例\n",
    "    if len(df) > 0:\n",
    "        print(\"\\nSample texts:\")\n",
    "        for label in [0, 1]:\n",
    "            label_name = \"Human\" if label == 0 else \"ChatGPT\"\n",
    "            samples = df[df['label'] == label]\n",
    "            if len(samples) > 0:\n",
    "                sample_text = samples.iloc[0]['text']\n",
    "                print(f\"\\n[{label_name}]: {sample_text[:100]}...\")\n",
    "    \n",
    "    return df[['id', 'text', 'label']]\n",
    "\n",
    "def create_sample_dataset(n_samples: int = 100) -> pd.DataFrame:\n",
    "    \"\"\"创建示例数据集用于测试\"\"\"\n",
    "    import random\n",
    "\n",
    "    human_texts = [\n",
    "        \"I think artificial intelligence will revolutionize many industries in the coming decades.\",\n",
    "        \"The weather today is quite pleasant, perfect for a walk in the park.\",\n",
    "        \"Machine learning algorithms have shown remarkable progress in recent years.\",\n",
    "        \"I love reading books, especially mystery novels and science fiction.\",\n",
    "        \"Climate change is one of the most pressing issues facing humanity today.\",\n",
    "        \"Just finished my morning coffee, feeling ready to tackle the day!\",\n",
    "        \"My cat knocked over my plant again. Why do they always do that?\",\n",
    "        \"Been trying to learn guitar but my fingers hurt so much. Is this normal?\",\n",
    "        \"The new restaurant downtown is pretty good but a bit pricey for what you get.\",\n",
    "        \"I can't believe how fast this year has gone by. Feels like January was yesterday.\"\n",
    "    ] * (n_samples // 20)\n",
    "\n",
    "    ai_texts = [\n",
    "        \"Artificial intelligence represents a paradigm shift in technological advancement, offering unprecedented opportunities for innovation across multiple sectors.\",\n",
    "        \"The meteorological conditions today present optimal parameters for outdoor recreational activities.\",\n",
    "        \"Contemporary machine learning methodologies demonstrate significant improvements in performance metrics.\",\n",
    "        \"Literary consumption, particularly within the mystery and science fiction genres, provides considerable intellectual stimulation.\",\n",
    "        \"Climate change constitutes a critical global challenge requiring immediate and comprehensive action.\",\n",
    "        \"The implementation of blockchain technology has demonstrated substantial potential for revolutionizing financial systems.\",\n",
    "        \"Quantum computing represents a fundamental breakthrough in computational capabilities with far-reaching implications.\",\n",
    "        \"The integration of renewable energy sources is essential for achieving sustainable development goals.\",\n",
    "        \"Advancements in biotechnology continue to push the boundaries of medical treatment possibilities.\",\n",
    "        \"The digital transformation of industries necessitates comprehensive adaptation strategies for organizational success.\"\n",
    "    ] * (n_samples // 20)\n",
    "\n",
    "    texts = human_texts[:n_samples//2] + ai_texts[:n_samples//2]\n",
    "    labels = [0] * (n_samples//2) + [1] * (n_samples//2)\n",
    "\n",
    "    # 随机打乱\n",
    "    combined = list(zip(texts, labels))\n",
    "    random.shuffle(combined)\n",
    "    texts, labels = zip(*combined)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'id': range(len(texts)),\n",
    "        'text': texts,\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_detector(detector: AITextDetector,\n",
    "                     texts: List[str],\n",
    "                     labels: List[int]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    评估检测器性能\n",
    "    \"\"\"\n",
    "    results = detector.detect_batch(texts, labels)\n",
    "\n",
    "    predictions = results['predictions']\n",
    "    probabilities = results['probabilities']\n",
    "\n",
    "    # 计算指标\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'auc': roc_auc_score(labels, probabilities),\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predictions,\n",
    "                              target_names=['Human', 'AI']))\n",
    "\n",
    "    # 分析相似度分布\n",
    "    human_scores = results['similarity_scores'][np.array(labels) == 0]\n",
    "    ai_scores = results['similarity_scores'][np.array(labels) == 1]\n",
    "\n",
    "    print(f\"\\nSimilarity Score Distribution:\")\n",
    "    print(f\"Human texts - Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\")\n",
    "    print(f\"AI texts - Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# 主执行函数\n",
    "def main(data_path: str = \"finance\",\n",
    "         max_samples: Optional[int] = None,\n",
    "         use_cache: bool = True,\n",
    "         config: Optional[DetectionConfig] = None):\n",
    "    \"\"\"\n",
    "    主执行函数 - 使用增强版检测器\n",
    "\n",
    "    Args:\n",
    "        data_path: 数据文件路径或 'finance'/'sample'\n",
    "        max_samples: 限制处理的样本数量（用于快速测试）\n",
    "        use_cache: 是否使用缓存\n",
    "        config: 检测器配置\n",
    "    \"\"\"\n",
    "    print(\"=== Enhanced AI Text Detection ===\")\n",
    "    print(f\"Method: Multi-feature extraction with {config.revision_model if config else 'default model'}\")\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    df = load_data(data_path)\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Using first {max_samples} samples for testing\")\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['label'].tolist()\n",
    "\n",
    "    # 2. 数据分割（70% 训练，30% 测试）\n",
    "    split_idx = int(len(texts) * 0.4)\n",
    "    train_texts, test_texts = texts[:split_idx], texts[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "    print(f\"\\nDataset split: {len(train_texts)} training, {len(test_texts)} testing\")\n",
    "\n",
    "    # 3. 创建检测器\n",
    "    if config is None:\n",
    "        config = DetectionConfig(\n",
    "            revision_model=\"t5-small\",  # 或 \"gpt2\" t5-small\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True\n",
    "        )\n",
    "\n",
    "    detector = AITextDetector(config)\n",
    "\n",
    "    # 4. 加载缓存\n",
    "    cache_path = f\"enhanced_cache_{config.revision_model}.json\"\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        with open(cache_path, 'r') as f:\n",
    "            detector.cache = json.load(f)\n",
    "        print(f\"Loaded cache with {len(detector.cache)} entries\")\n",
    "\n",
    "    # 5. 训练阶段\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    print(\"Train labels distribution:\",\n",
    "      pd.Series(train_labels).value_counts().to_dict())\n",
    "    train_results = detector.detect_batch(train_texts, train_labels)\n",
    "\n",
    "    # 6. 测试阶段\n",
    "    print(\"\\n=== Testing Phase ===\")\n",
    "    test_results = detector.detect_batch(test_texts)\n",
    "\n",
    "    # 7. 评估结果\n",
    "    test_predictions = test_results['predictions']\n",
    "    test_probabilities = test_results['probabilities']\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # 计算指标\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    auc = roc_auc_score(test_labels, test_probabilities)\n",
    "\n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "\n",
    "    # 详细报告\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_labels, test_predictions,\n",
    "                              target_names=['Human', 'AI']))\n",
    "\n",
    "    # 混淆矩阵\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"            Human    AI\")\n",
    "    print(f\"Actual Human  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"       AI     {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "    # 特征分析\n",
    "    features_df = test_results['features']\n",
    "    similarity_scores = test_results['similarity_scores']\n",
    "\n",
    "    # 分组统计\n",
    "    human_scores = similarity_scores[np.array(test_labels) == 0]\n",
    "    ai_scores = similarity_scores[np.array(test_labels) == 1]\n",
    "\n",
    "    print(f\"\\n=== Similarity Score Analysis ===\")\n",
    "    print(f\"Human texts:\")\n",
    "    print(f\"  Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(human_scores):.3f}, Max: {np.max(human_scores):.3f}\")\n",
    "\n",
    "    print(f\"AI texts:\")\n",
    "    print(f\"  Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(ai_scores):.3f}, Max: {np.max(ai_scores):.3f}\")\n",
    "\n",
    "    # 显示错误分类的例子\n",
    "    print(\"\\n=== Misclassified Examples ===\")\n",
    "    misclassified_idx = np.where(test_predictions != test_labels)[0]\n",
    "\n",
    "    for i, idx in enumerate(misclassified_idx[:50]):  # 显示前5个\n",
    "        true_label = \"AI\" if test_labels[idx] == 1 else \"Human\"\n",
    "        pred_label = \"AI\" if test_predictions[idx] == 1 else \"Human\"\n",
    "        text = test_texts[idx][:100] + \"...\" if len(test_texts[idx]) > 100 else test_texts[idx]\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "        print(f\"Similarity: {similarity_scores[idx]:.3f}, Probability: {test_probabilities[idx]:.3f}\")\n",
    "\n",
    "    # 8. 保存缓存和模型\n",
    "    if use_cache:\n",
    "        with open(cache_path, 'w') as f:\n",
    "            json.dump(detector.cache, f)\n",
    "        print(f\"\\nCache saved to {cache_path}\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_path = f\"enhanced_detector_{config.revision_model}.pkl\"\n",
    "    detector.save_model(model_path)\n",
    "\n",
    "    return {\n",
    "        'detector': detector,\n",
    "        'test_results': test_results,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "# 实验对比函数\n",
    "def run_comparison_experiment(data_path: str = \"finance\", max_samples: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    运行对比实验，比较不同配置的效果\n",
    "    \"\"\"\n",
    "    print(\"=== Comparison Experiment ===\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # 配置1: T5-small\n",
    "    print(\"\\n1. Testing with T5-small...\")\n",
    "    config1 = DetectionConfig(\n",
    "        revision_model=\"t5-small\",\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        perturbation_rate=0.15,\n",
    "        use_ml_classifier=True\n",
    "    )\n",
    "    results['t5'] = main(data_path, max_samples, use_cache=True, config=config1)\n",
    "\n",
    "    # 配置2: GPT-2\n",
    "    print(\"\\n2. Testing with GPT-2...\")\n",
    "    config2 = DetectionConfig(\n",
    "        revision_model=\"gpt2\",\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        perturbation_rate=0.15,\n",
    "        use_ml_classifier=True\n",
    "    )\n",
    "    results['gpt2'] = main(data_path, max_samples, use_cache=True, config=config2)\n",
    "    \n",
    "    # 配置3: GPT-3.5-Turbo (API模型)\n",
    "    print(\"\\n3. Testing with GPT-3.5-Turbo API...\")\n",
    "    api_key_for_proxy = \"sk-RwjJjBq7VVTFvhv9352929B353Bb41D68d2f0959EcEe3b6f\"\n",
    "    base_url_for_proxy = \"https://api.bltcy.ai/v1\"\n",
    "    config3 = DetectionConfig(\n",
    "        revision_model=\"gpt-3.5-turbo\",\n",
    "        api_key=api_key_for_proxy,\n",
    "        base_url=base_url_for_proxy\n",
    "    )\n",
    "    results['gpt3.5'] = main(data_path, max_samples, use_cache=True, config=config3)\n",
    "\n",
    "    # 配置4: 不同扰动率\n",
    "    print(\"\\n3. Testing with higher perturbation rate...\")\n",
    "    config4 = DetectionConfig(\n",
    "        revision_model=\"t5-small\",\n",
    "        embedding_model=\"all-MiniLM-L6-v2\",\n",
    "        perturbation_rate=0.25,\n",
    "        perturbation_methods=[\"synonym\", \"contextual\", \"random_swap\"],\n",
    "        use_ml_classifier=True\n",
    "    )\n",
    "    results['high_perturb'] = main(data_path, max_samples, use_cache=True, config=config4)\n",
    "\n",
    "    # 比较结果\n",
    "    print(\"\\n=== Comparison Results ===\")\n",
    "    print(f\"{'Configuration':<20} {'Accuracy':<10} {'AUC':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"{'T5-small':<20} {results['t5']['accuracy']:<10.4f} {results['t5']['auc']:<10.4f}\")\n",
    "    print(f\"{'GPT-2':<20} {results['gpt2']['accuracy']:<10.4f} {results['gpt2']['auc']:<10.4f}\")\n",
    "    print(f\"{'High Perturbation':<20} {results['high_perturb']['accuracy']:<10.4f} {results['high_perturb']['auc']:<10.4f}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from openai import OpenAI\n",
    "\n",
    "@dataclass\n",
    "class RevisedTextEntry:\n",
    "    \"\"\"Data structure for storing original and revised text pairs\"\"\"\n",
    "    text_id: int\n",
    "    original_text: str\n",
    "    revised_text: str\n",
    "    text_type: str  # 'human' or 'ai'\n",
    "    revision_model: str\n",
    "    timestamp: str\n",
    "    \n",
    "class TextRevisionPreprocessor:\n",
    "    \"\"\"Preprocess and cache all text revisions to avoid repeated API calls\"\"\"\n",
    "    \n",
    "    def __init__(self, config, output_dir: str = \"./preprocessed_data\"):\n",
    "        self.config = config\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            api_key=config.api_key,\n",
    "            base_url=config.base_url\n",
    "        )\n",
    "        \n",
    "        # Paths for different files\n",
    "        self.human_original_path = os.path.join(output_dir, \"human_original_texts.json\")\n",
    "        self.human_revised_path = os.path.join(output_dir, \"human_revised_texts.json\")\n",
    "        self.ai_original_path = os.path.join(output_dir, \"ai_original_texts.json\")\n",
    "        self.ai_revised_path = os.path.join(output_dir, \"ai_revised_texts.json\")\n",
    "        self.progress_path = os.path.join(output_dir, \"processing_progress.json\")\n",
    "        self.combined_path = os.path.join(output_dir, \"combined_dataset.pkl\")\n",
    "        \n",
    "    def load_progress(self) -> Dict:\n",
    "        \"\"\"Load processing progress for resumption\"\"\"\n",
    "        if os.path.exists(self.progress_path):\n",
    "            with open(self.progress_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {\n",
    "            'human_processed': [],\n",
    "            'ai_processed': [],\n",
    "            'last_update': None\n",
    "        }\n",
    "    \n",
    "    def save_progress(self, progress: Dict):\n",
    "        \"\"\"Save processing progress\"\"\"\n",
    "        progress['last_update'] = time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        with open(self.progress_path, 'w') as f:\n",
    "            json.dump(progress, f, indent=2)\n",
    "    \n",
    "    def revise_text_with_retry(self, text: str, max_retries: int = 3) -> str:\n",
    "        \"\"\"Revise text with retry logic\"\"\"\n",
    "        system_prompt = \"\"\"You are a professional editor. Your task is to rewrite the given text to make it more formal, standardized, and academic in style. \n",
    "\n",
    "Key guidelines:\n",
    "- Replace casual expressions with formal language\n",
    "- Use precise technical terms where appropriate\n",
    "- Maintain professional tone throughout\n",
    "- Standardize sentence structure\n",
    "- Remove colloquialisms and personal expressions\n",
    "- Keep the core meaning but express it in a more refined way\n",
    "\n",
    "The more casual or informal the original text, the more changes you should make.\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.config.revision_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": f\"Please rewrite this text in a formal, academic style:\\n\\n{text}\"}\n",
    "                    ],\n",
    "                    temperature=0.3,\n",
    "                    top_p=0.9\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"API error (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                else:\n",
    "                    print(f\"Failed to revise text after {max_retries} attempts\")\n",
    "                    return text  # Return original text as fallback\n",
    "    \n",
    "    def process_dataset(self, df: pd.DataFrame, batch_size: int = 10, use_multithread: bool = False):\n",
    "        \"\"\"Process entire dataset and save revised texts\"\"\"\n",
    "        print(f\"=== Processing Dataset ===\")\n",
    "        print(f\"Total samples: {len(df)}\")\n",
    "        \n",
    "        # Load existing progress\n",
    "        progress = self.load_progress()\n",
    "        \n",
    "        # Separate human and AI texts\n",
    "        human_df = df[df['label'] == 0].copy()\n",
    "        ai_df = df[df['label'] == 1].copy()\n",
    "        \n",
    "        print(f\"Human texts: {len(human_df)}\")\n",
    "        print(f\"AI texts: {len(ai_df)}\")\n",
    "        \n",
    "        # Process human texts\n",
    "        print(\"\\n--- Processing Human Texts ---\")\n",
    "        human_data = self._process_text_group(\n",
    "            human_df, \n",
    "            'human',\n",
    "            progress['human_processed'],\n",
    "            batch_size,\n",
    "            use_multithread\n",
    "        )\n",
    "        \n",
    "        # Process AI texts\n",
    "        print(\"\\n--- Processing AI Texts ---\")\n",
    "        ai_data = self._process_text_group(\n",
    "            ai_df,\n",
    "            'ai', \n",
    "            progress['ai_processed'],\n",
    "            batch_size,\n",
    "            use_multithread\n",
    "        )\n",
    "        \n",
    "        # Save all data\n",
    "        self._save_all_data(human_data, ai_data)\n",
    "        \n",
    "        print(\"\\n=== Processing Complete ===\")\n",
    "        print(f\"Data saved to: {self.output_dir}\")\n",
    "        \n",
    "        return human_data, ai_data\n",
    "    \n",
    "    def _process_text_group(self, df: pd.DataFrame, text_type: str, \n",
    "                           processed_ids: List[int], batch_size: int,\n",
    "                           use_multithread: bool) -> Dict:\n",
    "        \"\"\"Process a group of texts (human or AI)\"\"\"\n",
    "        data = {\n",
    "            'ids': [],\n",
    "            'original_texts': [],\n",
    "            'revised_texts': []\n",
    "        }\n",
    "        \n",
    "        # Filter out already processed texts\n",
    "        remaining_df = df[~df['id'].isin(processed_ids)]\n",
    "        print(f\"Already processed: {len(processed_ids)}\")\n",
    "        print(f\"Remaining to process: {len(remaining_df)}\")\n",
    "        \n",
    "        if len(remaining_df) == 0:\n",
    "            # Load existing data\n",
    "            if text_type == 'human':\n",
    "                data = self._load_existing_data(self.human_original_path, self.human_revised_path)\n",
    "            else:\n",
    "                data = self._load_existing_data(self.ai_original_path, self.ai_revised_path)\n",
    "            return data\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(remaining_df), batch_size), desc=f\"Processing {text_type} texts\"):\n",
    "            batch_df = remaining_df.iloc[i:i+batch_size]\n",
    "            \n",
    "            if use_multithread and batch_size > 1:\n",
    "                # Parallel processing\n",
    "                revised_texts = self._process_batch_parallel(batch_df['text'].tolist())\n",
    "            else:\n",
    "                # Sequential processing\n",
    "                revised_texts = []\n",
    "                for text in batch_df['text']:\n",
    "                    revised = self.revise_text_with_retry(text)\n",
    "                    revised_texts.append(revised)\n",
    "                    time.sleep(0.5)  # Rate limiting\n",
    "            \n",
    "            # Add to data\n",
    "            data['ids'].extend(batch_df['id'].tolist())\n",
    "            data['original_texts'].extend(batch_df['text'].tolist())\n",
    "            data['revised_texts'].extend(revised_texts)\n",
    "            \n",
    "            # Update progress\n",
    "            progress = self.load_progress()\n",
    "            progress[f'{text_type}_processed'].extend(batch_df['id'].tolist())\n",
    "            self.save_progress(progress)\n",
    "            \n",
    "            # Periodic save\n",
    "            if (i + batch_size) % 100 == 0:\n",
    "                self._save_intermediate_data(data, text_type)\n",
    "        \n",
    "        # Final save\n",
    "        self._save_intermediate_data(data, text_type)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _process_batch_parallel(self, texts: List[str], max_workers: int = 5) -> List[str]:\n",
    "        \"\"\"Process texts in parallel\"\"\"\n",
    "        revised_texts = [None] * len(texts)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            future_to_idx = {\n",
    "                executor.submit(self.revise_text_with_retry, text): idx \n",
    "                for idx, text in enumerate(texts)\n",
    "            }\n",
    "            \n",
    "            for future in as_completed(future_to_idx):\n",
    "                idx = future_to_idx[future]\n",
    "                try:\n",
    "                    revised_texts[idx] = future.result()\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing text {idx}: {e}\")\n",
    "                    revised_texts[idx] = texts[idx]  # Use original as fallback\n",
    "        \n",
    "        return revised_texts\n",
    "    \n",
    "    def _save_intermediate_data(self, data: Dict, text_type: str):\n",
    "        \"\"\"Save intermediate results\"\"\"\n",
    "        if text_type == 'human':\n",
    "            original_path = self.human_original_path\n",
    "            revised_path = self.human_revised_path\n",
    "        else:\n",
    "            original_path = self.ai_original_path\n",
    "            revised_path = self.ai_revised_path\n",
    "        \n",
    "        # Save as JSON for readability\n",
    "        with open(original_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'ids': data['ids'],\n",
    "                'texts': data['original_texts']\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        with open(revised_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'ids': data['ids'],\n",
    "                'texts': data['revised_texts']\n",
    "            }, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def _load_existing_data(self, original_path: str, revised_path: str) -> Dict:\n",
    "        \"\"\"Load existing preprocessed data\"\"\"\n",
    "        data = {'ids': [], 'original_texts': [], 'revised_texts': []}\n",
    "        \n",
    "        if os.path.exists(original_path) and os.path.exists(revised_path):\n",
    "            with open(original_path, 'r', encoding='utf-8') as f:\n",
    "                original_data = json.load(f)\n",
    "            with open(revised_path, 'r', encoding='utf-8') as f:\n",
    "                revised_data = json.load(f)\n",
    "            \n",
    "            data['ids'] = original_data['ids']\n",
    "            data['original_texts'] = original_data['texts']\n",
    "            data['revised_texts'] = revised_data['texts']\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _save_all_data(self, human_data: Dict, ai_data: Dict):\n",
    "        \"\"\"Save all data in various formats for easy access\"\"\"\n",
    "        # Create combined dataset\n",
    "        combined_data = {\n",
    "            'human': {\n",
    "                'ids': human_data['ids'],\n",
    "                'original': human_data['original_texts'],\n",
    "                'revised': human_data['revised_texts']\n",
    "            },\n",
    "            'ai': {\n",
    "                'ids': ai_data['ids'],\n",
    "                'original': ai_data['original_texts'],\n",
    "                'revised': ai_data['revised_texts']\n",
    "            },\n",
    "            'metadata': {\n",
    "                'revision_model': self.config.revision_model,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'total_human': len(human_data['ids']),\n",
    "                'total_ai': len(ai_data['ids'])\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Save as pickle for fast loading\n",
    "        with open(self.combined_path, 'wb') as f:\n",
    "            pickle.dump(combined_data, f)\n",
    "        \n",
    "        # Save summary\n",
    "        summary_path = os.path.join(self.output_dir, \"preprocessing_summary.txt\")\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(\"=== Preprocessing Summary ===\\n\\n\")\n",
    "            f.write(f\"Revision Model: {self.config.revision_model}\\n\")\n",
    "            f.write(f\"Timestamp: {combined_data['metadata']['timestamp']}\\n\")\n",
    "            f.write(f\"Total Human Texts: {combined_data['metadata']['total_human']}\\n\")\n",
    "            f.write(f\"Total AI Texts: {combined_data['metadata']['total_ai']}\\n\")\n",
    "            f.write(f\"\\nFiles created:\\n\")\n",
    "            f.write(f\"- {self.human_original_path}\\n\")\n",
    "            f.write(f\"- {self.human_revised_path}\\n\")\n",
    "            f.write(f\"- {self.ai_original_path}\\n\")\n",
    "            f.write(f\"- {self.ai_revised_path}\\n\")\n",
    "            f.write(f\"- {self.combined_path}\\n\")\n",
    "    \n",
    "    def load_preprocessed_data(self) -> Dict:\n",
    "        \"\"\"Load preprocessed data for training\"\"\"\n",
    "        if not os.path.exists(self.combined_path):\n",
    "            raise FileNotFoundError(f\"Preprocessed data not found at {self.combined_path}\")\n",
    "        \n",
    "        with open(self.combined_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        print(f\"Loaded preprocessed data:\")\n",
    "        print(f\"- Human texts: {len(data['human']['ids'])}\")\n",
    "        print(f\"- AI texts: {len(data['ai']['ids'])}\")\n",
    "        print(f\"- Revision model: {data['metadata']['revision_model']}\")\n",
    "        \n",
    "        return data\n",
    "\n",
    "def create_training_data_from_preprocessed(preprocessed_data: Dict, num_samples: int = None) -> List:\n",
    "    \"\"\"Create training samples from preprocessed data\"\"\"\n",
    "    from sentence_transformers import InputExample\n",
    "    \n",
    "    samples = []\n",
    "    \n",
    "    # Process human texts\n",
    "    human_data = preprocessed_data['human']\n",
    "    n_human = len(human_data['ids']) if num_samples is None else min(num_samples // 2, len(human_data['ids']))\n",
    "    \n",
    "    for i in range(n_human):\n",
    "        samples.append(InputExample(\n",
    "            texts=[human_data['original'][i], human_data['revised'][i]],\n",
    "            label=0.2  # Low similarity expected\n",
    "        ))\n",
    "    \n",
    "    # Process AI texts\n",
    "    ai_data = preprocessed_data['ai']\n",
    "    n_ai = len(ai_data['ids']) if num_samples is None else min(num_samples // 2, len(ai_data['ids']))\n",
    "    \n",
    "    for i in range(n_ai):\n",
    "        samples.append(InputExample(\n",
    "            texts=[ai_data['original'][i], ai_data['revised'][i]],\n",
    "            label=0.85  # High similarity expected\n",
    "        ))\n",
    "    \n",
    "    print(f\"Created {len(samples)} training samples\")\n",
    "    return samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution script that can be run in three modes:\n",
    "    - 'human': Process only human-written texts.\n",
    "    - 'ai': Process only AI-generated texts.\n",
    "    - 'combine': Combine the results after 'human' and 'ai' modes are complete.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    # NOTE: It's recommended to load API keys from environment variables for security.\n",
    "    config = DetectionConfig(\n",
    "        revision_model=\"gpt-3.5-turbo\",\n",
    "        api_key=\"sk-rnuw0ejSvret9oLc6GHTgvHjWHyGgnyrLUvrta3o8cN1nZpw\",\n",
    "        base_url=\"https://api.chatanywhere.com.cn/v1\"\n",
    "    )\n",
    "    preprocessor = TextRevisionPreprocessor(config)\n",
    "    \n",
    "    # --- Mode Selection from Command-Line Argument---\n",
    "    if len(sys.argv) < 2 or sys.argv[1] not in ['human', 'ai', 'combine']:\n",
    "        print(\"Usage: python <script_name>.py [human|ai|combine]\")\n",
    "        print(\"\\n  'human'  : Preprocess only human texts.\")\n",
    "        print(\"  'ai'     : Preprocess only AI texts.\")\n",
    "        print(\"  'combine': Combine the outputs after both 'human' and 'ai' runs are finished.\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "    mode = sys.argv[1]\n",
    "\n",
    "    # --- Logic for 'human' or 'ai' processing modes ---\n",
    "    if mode in ['human', 'ai']:\n",
    "        print(f\"--- Running in '{mode}' mode ---\")\n",
    "        \n",
    "        # Load the full dataset\n",
    "        print(\"Loading full dataset...\")\n",
    "        df = load_data(\"finance\")\n",
    "        \n",
    "        # Determine which part of the dataset to process\n",
    "        process_df = df[df['label'] == (0 if mode == 'human' else 1)].copy()\n",
    "        text_type = mode\n",
    "\n",
    "        # Load progress to allow for resumption\n",
    "        progress = preprocessor.load_progress()\n",
    "        processed_ids = progress.get(f'{text_type}_processed', [])\n",
    "\n",
    "        # Process the selected text group\n",
    "        preprocessor._process_text_group(\n",
    "            df=process_df,\n",
    "            text_type=text_type,\n",
    "            processed_ids=processed_ids,\n",
    "            batch_size=10,\n",
    "            use_multithread=True\n",
    "        )\n",
    "        print(f\"\\n--- '{mode}' mode finished successfully. ---\")\n",
    "        print(f\"Output files saved to directory: '{preprocessor.output_dir}'\")\n",
    "\n",
    "    # --- Logic for 'combine' mode ---\n",
    "    elif mode == 'combine':\n",
    "        print(\"--- Running in 'combine' mode ---\")\n",
    "        \n",
    "        try:\n",
    "            print(\"Loading preprocessed human data...\")\n",
    "            human_data = preprocessor._load_existing_data(\n",
    "                preprocessor.human_original_path, \n",
    "                preprocessor.human_revised_path\n",
    "            )\n",
    "            \n",
    "            print(\"Loading preprocessed AI data...\")\n",
    "            ai_data = preprocessor._load_existing_data(\n",
    "                preprocessor.ai_original_path, \n",
    "                preprocessor.ai_revised_path\n",
    "            )\n",
    "            \n",
    "            if not human_data['ids'] or not ai_data['ids']:\n",
    "                raise FileNotFoundError(\"One or both preprocessed data files are missing or empty.\")\n",
    "                \n",
    "            print(\"\\nCombining and saving final dataset...\")\n",
    "            preprocessor._save_all_data(human_data, ai_data)\n",
    "            \n",
    "            print(\"\\n--- 'combine' mode finished successfully. ---\")\n",
    "            print(f\"Final combined dataset saved to: {preprocessor.combined_path}\")\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(\"Please ensure you have run both 'human' and 'ai' modes successfully before running 'combine'.\")\n",
    "            sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.argv = ['ai_generated.ipynb','combine']\n",
    "\n",
    "# To process AI texts\n",
    "# sys.argv = ['your_script_name.py', 'ai']\n",
    "\n",
    "# To COMBINE the results after the other two are done\n",
    "#sys.argv = ['your_script_name.py', 'combine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running in 'combine' mode ---\n",
      "Loading preprocessed human data...\n",
      "Loading preprocessed AI data...\n",
      "\n",
      "Combining and saving final dataset...\n",
      "\n",
      "--- 'combine' mode finished successfully. ---\n",
      "Final combined dataset saved to: ./preprocessed_data\\combined_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7866415,
     "sourceId": 12468971,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
