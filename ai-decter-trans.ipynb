{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-13T17:02:44.463591Z",
     "iopub.status.busy": "2025-07-13T17:02:44.463299Z",
     "iopub.status.idle": "2025-07-13T17:04:03.963591Z",
     "shell.execute_reply": "2025-07-13T17:04:03.962909Z",
     "shell.execute_reply.started": "2025-07-13T17:02:44.463569Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading required NLTK data...\n",
      "NLTK data download completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\suzehao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Try to import, if it fails prompt to install\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pickle\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI library imports\n",
    "import asyncio\n",
    "import openai  # Good for catching error types like openai.RateLimitError\n",
    "from openai import OpenAI\n",
    "from openai import AsyncOpenAI\n",
    "import tiktoken\n",
    "\n",
    "# Basic libraries\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, time, hashlib, json, random\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report\n",
    "\n",
    "# Transformers\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Text Scrambler Library\n",
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "except ImportError:\n",
    "    print(\"nlpaug not found, installing...\")\n",
    "    os.system(\"pip install nlpaug\")\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.sentence as nas\n",
    "\n",
    "# Transformers and sentence-transformers\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# NLTK data download\n",
    "import nltk\n",
    "\n",
    "print(\"Downloading required NLTK data...\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "print(\"NLTK data download completed.\")\n",
    "import logging\n",
    "\n",
    "# Configuration log\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DetectionConfig:\n",
    "    \"\"\"Detecting Configuration Classes\"\"\"\n",
    "\n",
    "    revision_model: str = \"t5-small\"\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "    api_key: Optional[str] = field(default_factory=lambda: os.getenv(\"OPENAI_API_KEY\"))\n",
    "    base_url: Optional[str] = field(\n",
    "        default_factory=lambda: os.getenv(\"OPENAI_BASE_URL\")\n",
    "    )\n",
    "\n",
    "    perturbation_rate: float = 0.15\n",
    "\n",
    "    perturbation_methods: List[str] = field(\n",
    "        default_factory=lambda: [\"synonym\", \"contextual\"]\n",
    "    )\n",
    "\n",
    "    similarity_threshold: float = 0.95\n",
    "    use_ml_classifier: bool = True\n",
    "\n",
    "    batch_size: int = 16\n",
    "    max_length: int = 512\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.perturbation_methods is None:\n",
    "            self.perturbation_methods = [\"synonym\", \"contextual\", \"backtranslation\"]\n",
    "\n",
    "\n",
    "class EnhancedTextPerturber:\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self._init_augmenters()\n",
    "\n",
    "    def _init_augmenters(self):\n",
    "\n",
    "        self.augmenters = {\n",
    "            \"synonym\": naw.SynonymAug(\n",
    "                aug_src=\"wordnet\", aug_p=self.config.perturbation_rate\n",
    "            ),\n",
    "            \"contextual\": naw.ContextualWordEmbsAug(\n",
    "                model_path=\"distilbert-base-uncased\",\n",
    "                action=\"substitute\",\n",
    "                aug_p=self.config.perturbation_rate,\n",
    "            ),\n",
    "            \"random_swap\": naw.RandomWordAug(\n",
    "                action=\"swap\", aug_p=self.config.perturbation_rate\n",
    "            ),\n",
    "            \"spelling\": naw.SpellingAug(aug_p=self.config.perturbation_rate),\n",
    "        }\n",
    "\n",
    "    def perturb(self, text: str, method: Optional[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Intelligent Perturbation Strategies\n",
    "\n",
    "        Args.\n",
    "            text: original text\n",
    "            method: specified method, None is chosen randomly\n",
    "        Returns.\n",
    "            Scrambled text\n",
    "        \"\"\"\n",
    "        if method is None:\n",
    "            method = np.random.choice(self.config.perturbation_methods)\n",
    "\n",
    "        try:\n",
    "            if method == \"backtranslation\":\n",
    "                return self._backtranslate(text)\n",
    "            elif method in self.augmenters:\n",
    "                augmented = self.augmenters[method].augment(text)\n",
    "                return augmented[0] if isinstance(augmented, list) else augmented\n",
    "            else:\n",
    "\n",
    "                return self._mixed_perturbation(text)\n",
    "        except Exception as e:\n",
    "            print(f\"Perturbation failed: {e}\")\n",
    "            return self._simple_perturb(text)\n",
    "\n",
    "    def _mixed_perturbation(self, text: str) -> str:\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        perturbed_sentences = []\n",
    "\n",
    "        for sent in sentences:\n",
    "\n",
    "            method = np.random.choice(list(self.augmenters.keys()))\n",
    "            try:\n",
    "                perturbed = self.augmenters[method].augment(sent)\n",
    "                perturbed_sent = (\n",
    "                    perturbed[0] if isinstance(perturbed, list) else perturbed\n",
    "                )\n",
    "                perturbed_sentences.append(perturbed_sent)\n",
    "            except:\n",
    "                perturbed_sentences.append(sent)\n",
    "\n",
    "        return \" \".join(perturbed_sentences)\n",
    "\n",
    "    def _simple_perturb(self, text: str) -> str:\n",
    "\n",
    "        words = text.split()\n",
    "\n",
    "        num_changes = max(1, int(len(words) * self.config.perturbation_rate))\n",
    "        indices = np.random.choice(\n",
    "            range(len(words)), size=min(num_changes, len(words)), replace=False\n",
    "        )\n",
    "\n",
    "        for idx in indices:\n",
    "\n",
    "            word = words[idx]\n",
    "            if len(word) > 3:\n",
    "                words[idx] = word[:-1] + np.random.choice(list(\"aeiou\"))\n",
    "\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def _backtranslate(self, text: str) -> str:\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "class EnhancedLLMReviser:\n",
    "    \"\"\"Enhanced LLM Rewriter\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.client = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._init_reviser()\n",
    "\n",
    "    def _init_model(self):\n",
    "        model_name = self.config.revision_model\n",
    "        \"\"\"Initialising the rewrite model\"\"\"\n",
    "        if self.config.revision_model.startswith(\"t5\"):\n",
    "\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.config.revision_model)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(\n",
    "                self.config.revision_model\n",
    "            )\n",
    "        elif self.config.revision_model == \"gpt2\":\n",
    "            # GPT-2 as an option\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # Initialising the OpenAI API client\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"Please provide api_key in DetectionConfig or set the OPENAI_API_KEY environment variable.\"\n",
    "                )\n",
    "\n",
    "            self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\"\n",
    "            )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def _init_reviser(self):\n",
    "        \"\"\"Initialise rewrite model or API client according to configuration\"\"\"\n",
    "        model_name = self.config.revision_model\n",
    "\n",
    "        if model_name.startswith(\"t5\"):\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "            self.model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local T5 model loaded: {model_name}\")\n",
    "\n",
    "        elif model_name == \"gpt2\":\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.model.to(self.device).eval()\n",
    "            print(f\"Local GPT-2 model loaded: {model_name}\")\n",
    "\n",
    "        elif model_name.startswith(\"gpt-\"):\n",
    "            # Initialising the OpenAI API client\n",
    "            api_key = self.config.api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "            if not api_key:\n",
    "                raise ValueError(\n",
    "                    \"Please provide api_key in DetectionConfig or set the OPENAI_API_KEY environment variable.\"\n",
    "                )\n",
    "\n",
    "            # self.client = OpenAI(api_key=api_key, base_url=self.config.base_url)\n",
    "            self.client = AsyncOpenAI(\n",
    "                api_key=api_key, base_url=self.config.base_url\n",
    "            )  # Using Asynchronous Clients\n",
    "            self.max_retries = 3\n",
    "            self.retry_delay = 1.0\n",
    "            print(f\"OpenAI client initialized for model: {model_name}\")\n",
    "            if self.config.base_url:\n",
    "                print(f\"Using custom base URL: {self.config.base_url}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Warning: Unknown revision model '{model_name}'. Will use rule-based fallback.\"\n",
    "            )\n",
    "\n",
    "    # def revise(self, text: str, cache: Dict[str, str]) -> str:\n",
    "    async def revise(\n",
    "        self, original_text: str, perturbed_text: str, cache: Dict[str, str]\n",
    "    ) -> str:  # change to async def\n",
    "        \"\"\"\n",
    "        Rewriting Text with LLM\n",
    "\n",
    "        Args.\n",
    "            original_text: text to be rewritten\n",
    "            cache: Cached dictionary\n",
    "        Returns.\n",
    "            Rewritten text\n",
    "        \"\"\"\n",
    "        # 生成缓存键\n",
    "        cache_key = hashlib.md5(\n",
    "            f\"{original_text}_{self.config.revision_model}\".encode()\n",
    "        ).hexdigest()\n",
    "\n",
    "        # Check if the result for the original text is already in the cache\n",
    "        if cache_key in cache:\n",
    "            return cache[cache_key]\n",
    "\n",
    "        # print(f\"\\norigin text: {original_text}\")\n",
    "\n",
    "        try:\n",
    "            model_name = self.config.revision_model\n",
    "            revised = \"\"\n",
    "            if model_name.startswith(\"t5\") or model_name == \"gpt2\":\n",
    "                if model_name.startswith(\"t5\"):\n",
    "                    revised = self._revise_with_t5(perturbed_text)\n",
    "                    # print(f\"t5 rewrite: {revised}\")\n",
    "                else:  # gpt2\n",
    "                    revised = self._revise_with_gpt2(perturbed_text)\n",
    "                    # print(f\"gpt2 rewrite: {revised}\")\n",
    "            elif model_name.startswith(\"gpt-\"):\n",
    "                # api rewrite\n",
    "                # revised = self._revise_with_api(perturbed_text)\n",
    "                revised = await self._revise_with_api(perturbed_text)  # change to await\n",
    "                # print(f\"gpt3.5 rewrite: {revised}\")\n",
    "            else:\n",
    "                revised = self._rule_based_revision(perturbed_text)\n",
    "\n",
    "            cache[cache_key] = revised\n",
    "            return revised\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Revision failed with model {self.config.revision_model}: {e}\")\n",
    "            fallback_revision = self._rule_based_revision(perturbed_text)\n",
    "            cache[cache_key] = fallback_revision\n",
    "            return fallback_revision\n",
    "\n",
    "    def _revise_with_t5(self, text: str) -> str:\n",
    "        \"\"\"Rewrite using the T5 model\"\"\"\n",
    "\n",
    "        prompt = f\"paraphrase: {text}\"\n",
    "\n",
    "        inputs = self.tokenizer.encode(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512,  # Maximum length of input truncation\n",
    "            truncation=True,\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                max_length=256,\n",
    "                min_length=40,\n",
    "                repetition_penalty=1.2,\n",
    "            )\n",
    "\n",
    "        revised = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return revised\n",
    "\n",
    "    def _revise_with_gpt2(self, text: str) -> str:\n",
    "        \"\"\"Rewrite using the GPT-2 model\"\"\"\n",
    "        try:\n",
    "            prompt_templates = [\n",
    "                f\"Paraphrase the following text while keeping the same meaning: {text}\\n\\nParaphrased version:\",\n",
    "                f\"Rewrite this sentence in a different way: {text}\\n\\nRewritten:\",\n",
    "                f\"Express this differently: {text}\\n\\nAlternative expression:\",\n",
    "            ]\n",
    "            prompt = np.random.choice(prompt_templates)\n",
    "            inputs = self.tokenizer.encode(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                max_length=512,\n",
    "                truncation=True,\n",
    "                padding=True,  # Ensure padding is enabled to handle the mask\n",
    "            )\n",
    "            input_ids = inputs.input_ids.to(self.device)\n",
    "            attention_mask = inputs.attention_mask.to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    input_ids=input_ids,  # Pass input_ids explicitly\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=min(len(input_ids[0]) + 100, 1024),\n",
    "                    num_return_sequences=1,\n",
    "                    temperature=0.8,\n",
    "                    do_sample=True,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id,\n",
    "                    no_repeat_ngram_size=3,\n",
    "                    # early_stopping=True,\n",
    "                )\n",
    "\n",
    "            #\n",
    "            full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            rewritten = \"\"\n",
    "            for marker in [\n",
    "                \"Paraphrased version:\",\n",
    "                \"Rewritten:\",\n",
    "                \"Alternative expression:\",\n",
    "            ]:\n",
    "                if marker in full_text:\n",
    "                    rewritten = full_text.split(marker)[-1].strip()\n",
    "                    break\n",
    "\n",
    "            if not rewritten:\n",
    "                rewritten = full_text[len(prompt) :].strip()\n",
    "\n",
    "            if \"\\n\" in rewritten:\n",
    "                rewritten = rewritten.split(\"\\n\")[0]\n",
    "            if \".\" in rewritten:\n",
    "                rewritten = rewritten.split(\".\")[0] + \".\"\n",
    "\n",
    "            return rewritten if rewritten else text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Fixed Rewrite Failure: {e}\")\n",
    "            return text\n",
    "\n",
    "    # def _revise_with_api(self, text: str) -> str:\n",
    "    async def _revise_with_api(self, text: str) -> str:  # change to async def\n",
    "        \"\"\"Use OpenAI API with better prompting strategy\"\"\"\n",
    "        if not self.client:\n",
    "            raise ValueError(\"OpenAI client not initialized.\")\n",
    "\n",
    "        # Better prompt that encourages standardization\n",
    "        system_prompt = \"\"\"You are a professional editor. Your task is to rewrite the given text to make it more formal, standardized, and academic in style. \n",
    "\n",
    "Key guidelines:\n",
    "- Replace casual expressions with formal language\n",
    "- Use precise technical terms where appropriate\n",
    "- Maintain professional tone throughout\n",
    "- Standardize sentence structure\n",
    "- Remove colloquialisms and personal expressions\n",
    "- Keep the core meaning but express it in a more refined way\n",
    "\n",
    "The more casual or informal the original text, the more changes you should make.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Please rewrite this text in a formal, academic style:\\n\\n{text}\"\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = await self.client.chat.completions.create(  # change to await\n",
    "                    model=self.config.revision_model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_prompt},\n",
    "                    ],\n",
    "                    temperature=0.3,  # Lower temperature for more consistent output\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "                return response.choices[0].message.content.strip()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"API error (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    await asyncio.time.sleep(\n",
    "                        self.retry_delay\n",
    "                    )  # Using asynchronous sleep\n",
    "                else:\n",
    "                    return text\n",
    "\n",
    "        return text\n",
    "\n",
    "    def _rule_based_revision(self, text: str) -> str:\n",
    "\n",
    "        import re\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        if len(sentences) > 2:\n",
    "\n",
    "            sentences = self._reorder_sentences(sentences)\n",
    "\n",
    "        revised_sentences = []\n",
    "        for sent in sentences:\n",
    "\n",
    "            synonyms = {\n",
    "                \"important\": \"significant\",\n",
    "                \"however\": \"nevertheless\",\n",
    "                \"therefore\": \"thus\",\n",
    "                \"because\": \"since\",\n",
    "                \"many\": \"numerous\",\n",
    "                \"show\": \"demonstrate\",\n",
    "            }\n",
    "\n",
    "            for word, syn in synonyms.items():\n",
    "                sent = re.sub(r\"\\b\" + word + r\"\\b\", syn, sent, flags=re.IGNORECASE)\n",
    "\n",
    "            revised_sentences.append(sent)\n",
    "\n",
    "        return \" \".join(revised_sentences)\n",
    "\n",
    "    def _reorder_sentences(self, sentences: List[str]) -> List[str]:\n",
    "        \"\"\"Intelligent Sentence Rearrangement\"\"\"\n",
    "        # Keep the first and last sentences\n",
    "        if len(sentences) <= 2:\n",
    "            return sentences\n",
    "\n",
    "        first = sentences[0]\n",
    "        last = sentences[-1]\n",
    "        middle = sentences[1:-1]\n",
    "\n",
    "        # break up the middle of a sentence\n",
    "        np.random.shuffle(middle)\n",
    "\n",
    "        return [first] + middle + [last]\n",
    "\n",
    "\n",
    "class MultiFeatureExtractor:\n",
    "    \"\"\"Multi-dimensional feature extractor\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig):\n",
    "        self.config = config\n",
    "        self.sentence_model = SentenceTransformer(config.embedding_model)\n",
    "\n",
    "        if self.sentence_model.tokenizer.pad_token is None:\n",
    "            print(\n",
    "                f\"Warning: Tokenizer for '{config.embedding_model}' is missing a pad token. Setting it to eos_token.\"\n",
    "            )\n",
    "            self.sentence_model.tokenizer.pad_token = (\n",
    "                self.sentence_model.tokenizer.eos_token\n",
    "            )\n",
    "        # --- Add these lines to view parameters ---\n",
    "        print(f\"\\n--- Parameters for Embedding Model: {config.embedding_model} ---\")\n",
    "        # Print the model architecture (all layers)\n",
    "        print(self.sentence_model)\n",
    "\n",
    "        # Calculate and print the total number of trainable parameters\n",
    "        total_params = sum(\n",
    "            p.numel() for p in self.sentence_model.parameters() if p.requires_grad\n",
    "        )\n",
    "        print(f\"Total Trainable Parameters: {total_params:,}\")\n",
    "        self._init_linguistic_features()\n",
    "\n",
    "    def _init_linguistic_features(self):\n",
    "        \"\"\"Initialised Linguistic Feature Extraction\"\"\"\n",
    "        try:\n",
    "            nltk.download(\"punkt\", quiet=True)\n",
    "            nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "            nltk.download(\"stopwords\", quiet=True)\n",
    "            self.stop_words = set(stopwords.words(\"english\"))\n",
    "        except:\n",
    "            self.stop_words = set()\n",
    "\n",
    "    def extract_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Extract multi-dimensional features\n",
    "\n",
    "        Returns:\n",
    "            feature dictionary\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # 1. semantic similarity\n",
    "        features[\"semantic_similarity\"] = self._compute_semantic_similarity(\n",
    "            original, revised\n",
    "        )\n",
    "\n",
    "        # 2. lexical level feature\n",
    "        features.update(self._extract_lexical_features(original, revised))\n",
    "\n",
    "        # 3. Syntactic features\n",
    "        features.update(self._extract_syntactic_features(original, revised))\n",
    "\n",
    "        # 4. style\n",
    "        features.update(self._extract_stylistic_features(original, revised))\n",
    "\n",
    "        # 5. Edit Distance Characteristics\n",
    "        features.update(self._extract_edit_features(original, revised))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _compute_semantic_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculating semantic similarity\"\"\"\n",
    "        emb1 = self.sentence_model.encode(text1)\n",
    "        emb2 = self.sentence_model.encode(text2)\n",
    "\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        return float(cosine_similarity([emb1], [emb2])[0, 0])\n",
    "\n",
    "    def _extract_lexical_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Extraction of vocabulary-level features\"\"\"\n",
    "        orig_words = set(word_tokenize(original.lower()))\n",
    "        rev_words = set(word_tokenize(revised.lower()))\n",
    "\n",
    "        # overlap rate of vocabulary\n",
    "        overlap = len(orig_words & rev_words)\n",
    "        total = len(orig_words | rev_words)\n",
    "\n",
    "        features = {\n",
    "            \"word_overlap_ratio\": overlap / total if total > 0 else 0,\n",
    "            \"vocabulary_change_ratio\": (\n",
    "                len(orig_words ^ rev_words) / total if total > 0 else 0\n",
    "            ),\n",
    "            \"unique_words_ratio\": (\n",
    "                len(rev_words - orig_words) / len(rev_words) if rev_words else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Change in proportion of deactivated words\n",
    "        orig_stop = len([w for w in orig_words if w in self.stop_words])\n",
    "        rev_stop = len([w for w in rev_words if w in self.stop_words])\n",
    "\n",
    "        features[\"stopword_ratio_change\"] = abs(\n",
    "            (orig_stop / len(orig_words) if orig_words else 0)\n",
    "            - (rev_stop / len(rev_words) if rev_words else 0)\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_syntactic_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Extracting syntactic features\"\"\"\n",
    "        orig_sents = sent_tokenize(original)\n",
    "        rev_sents = sent_tokenize(revised)\n",
    "\n",
    "        features = {\n",
    "            \"sentence_count_ratio\": (\n",
    "                len(rev_sents) / len(orig_sents) if orig_sents else 1\n",
    "            ),\n",
    "            \"avg_sentence_length_change\": (\n",
    "                abs(\n",
    "                    np.mean([len(word_tokenize(s)) for s in orig_sents])\n",
    "                    - np.mean([len(word_tokenize(s)) for s in rev_sents])\n",
    "                )\n",
    "                if orig_sents and rev_sents\n",
    "                else 0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            orig_pos = nltk.pos_tag(word_tokenize(original))\n",
    "            rev_pos = nltk.pos_tag(word_tokenize(revised))\n",
    "\n",
    "            # Calculate the percentage change in the main lexical categories\n",
    "            for pos_type in [\n",
    "                \"NN\",\n",
    "                \"VB\",\n",
    "                \"JJ\",\n",
    "                \"RB\",\n",
    "            ]:  # Nouns, verbs, adjectives, adverbs\n",
    "                orig_count = sum(1 for _, pos in orig_pos if pos.startswith(pos_type))\n",
    "                rev_count = sum(1 for _, pos in rev_pos if pos.startswith(pos_type))\n",
    "\n",
    "                features[f\"pos_{pos_type}_ratio_change\"] = abs(\n",
    "                    (orig_count / len(orig_pos) if orig_pos else 0)\n",
    "                    - (rev_count / len(rev_pos) if rev_pos else 0)\n",
    "                )\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_stylistic_features(\n",
    "        self, original: str, revised: str\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"Extracting stylistic features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Changes in the use of punctuation\n",
    "        orig_punct = sum(1 for c in original if c in \".,!?;:\")\n",
    "        rev_punct = sum(1 for c in revised if c in \".,!?;:\")\n",
    "\n",
    "        features[\"punctuation_ratio_change\"] = abs(\n",
    "            (orig_punct / len(original) if original else 0)\n",
    "            - (rev_punct / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # Change in proportion of capital letters\n",
    "        orig_caps = sum(1 for c in original if c.isupper())\n",
    "        rev_caps = sum(1 for c in revised if c.isupper())\n",
    "\n",
    "        features[\"capitalization_ratio_change\"] = abs(\n",
    "            (orig_caps / len(original) if original else 0)\n",
    "            - (rev_caps / len(revised) if revised else 0)\n",
    "        )\n",
    "\n",
    "        # Change in average word length\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "\n",
    "        features[\"avg_word_length_change\"] = (\n",
    "            abs(\n",
    "                np.mean([len(w) for w in orig_words])\n",
    "                - np.mean([len(w) for w in rev_words])\n",
    "            )\n",
    "            if orig_words and rev_words\n",
    "            else 0\n",
    "        )\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _extract_edit_features(self, original: str, revised: str) -> Dict[str, float]:\n",
    "        \"\"\"Extraction of edit distance related features\"\"\"\n",
    "        from difflib import SequenceMatcher\n",
    "\n",
    "        # Character-level similarity\n",
    "        char_similarity = SequenceMatcher(None, original, revised).ratio()\n",
    "\n",
    "        # word-level similarity\n",
    "        orig_words = word_tokenize(original)\n",
    "        rev_words = word_tokenize(revised)\n",
    "        word_similarity = SequenceMatcher(None, orig_words, rev_words).ratio()\n",
    "\n",
    "        features = {\n",
    "            \"char_level_similarity\": char_similarity,\n",
    "            \"word_level_similarity\": word_similarity,\n",
    "            \"length_ratio\": len(revised) / len(original) if original else 1,\n",
    "        }\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "class AITextDetector:\n",
    "    \"\"\"Main detector class\"\"\"\n",
    "\n",
    "    def __init__(self, config: DetectionConfig = None):\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.perturber = EnhancedTextPerturber(self.config)\n",
    "        self.reviser = EnhancedLLMReviser(self.config)\n",
    "        self.feature_extractor = MultiFeatureExtractor(self.config)\n",
    "        self.classifier = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.cache = {}\n",
    "        self.api_concurrency_limit = asyncio.Semaphore(10)\n",
    "\n",
    "    # In the AITextDetector class\n",
    "\n",
    "    async def detect_batch(\n",
    "        self, texts: List[str], labels: Optional[List[int]] = None\n",
    "    ) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Asynchronously detects text in a batch, with robust caching and parallel API calls.\n",
    "        \"\"\"\n",
    "        all_features = []\n",
    "        all_scores = []\n",
    "\n",
    "        if not texts:\n",
    "            # Handle the case where an empty list is passed\n",
    "            return {\n",
    "                \"predictions\": np.array([]),\n",
    "                \"probabilities\": np.array([]),\n",
    "                \"features\": np.array([]),\n",
    "                \"similarity_scores\": np.array([]),\n",
    "            }\n",
    "\n",
    "        print(\"Processing texts...\")\n",
    "\n",
    "        # Step 1: Perform synchronous, CPU-bound perturbation first.\n",
    "        perturbed_texts = [\n",
    "            self.perturber.perturb(text) for text in tqdm(texts, desc=\"Perturbing\")\n",
    "        ]\n",
    "\n",
    "        # Step 2: Define a helper function to manage semaphore and call the new revise method.\n",
    "        # This helper will be used to create our list of concurrent tasks.\n",
    "        async def revise_with_limit(original_text, perturbed_text):\n",
    "            async with self.api_concurrency_limit:\n",
    "                # Call the updated revise function with both original and perturbed text\n",
    "                return await self.reviser.revise(\n",
    "                    original_text, perturbed_text, self.cache\n",
    "                )\n",
    "\n",
    "        # Step 3: Create a list of NEW coroutine tasks.\n",
    "        # Each task is a call to our helper function with a pair of original and perturbed texts.\n",
    "        print(f\"Revising texts with {self.reviser.config.revision_model}...\")\n",
    "        revision_tasks = [\n",
    "            revise_with_limit(orig, pert) for orig, pert in zip(texts, perturbed_texts)\n",
    "        ]\n",
    "\n",
    "        # Step 4: Concurrently run all revision tasks using asyncio.gather.\n",
    "        # This preserves the order of the results and is wrapped in tqdm for a progress bar.\n",
    "        all_revised_texts = await asyncio.gather(\n",
    "            *tqdm(revision_tasks, desc=\"Revising\", total=len(revision_tasks))\n",
    "        )\n",
    "\n",
    "        # Step 5: Process the results. This part is synchronous again.\n",
    "        print(\"Extracting features...\")\n",
    "        for i, original_text in enumerate(texts):\n",
    "            # The order is preserved, so we can safely pair original and revised texts.\n",
    "            revised_text = all_revised_texts[i]\n",
    "            features = self.feature_extractor.extract_features(\n",
    "                original_text, revised_text\n",
    "            )\n",
    "            all_features.append(features)\n",
    "            all_scores.append(features.get(\"semantic_similarity\", 0.5))\n",
    "\n",
    "        # --- The rest of the function remains the same ---\n",
    "        feature_matrix = pd.DataFrame(all_features).fillna(0).values\n",
    "\n",
    "        if self.config.use_ml_classifier:\n",
    "            if labels is not None and self.classifier is None:\n",
    "                self._train_classifier(feature_matrix, labels)\n",
    "\n",
    "            if self.classifier is not None:\n",
    "                feature_matrix_scaled = self.scaler.transform(feature_matrix)\n",
    "                predictions = self.classifier.predict(feature_matrix_scaled)\n",
    "                probabilities = self.classifier.predict_proba(feature_matrix_scaled)[\n",
    "                    :, 1\n",
    "                ]\n",
    "            else:\n",
    "                # Fallback if classifier isn't trained\n",
    "                predictions = (\n",
    "                    np.array(all_scores) > self.config.similarity_threshold\n",
    "                ).astype(int)\n",
    "                probabilities = np.array(all_scores)\n",
    "        else:\n",
    "            # Logic for when the ML classifier is turned off\n",
    "            predictions = (\n",
    "                np.array(all_scores) > self.config.similarity_threshold\n",
    "            ).astype(int)\n",
    "            probabilities = np.array(all_scores)\n",
    "\n",
    "        return {\n",
    "            \"predictions\": predictions,\n",
    "            \"probabilities\": probabilities,\n",
    "            \"features\": feature_matrix,\n",
    "            \"similarity_scores\": np.array(all_scores),\n",
    "        }\n",
    "\n",
    "    def _train_classifier(self, features: np.ndarray, labels: List[int]):\n",
    "        \"\"\"Training Machine Learning Classifiers\"\"\"\n",
    "        print(\"Training classifier...\")\n",
    "\n",
    "        # Standardised features\n",
    "        features_scaled = self.scaler.fit_transform(features)\n",
    "\n",
    "        # Using Random Forests\n",
    "        self.classifier = RandomForestClassifier(\n",
    "            n_estimators=100, max_depth=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- RandomForestClassifier Parameters ---\")\n",
    "        print(self.classifier.get_params())\n",
    "\n",
    "        self.classifier.fit(features_scaled, labels)\n",
    "\n",
    "        # Characteristic importance\n",
    "        feature_names = list(pd.DataFrame(features).columns)\n",
    "        importances = self.classifier.feature_importances_\n",
    "\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        for feat, imp in sorted(\n",
    "            zip(feature_names, importances), key=lambda x: x[1], reverse=True\n",
    "        )[:10]:\n",
    "            print(f\"  {feat}: {imp:.4f}\")\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \"\"\"Save the model\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        model_data = {\n",
    "            \"classifier\": self.classifier,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"config\": self.config,\n",
    "            \"cache\": self.cache,\n",
    "        }\n",
    "\n",
    "        with open(path, \"wb\") as f:\n",
    "            pickle.dump(model_data, f)\n",
    "\n",
    "        print(f\"Model saved to {path}\")\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \"\"\"Loading Models\"\"\"\n",
    "        import pickle\n",
    "\n",
    "        with open(path, \"rb\") as f:\n",
    "            model_data = pickle.load(f)\n",
    "\n",
    "        self.classifier = model_data[\"classifier\"]\n",
    "        self.scaler = model_data[\"scaler\"]\n",
    "        self.config = model_data[\"config\"]\n",
    "        self.cache = model_data[\"cache\"]\n",
    "\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "\n",
    "class CustomDataLoader:\n",
    "    \"\"\"Data Loader Class\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: str = \"./data\"):\n",
    "        self.data_dir = data_dir\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "            logger.info(f\"Created data directory: {data_dir}\")\n",
    "\n",
    "    def load_data(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads a dataset with 'text' and 'label' columns and returns a DataFrame.\n",
    "\n",
    "        Args.\n",
    "            path: path to the data file, 'finance' means use financial dataset, 'sample' means create sample data.\n",
    "        Returns.\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "        \"\"\"\n",
    "        if path == \"finance\":\n",
    "            return self.load_finance_dataset()\n",
    "        if path == \"sample\":\n",
    "            return self.create_sample_dataset()\n",
    "        elif path == \"test\":\n",
    "            return self.load_origin_dataset()\n",
    "\n",
    "        # If it is a relative path, add a data catalogue prefix\n",
    "        if not os.path.isabs(path) and not os.path.exists(path):\n",
    "            path = os.path.join(self.data_dir, path)\n",
    "\n",
    "        try:\n",
    "            if path.endswith(\".csv\"):\n",
    "                df = pd.read_csv(path)\n",
    "            elif path.endswith(\".jsonl\"):\n",
    "                df = pd.read_json(path, lines=True)\n",
    "            elif path.endswith(\".json\"):\n",
    "                df = pd.read_json(path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file format: {path}\")\n",
    "\n",
    "            # Ensure that the necessary columns are available\n",
    "            if \"text\" not in df.columns:\n",
    "                raise ValueError(\"Missing 'text' column in dataset\")\n",
    "            if \"label\" not in df.columns:\n",
    "                raise ValueError(\"Missing 'label' column in dataset\")\n",
    "\n",
    "            # If there is no id column, automatically generate\n",
    "            if \"id\" not in df.columns:\n",
    "                df[\"id\"] = range(len(df))\n",
    "\n",
    "            # Data Cleaning\n",
    "            df = df.dropna(subset=[\"text\", \"label\"])\n",
    "            df = df[df[\"text\"].str.len() > 10]  # Filtering text that is too short\n",
    "\n",
    "            logger.info(f\"Loaded {len(df)} samples from {path}\")\n",
    "            logger.info(f\"Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "            return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"File {path} not found. Creating sample dataset...\")\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "    def find_finance_data_files(self) -> Dict[str, str]:\n",
    "        \"\"\"Find financial dataset files\"\"\"\n",
    "        logger.info(\"Searching for finance data files...\")\n",
    "\n",
    "        # Local environment path\n",
    "        possible_paths = [\n",
    "            self.data_dir,\n",
    "            os.path.join(self.data_dir, \"finance\"),\n",
    "            os.path.join(self.data_dir, \"finance-dataset\"),\n",
    "            \"./finance_data\",\n",
    "            \".\",\n",
    "            \"./data\",\n",
    "        ]\n",
    "\n",
    "        files_found = {}\n",
    "\n",
    "        for path in possible_paths:\n",
    "            if os.path.exists(path):\n",
    "                logger.debug(f\"Checking path: {path}\")\n",
    "                for root, dirs, files in os.walk(path):\n",
    "                    for file in files:\n",
    "                        if \"revised_human_finance\" in file or \"revised_human\" in file:\n",
    "                            files_found[\"revised_human\"] = os.path.join(root, file)\n",
    "                        elif (\n",
    "                            \"revised_chatgpt_finance\" in file\n",
    "                            or \"revised_chatgpt\" in file\n",
    "                        ):\n",
    "                            files_found[\"revised_chatgpt\"] = os.path.join(root, file)\n",
    "                        elif \"finance.jsonl\" in file:\n",
    "                            files_found[\"original\"] = os.path.join(root, file)\n",
    "\n",
    "        return files_found\n",
    "\n",
    "    def load_finance_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load financial dataset - use revised text as data\n",
    "\n",
    "        Returns.\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "            where text is the revised text, label: 0=human, 1=chatgpt\n",
    "        \"\"\"\n",
    "        files = self.find_finance_data_files()\n",
    "\n",
    "        logger.info(\"Loading finance datasets...\")\n",
    "        logger.info(f\"Found files: {list(files.keys())}\")\n",
    "\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if \"revised_human\" in files:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Loading revised human texts from: {files['revised_human']}\"\n",
    "                )\n",
    "                with open(files[\"revised_human\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line_no, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            try:\n",
    "\n",
    "                                item = json.loads(line)\n",
    "                                for idx, text in item.items():\n",
    "\n",
    "                                    cleaned_text = (\n",
    "                                        text.strip()\n",
    "                                        .replace(\"\\\\n\\\\n\", \" \")\n",
    "                                        .replace(\"\\\\n\", \" \")\n",
    "                                    )\n",
    "                                    if len(cleaned_text) > 20:\n",
    "                                        all_texts.append(cleaned_text)\n",
    "                                        all_labels.append(0)  # 0 = human\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                logger.warning(f\"  Line {line_no} parse error: {e}\")\n",
    "                logger.info(\n",
    "                    f\"  Loaded {len([l for l in all_labels if l == 0])} human texts\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading revised human texts: {e}\")\n",
    "\n",
    "        # Load revised ChatGPT text (one JSON object per line)\n",
    "        if \"revised_chatgpt\" in files:\n",
    "            try:\n",
    "                logger.info(\n",
    "                    f\"Loading revised ChatGPT texts from: {files['revised_chatgpt']}\"\n",
    "                )\n",
    "                with open(files[\"revised_chatgpt\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                    for line_no, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            try:\n",
    "\n",
    "                                item = json.loads(line)\n",
    "                                for idx, text in item.items():\n",
    "\n",
    "                                    cleaned_text = (\n",
    "                                        text.strip()\n",
    "                                        .replace(\"\\\\n\\\\n\", \" \")\n",
    "                                        .replace(\"\\\\n\", \" \")\n",
    "                                    )\n",
    "                                    if len(cleaned_text) > 20:\n",
    "                                        all_texts.append(cleaned_text)\n",
    "                                        all_labels.append(1)  # 1 = chatgpt\n",
    "                            except json.JSONDecodeError as e:\n",
    "                                logger.warning(f\"  Line {line_no} parse error: {e}\")\n",
    "                logger.info(\n",
    "                    f\"  Loaded {len([l for l in all_labels if l == 1])} ChatGPT texts\"\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading revised ChatGPT texts: {e}\")\n",
    "\n",
    "        # Creating a DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            {\"id\": range(len(all_texts)), \"text\": all_texts, \"label\": all_labels}\n",
    "        )\n",
    "\n",
    "        # Disrupt the data\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        df[\"id\"] = range(len(df))\n",
    "\n",
    "        logger.info(f\"\\n=== Dataset Summary ===\")\n",
    "        logger.info(f\"Total samples: {len(df)}\")\n",
    "        logger.info(f\"Human texts: {len(df[df['label'] == 0])}\")\n",
    "        logger.info(f\"ChatGPT texts: {len(df[df['label'] == 1])}\")\n",
    "\n",
    "        # Show Example\n",
    "        if len(df) > 0:\n",
    "            logger.info(\"\\nSample texts:\")\n",
    "            for label in [0, 1]:\n",
    "                label_name = \"Human\" if label == 0 else \"ChatGPT\"\n",
    "                samples = df[df[\"label\"] == label]\n",
    "                if len(samples) > 0:\n",
    "                    sample_text = samples.iloc[0][\"text\"]\n",
    "                    logger.info(f\"[{label_name}]: {sample_text[:100]}...\")\n",
    "\n",
    "        return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "    def create_sample_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a sample dataset for testing\n",
    "\n",
    "        Returns:\n",
    "            df: columns = ['id', 'text', 'label']\n",
    "        \"\"\"\n",
    "        sample_data = {\n",
    "            \"id\": range(6),\n",
    "            \"text\": [\n",
    "                \"There is most likely an error in the WSJ's data.  Yahoo! Finance reports the PE on the Russell 2000 to be 15 as of 83111 and SP 500 PE to be 13 (about the same as WSJ). Good catch, though!  E-mail WSJ, perhaps they will be grateful.\",\n",
    "                \"I know this question has a lot of answers already, but I feel the answers are phrased either strongly against, or mildly for, co-signing. What it amounts down to is that this is a personal choice. You cannot receive reliable information as to whether or not co-signing this loan is a good move due to lack of information. The person involved is going to know the person they would be co-signing for, and the people on this site will only have their own personal preferences of experiences to draw from. You know if they are reliable, if they will be able to pay off the loan without need for the banks to come after you.  This site can offer general theories, but I think it should be kept in mind that this is wholly a personal decision for the person involved, and them alone to make based on the facts that they know and we do not.\",\n",
    "                \"I think the best investment strategy is to diversify your portfolio across different asset classes.\",\n",
    "                \"Historical price-to-earnings (PE) ratios for small-cap and large-cap stocks can vary significantly over time and may not be directly comparable due to the different characteristics of these two categories of stocks.Small-cap stocks, which are defined as stocks with a market capitalization of less than $2 billion, tend to be riskier and more volatile than large-cap stocks, which have a market capitalization of $10 billion or more. As a result, investors may be willing to pay a higher price for the potential growth opportunities offered by small-cap stocks, which can lead to higher PE ratios.On the other hand, large-cap stocks tend to be more established and stable, with a longer track record of earnings and revenue growth. As a result, these stocks may trade at lower PE ratios, as investors may be less willing to pay a premium for their growth potential.It is important to note that PE ratios are just one factor to consider when evaluating a stock and should not be used in isolation. Other factors, such as the company's financial health, industry trends, and macroeconomic conditions, can also impact a stock's PE ratio.\",\n",
    "                \"Co-signing a personal loan for a friend or family member can be a risky proposition. When you co-sign a loan, you are agreeing to be responsible for the loan if the borrower is unable to make the payments. This means that if your friend or family member defaults on the loan, you will be on the hook for the remaining balance.There are a few things to consider before co-signing a personal loan for someone:Do you trust the borrower to make the payments on time and in full? If you are not confident that the borrower will be able to make the payments, it may not be a good idea to co-sign the loan.Can you afford to make the payments if the borrower defaults? If you are unable to make the payments, co-signing the loan could put your own financial stability at risk.What is the purpose of the loan? If the borrower is using the loan for a risky or questionable venture, it may not be worth the risk to co-sign.Is there another way for the borrower to get the loan without a co-signer? If the borrower has a good credit score and is able to qualify for a loan on their own, it may not be necessary for you to co-sign.In general, it is important to carefully consider the risks and potential consequences before co-signing a loan for someone. If you do decide to co-sign, it is a good idea to have a conversation with the borrower about their plans for making the loan payments and to have a clear understanding of your responsibilities as a co-signer.\",\n",
    "                \"The optimal approach to risk management involves careful assessment of market conditions.\",\n",
    "            ],\n",
    "            \"label\": [0, 0, 0, 1, 1, 1],  # 0=human, 1=AI\n",
    "        }\n",
    "\n",
    "        df = pd.DataFrame(sample_data)\n",
    "        logger.info(f\"Created sample dataset with {len(df)} examples\")\n",
    "        return df\n",
    "\n",
    "    def load_origin_dataset(self) -> pd.DataFrame:\n",
    "        \"\"\"Load finance dataset from JSON Lines format\"\"\"\n",
    "        files = self.find_finance_data_files()\n",
    "\n",
    "        logger.info(\"Loading finance datasets...\")\n",
    "        logger.info(f\"Found files: {list(files.keys())}\")\n",
    "\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if \"original\" in files:\n",
    "            finance_path = files[\"original\"]\n",
    "            logger.info(f\"Loading from: {finance_path}\")\n",
    "\n",
    "            try:\n",
    "                with open(finance_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    line_count = 0\n",
    "                    for line_num, line in enumerate(f, 1):\n",
    "                        line = line.strip()\n",
    "                        if not line:\n",
    "                            continue\n",
    "\n",
    "                        try:\n",
    "                            data = json.loads(line)\n",
    "                            line_count += 1\n",
    "\n",
    "                            # Extract human answers\n",
    "                            human_answers = data.get(\"human_answers\", [])\n",
    "                            if (\n",
    "                                human_answers\n",
    "                                and len(human_answers) > 0\n",
    "                                and human_answers[0]\n",
    "                            ):\n",
    "                                all_texts.append(human_answers[0])\n",
    "                                all_labels.append(0)  # 0 for human\n",
    "\n",
    "                            # Extract ChatGPT answers\n",
    "                            chatgpt_answers = data.get(\"chatgpt_answers\", [])\n",
    "                            if (\n",
    "                                chatgpt_answers\n",
    "                                and len(chatgpt_answers) > 0\n",
    "                                and chatgpt_answers[0]\n",
    "                            ):\n",
    "                                all_texts.append(chatgpt_answers[0])\n",
    "                                all_labels.append(1)  # 1 for ChatGPT\n",
    "\n",
    "                            # Log progress every 1000 lines\n",
    "                            if line_num % 1000 == 0:\n",
    "                                logger.info(\n",
    "                                    f\"Processed {line_num} lines, extracted {len(all_texts)} texts\"\n",
    "                                )\n",
    "\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            logger.error(f\"Error parsing line {line_num}: {e}\")\n",
    "                            logger.error(f\"Line content: {line[:100]}...\")\n",
    "                            continue\n",
    "\n",
    "                logger.info(f\"Successfully loaded {line_count} JSON objects\")\n",
    "                logger.info(f\"Total texts extracted: {len(all_texts)}\")\n",
    "\n",
    "            except FileNotFoundError:\n",
    "                logger.error(f\"File not found: {finance_path}\")\n",
    "                return self.create_sample_dataset()\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading finance data: {e}\")\n",
    "                return self.create_sample_dataset()\n",
    "        else:\n",
    "            logger.warning(\"No finance data file found. Creating sample dataset...\")\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "        if len(all_texts) == 0:\n",
    "            logger.warning(\n",
    "                \"No texts extracted from finance data. Creating sample dataset...\"\n",
    "            )\n",
    "            return self.create_sample_dataset()\n",
    "\n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(\n",
    "            {\"id\": range(len(all_texts)), \"text\": all_texts, \"label\": all_labels}\n",
    "        )\n",
    "\n",
    "        # Shuffle data\n",
    "        df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "        df[\"id\"] = range(len(df))\n",
    "\n",
    "        # Print summary\n",
    "        logger.info(f\"\\n=== Dataset Summary ===\")\n",
    "        logger.info(f\"Total samples: {len(df)}\")\n",
    "        logger.info(f\"Human texts: {len(df[df['label'] == 0])}\")\n",
    "        logger.info(f\"ChatGPT texts: {len(df[df['label'] == 1])}\")\n",
    "\n",
    "        # Show balance\n",
    "        balance = len(df[df[\"label\"] == 0]) / len(df) * 100\n",
    "        logger.info(\n",
    "            f\"Dataset balance: {balance:.1f}% human, {100-balance:.1f}% ChatGPT\"\n",
    "        )\n",
    "\n",
    "        # Display samples\n",
    "        if len(df) > 0:\n",
    "            logger.info(\"\\nSample texts:\")\n",
    "            for label in [0, 1]:\n",
    "                label_name = \"Human\" if label == 0 else \"ChatGPT\"\n",
    "                samples = df[df[\"label\"] == label]\n",
    "                if len(samples) > 0:\n",
    "                    sample_text = samples.iloc[0][\"text\"]\n",
    "                    logger.info(f\"\\n[{label_name}]: {sample_text[:200]}...\")\n",
    "\n",
    "        return df[[\"id\", \"text\", \"label\"]]\n",
    "\n",
    "    def save_dataset(self, df: pd.DataFrame, filename: str, format: str = \"csv\"):\n",
    "        \"\"\"\n",
    "        Saving a dataset to a file\n",
    "\n",
    "        Args:\n",
    "            df: the DataFrame to be saved\n",
    "            filename: the name of the file (without extension)\n",
    "            format: file format ('csv', 'json', 'jsonl')\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.data_dir, f\"{filename}.{format}\")\n",
    "\n",
    "        if format == \"csv\":\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif format == \"json\":\n",
    "            df.to_json(filepath, orient=\"records\", indent=2)\n",
    "        elif format == \"jsonl\":\n",
    "            df.to_json(filepath, orient=\"records\", lines=True)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "\n",
    "        logger.info(f\"Saved dataset to {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "\n",
    "def load_data(path: str) -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_data(path)\n",
    "\n",
    "\n",
    "def load_finance_dataset() -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_finance_dataset()\n",
    "\n",
    "\n",
    "def load_origin_dataset() -> pd.DataFrame:\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.load_origin_dataset\n",
    "\n",
    "\n",
    "def create_sample_dataset() -> pd.DataFrame:\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    return loader.create_sample_dataset()\n",
    "\n",
    "\n",
    "def evaluate_detector(\n",
    "    detector: AITextDetector, texts: List[str], labels: List[int]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluating detector performance\n",
    "    \"\"\"\n",
    "    results = detector.detect_batch(texts, labels)\n",
    "\n",
    "    predictions = results[\"predictions\"]\n",
    "    probabilities = results[\"probabilities\"]\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"auc\": roc_auc_score(labels, probabilities),\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"AUC: {metrics['auc']:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(labels, predictions, target_names=[\"Human\", \"AI\"]))\n",
    "\n",
    "    human_scores = results[\"similarity_scores\"][np.array(labels) == 0]\n",
    "    ai_scores = results[\"similarity_scores\"][np.array(labels) == 1]\n",
    "\n",
    "    print(f\"\\nSimilarity Score Distribution:\")\n",
    "    print(\n",
    "        f\"Human texts - Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\"\n",
    "    )\n",
    "    print(f\"AI texts - Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_roc_curves(results: Dict):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        fpr, tpr, _ = roc_curve(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        auc = roc_auc_score(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curves(results: Dict):\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"]\n",
    "        )\n",
    "        plt.plot(recall, precision, label=f\"{name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(results: Dict):\n",
    "\n",
    "    n_results = len(results)\n",
    "    fig, axes = plt.subplots(1, n_results, figsize=(6 * n_results, 5))\n",
    "    if n_results == 1:\n",
    "        axes = [axes]  # Make it iterable\n",
    "    for ax, (name, result_data) in zip(axes, results.items()):\n",
    "        cm = confusion_matrix(\n",
    "            result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"]\n",
    "        )\n",
    "        sns.heatmap(\n",
    "            cm,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            ax=ax,\n",
    "            cbar=False,\n",
    "            xticklabels=[\"Human\", \"AI\"],\n",
    "            yticklabels=[\"Human\", \"AI\"],\n",
    "        )\n",
    "        ax.set_title(f\"Confusion Matrix: {name}\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_classification_metrics(results: Dict):\n",
    "\n",
    "    metrics_data = []\n",
    "    for name, result_data in results.items():\n",
    "        report = classification_report(\n",
    "            result_data[\"test_labels\"],\n",
    "            result_data[\"test_results\"][\"predictions\"],\n",
    "            target_names=[\"Human\", \"AI\"],\n",
    "            output_dict=True,\n",
    "        )\n",
    "        ai_metrics = report[\"AI\"]\n",
    "        metrics_data.append(\n",
    "            {\n",
    "                \"Configuration\": name,\n",
    "                \"Precision\": ai_metrics[\"precision\"],\n",
    "                \"Recall\": ai_metrics[\"recall\"],\n",
    "                \"F1-Score\": ai_metrics[\"f1-score\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df_melted = df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(data=df_melted, x=\"Configuration\", y=\"Score\", hue=\"Metric\")\n",
    "    plt.title('Classification Metrics for \"AI\" Class')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def find_optimal_threshold(labels: List[int], scores: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Find the optimal classification threshold using the ROC curve.\n",
    "\n",
    "    The optimal threshold is the one that maximizes the difference\n",
    "    between the true positive rate (TPR) and the false positive rate (FPR).\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "\n",
    "    # Calculate the optimal threshold\n",
    "    optimal_idx = np.argmax(tpr - fpr)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    print(\n",
    "        f\"\\nFound optimal similarity threshold from training data: {optimal_threshold:.4f}\"\n",
    "    )\n",
    "    return optimal_threshold\n",
    "\n",
    "\n",
    "\n",
    "async def main(\n",
    "    data_path: str = \"finance\",\n",
    "    max_samples: Optional[int] = None,\n",
    "    use_cache: bool = True,\n",
    "    config: Optional[DetectionConfig] = None,\n",
    "):\n",
    "\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    print(\"=== Enhanced AI Text Detection ===\")\n",
    "    print(\n",
    "        f\"Method: Multi-feature extraction with {config.revision_model if config else 'default model'}\"\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Loading data from: {data_path}\")\n",
    "    loader = CustomDataLoader()\n",
    "    df = loader.load_data(data_path)\n",
    "    if max_samples:\n",
    "        df = df.head(max_samples)\n",
    "        print(f\"Using first {max_samples} samples for testing\")\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    texts = df[\"text\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "\n",
    "    # Data segmentation\n",
    "    split_idx = int(len(texts) * 0.6)\n",
    "    train_texts, test_texts = texts[:split_idx], texts[split_idx:]\n",
    "    train_labels, test_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "    print(f\"\\nDataset split: {len(train_texts)} training, {len(test_texts)} testing\")\n",
    "\n",
    "    # Creating Detectors\n",
    "    if config is None:\n",
    "        config = DetectionConfig(\n",
    "            revision_model=\"t5-small\",  # 或 \"gpt2\" t5-small\n",
    "            embedding_model=\"all-MiniLM-L6-v2\",\n",
    "            perturbation_rate=0.15,\n",
    "            use_ml_classifier=True,\n",
    "        )\n",
    "\n",
    "    detector = AITextDetector(config)\n",
    "\n",
    "    # Load Cache\n",
    "    cache_path = f\"enhanced_cache_{config.revision_model}.json\"\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        with open(cache_path, \"r\") as f:\n",
    "            detector.cache = json.load(f)\n",
    "        print(f\"Loaded cache with {len(detector.cache)} entries\")\n",
    "\n",
    "    # training phase\n",
    "    print(\"\\n=== Training Phase ===\")\n",
    "    print(\n",
    "        \"Train labels distribution:\", pd.Series(train_labels).value_counts().to_dict()\n",
    "    )\n",
    "    train_results = await detector.detect_batch(train_texts, train_labels)\n",
    "    # If not using the ML classifier, find the best threshold from the training run\n",
    "    if not config.use_ml_classifier:\n",
    "        optimal_threshold = find_optimal_threshold(\n",
    "            train_labels, train_results[\"similarity_scores\"]\n",
    "        )\n",
    "        # Update the detector's config with the new, optimal threshold\n",
    "        detector.config.similarity_threshold = optimal_threshold\n",
    "\n",
    "    # testing phase\n",
    "    print(\"\\n=== Testing Phase ===\")\n",
    "    test_results = await detector.detect_batch(test_texts)\n",
    "\n",
    "    # Assessment results\n",
    "    test_predictions = test_results[\"predictions\"]\n",
    "    test_probabilities = test_results[\"probabilities\"]\n",
    "\n",
    "    # Calculation of indicators\n",
    "    accuracy = accuracy_score(test_labels, test_predictions)\n",
    "    auc = roc_auc_score(test_labels, test_probabilities)\n",
    "    f1 = f1_score(test_labels, test_predictions)\n",
    "\n",
    "    print(f\"\\n=== Test Results ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"F1 Scores: {f1:.4f}\")\n",
    "    # Detailed report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            test_labels, test_predictions, target_names=[\"Human\", \"AI\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # confusion matrix (math.)\n",
    "    cm = confusion_matrix(test_labels, test_predictions)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"              Predicted\")\n",
    "    print(f\"            Human    AI\")\n",
    "    print(f\"Actual Human  {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"       AI     {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "\n",
    "    # character analysis\n",
    "    features_df = test_results[\"features\"]\n",
    "    similarity_scores = test_results[\"similarity_scores\"]\n",
    "\n",
    "    # Subgroup statistics\n",
    "    human_scores = similarity_scores[np.array(test_labels) == 0]\n",
    "    ai_scores = similarity_scores[np.array(test_labels) == 1]\n",
    "\n",
    "    print(f\"\\n=== Similarity Score Analysis ===\")\n",
    "    print(f\"Human texts:\")\n",
    "    print(f\"  Mean: {np.mean(human_scores):.3f}, Std: {np.std(human_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(human_scores):.3f}, Max: {np.max(human_scores):.3f}\")\n",
    "\n",
    "    print(f\"AI texts:\")\n",
    "    print(f\"  Mean: {np.mean(ai_scores):.3f}, Std: {np.std(ai_scores):.3f}\")\n",
    "    print(f\"  Min: {np.min(ai_scores):.3f}, Max: {np.max(ai_scores):.3f}\")\n",
    "\n",
    "    print(\"\\n=== Misclassified Examples ===\")\n",
    "    misclassified_idx = np.where(test_predictions != test_labels)[0]\n",
    "\n",
    "    for i, idx in enumerate(misclassified_idx[:50]):  # 显示前5个\n",
    "        true_label = \"AI\" if test_labels[idx] == 1 else \"Human\"\n",
    "        pred_label = \"AI\" if test_predictions[idx] == 1 else \"Human\"\n",
    "        text = (\n",
    "            test_texts[idx][:100] + \"...\"\n",
    "            if len(test_texts[idx]) > 100\n",
    "            else test_texts[idx]\n",
    "        )\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"True: {true_label}, Predicted: {pred_label}\")\n",
    "        print(\n",
    "            f\"Similarity: {similarity_scores[idx]:.3f}, Probability: {test_probabilities[idx]:.3f}\"\n",
    "        )\n",
    "\n",
    "    if use_cache:\n",
    "        with open(cache_path, \"w\") as f:\n",
    "            json.dump(detector.cache, f)\n",
    "        print(f\"\\nCache saved to {cache_path}\")\n",
    "\n",
    "    model_path = f\"enhanced_detector_{config.revision_model}.pkl\"\n",
    "    detector.save_model(model_path)\n",
    "\n",
    "    return {\n",
    "        \"detector\": detector,\n",
    "        \"test_results\": test_results,\n",
    "        \"test_labels\": test_labels,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:04:03.965596Z",
     "iopub.status.busy": "2025-07-13T17:04:03.965019Z",
     "iopub.status.idle": "2025-07-13T17:04:03.993284Z",
     "shell.execute_reply": "2025-07-13T17:04:03.992562Z",
     "shell.execute_reply.started": "2025-07-13T17:04:03.965578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DualPathFusionModel(nn.Module):\n",
    "  \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer_name: str = \"distilbert-base-uncased\",\n",
    "        num_statistical_features: int = 13,\n",
    "        hidden_size: int = 256,\n",
    "        dropout_rate: float = 0.3,\n",
    "    ):\n",
    "   \n",
    "        super(DualPathFusionModel, self).__init__()\n",
    "\n",
    "        \n",
    "        self.transformer = AutoModel.from_pretrained(transformer_name)\n",
    "        self.transformer_dim = self.transformer.config.hidden_size\n",
    "\n",
    "        # Freeze the first few layers of the Transformer\n",
    "        for param in self.transformer.base_model.embeddings.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Path B uses the input statistical features directly\n",
    "\n",
    "        # Classification head after feature fusion\n",
    "        fusion_dim = self.transformer_dim + num_statistical_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size // 2, 2),  # binary classification\n",
    "        )\n",
    "\n",
    "        # feature normalisation layer\n",
    "        self.feature_norm = nn.BatchNorm1d(num_statistical_features)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        statistical_features: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        # Path A: Extract deep semantic features via Transformer\n",
    "        transformer_outputs = self.transformer(\n",
    "            input_ids=input_ids, attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        # Take the output of the [CLS] token as a sentence representation\n",
    "        cls_embedding = transformer_outputs.last_hidden_state[\n",
    "            :, 0, :\n",
    "        ]  # [batch_size, transformer_dim]\n",
    "\n",
    "        # Path B: Normalised Statistical Features\n",
    "        normalized_features = self.feature_norm(\n",
    "            statistical_features\n",
    "        )  # [batch_size, num_features]\n",
    "\n",
    "        # Feature fusion: stitching two features\n",
    "        fused_features = torch.cat([cls_embedding, normalized_features], dim=1)\n",
    "\n",
    "        # The final prediction is obtained by sorting the header\n",
    "        logits = self.classifier(fused_features)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class TextDetectionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    AI Text Detection Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        statistical_features: np.ndarray,\n",
    "        labels: List[int],\n",
    "        tokenizer,\n",
    "        max_length: int = 512,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args.\n",
    "            texts: list of original texts\n",
    "            statistical_features: matrix of statistical features [n_samples, n_features]\n",
    "            labels: list of labels (0=human, 1=AI)\n",
    "            tokenizer: Transformer lexicon\n",
    "            max_length: Maximum sequence length\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.statistical_features = statistical_features\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        features = self.statistical_features[idx]\n",
    "\n",
    "      \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"statistical_features\": torch.tensor(features, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "class DualPathDetector:\n",
    "    \"\"\"\n",
    "    Detectors using dual path fusion models\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"distilbert-base-uncased\", device: str = None):\n",
    "        \"\"\"\n",
    "        Initialising the detector\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        # Initialise model (will be adjusted later for number of features)\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "        print(f\"Dual Path Detector initialized on {self.device}\")\n",
    "\n",
    "    def prepare_data(\n",
    "        self, texts: List[str], features_df: pd.DataFrame, labels: List[int]\n",
    "    ) -> Tuple[Dataset, Dataset]:\n",
    "        \"\"\"\n",
    "        Preparing training and validation data\n",
    "\n",
    "        Args.\n",
    "            texts: list of texts\n",
    "            features_df: DataFrame with statistical features\n",
    "            labels: List of labels\n",
    "\n",
    "        Returns.\n",
    "            train_dataset, val_dataset\n",
    "        \"\"\"\n",
    "        # Standardised features\n",
    "        features_array = features_df.values\n",
    "        features_normalized = self.scaler.fit_transform(features_array)\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val, feat_train, feat_val = train_test_split(\n",
    "            texts,\n",
    "            labels,\n",
    "            features_normalized,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=labels,\n",
    "        )\n",
    "\n",
    "        # Creating a Data Set\n",
    "        train_dataset = TextDetectionDataset(\n",
    "            X_train, feat_train, y_train, self.tokenizer\n",
    "        )\n",
    "        val_dataset = TextDetectionDataset(X_val, feat_val, y_val, self.tokenizer)\n",
    "\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        num_epochs: int = 10,\n",
    "        batch_size: int = 16,\n",
    "        learning_rate: float = 2e-5,\n",
    "        save_path : str = \"best_dual_path_model.pt\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Training Models\n",
    "        \"\"\"\n",
    "        # If the model has not been initialised, initialise based on the number of features\n",
    "        if self.model is None:\n",
    "            num_features = train_dataset[0][\"statistical_features\"].shape[0]\n",
    "            self.model = DualPathFusionModel(\n",
    "                transformer_name=self.model_name, num_statistical_features=num_features\n",
    "            ).to(self.device)\n",
    "\n",
    "        # Create the data loader\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Optimiser and loss function\n",
    "        optimizer = torch.optim.AdamW(self.model.parameters(), lr=learning_rate)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Learning rate scheduler\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode=\"min\", patience=2, factor=0.5\n",
    "        )\n",
    "\n",
    "        # Training cycles\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            train_correct = 0\n",
    "            train_total = 0\n",
    "\n",
    "            train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
    "            for batch in train_bar:\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                features = batch[\"statistical_features\"].to(self.device)\n",
    "                labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward propagation\n",
    "                logits = self.model(input_ids, attention_mask, features)\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                # Reverse propagation\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                train_total += labels.size(0)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "                train_bar.set_postfix(\n",
    "                    {\n",
    "                        \"loss\": f\"{loss.item():.4f}\",\n",
    "                        \"acc\": f\"{100.*train_correct/train_total:.2f}%\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # validation phase\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            val_total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(\n",
    "                    val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"\n",
    "                ):\n",
    "                    input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                    attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                    features = batch[\"statistical_features\"].to(self.device)\n",
    "                    labels = batch[\"labels\"].to(self.device)\n",
    "\n",
    "                    logits = self.model(input_ids, attention_mask, features)\n",
    "                    loss = criterion(logits, labels)\n",
    "\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(logits.data, 1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Calculation of average loss and accuracy\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            train_acc = 100.0 * train_correct / train_total\n",
    "            val_acc = 100.0 * val_correct / val_total\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}:\")\n",
    "            print(f\"  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            \n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                self.save_model(save_path)\n",
    "                print(\"   Saved best modelto {save_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self, texts: List[str], features_df: pd.DataFrame\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Predict new text\n",
    "\n",
    "        Returns.\n",
    "            predictions: prediction labels\n",
    "            probabilities: Predicted probabilities\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        \n",
    "        features_normalized = self.scaler.transform(features_df.values)\n",
    "\n",
    "     \n",
    "        dataset = TextDetectionDataset(\n",
    "            texts, features_normalized, [0] * len(texts), self.tokenizer  # false label\n",
    "        )\n",
    "        loader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "        all_predictions = []\n",
    "        all_probabilities = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(loader, desc=\"Predicting\"):\n",
    "                input_ids = batch[\"input_ids\"].to(self.device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(self.device)\n",
    "                features = batch[\"statistical_features\"].to(self.device)\n",
    "\n",
    "                logits = self.model(input_ids, attention_mask, features)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_probabilities.extend(probs[:, 1].cpu().numpy())  # AI类的概率\n",
    "\n",
    "        return np.array(all_predictions), np.array(all_probabilities)\n",
    "\n",
    "    def save_model(self, path: str):\n",
    "        \n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"scaler\": self.scaler,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"num_features\": self.model.classifier[0].in_features\n",
    "                - self.model.transformer_dim,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "\n",
    "    def load_model(self, path: str):\n",
    "        \n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "\n",
    " \n",
    "        self.model = DualPathFusionModel(\n",
    "            transformer_name=checkpoint[\"model_name\"],\n",
    "            num_statistical_features=checkpoint[\"num_features\"],\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.scaler = checkpoint[\"scaler\"]\n",
    "        self.model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:13:03.033426Z",
     "iopub.status.busy": "2025-07-13T17:13:03.033092Z",
     "iopub.status.idle": "2025-07-13T17:13:09.757501Z",
     "shell.execute_reply": "2025-07-13T17:13:09.756335Z",
     "shell.execute_reply.started": "2025-07-13T17:13:03.033399Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging setup complete. All output will be saved to ./result\\output.log\n",
      "=== Transformer Model Comparison Experiment ===\n",
      "\n",
      "========================= Running Experiment: fine-tune model =========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: distilbert.transformer.layer.*.attention.out_lin.weight, distilbert.transformer.layer.*.attention.v_lin.bias, distilbert.embeddings.position_embeddings.weight, distilbert.transformer.layer.*.output_layer_norm.bias, vocab_transform.bias, distilbert.transformer.layer.*.ffn.lin*.bias, distilbert.transformer.layer.*.attention.k_lin.bias, vocab_layer_norm.bias, distilbert.transformer.layer.*.sa_layer_norm.bias, vocab_projector.weight, distilbert.transformer.layer.*.attention.q_lin.weight, distilbert.transformer.layer.*.attention.out_lin.bias, distilbert.embeddings.LayerNorm.weight, distilbert.transformer.layer.*.sa_layer_norm.weight, distilbert.transformer.layer.*.ffn.lin*.weight, distilbert.transformer.layer.*.attention.v_lin.weight, vocab_transform.weight, vocab_layer_norm.weight, distilbert.embeddings.word_embeddings.weight, distilbert.transformer.layer.*.attention.k_lin.weight, distilbert.embeddings.LayerNorm.bias, distilbert.transformer.layer.*.output_layer_norm.weight, distilbert.transformer.layer.*.attention.q_lin.bias, vocab_projector.bias\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized for model: gpt-3.5-turbo\n",
      "Using custom base URL: https://api.chatanywhere.tech/v1\n",
      "Use pytorch device_name: cuda:0\n",
      "Load pretrained SentenceTransformer: ./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following layers were not sharded: embeddings.token_type_embeddings.weight, encoder.layer.*.intermediate.dense.bias, pooler.dense.weight, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.attention.output.LayerNorm.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.attention.self.value.bias, embeddings.word_embeddings.weight, pooler.dense.bias, encoder.layer.*.output.LayerNorm.bias, embeddings.LayerNorm.bias, encoder.layer.*.attention.self.value.weight, embeddings.position_embeddings.weight, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.attention.self.query.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.output.LayerNorm.weight, embeddings.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight, encoder.layer.*.attention.output.dense.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Parameters for Embedding Model: ./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete ---\n",
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n",
      "Total Trainable Parameters: 22,713,216\n",
      "Dual Path Detector initialized on cuda\n",
      "Enhanced AI Detector initialized with Dual Path Model: roberta-base\n",
      "Successfully loaded cache with 7844 entries from enhanced_cache_gpt-3.5-turbo.json\n",
      "Searching for finance data files...\n",
      "Loading finance datasets...\n",
      "Found files: ['revised_chatgpt', 'revised_human', 'original']\n",
      "Loading from: ./data\\finance.jsonl\n",
      "Processed 1000 lines, extracted 2000 texts\n",
      "Processed 2000 lines, extracted 4000 texts\n",
      "Processed 3000 lines, extracted 6000 texts\n",
      "Successfully loaded 3933 JSON objects\n",
      "Total texts extracted: 7866\n",
      "\n",
      "=== Dataset Summary ===\n",
      "Total samples: 7866\n",
      "Human texts: 3933\n",
      "ChatGPT texts: 3933\n",
      "Dataset balance: 50.0% human, 50.0% ChatGPT\n",
      "\n",
      "Sample texts:\n",
      "\n",
      "[Human]: In the United States you can't, because the average millennial in the United States has no opportunity to save money. Either you get a college education, then you will be burdened with a student loan....\n",
      "\n",
      "[ChatGPT]: To determine your take-home pay if you claim 3 exemptions, you will need to calculate your federal income tax withholding and any other applicable deductions.First, you will need to determine your tax...\n",
      "Data prepared: 5506 training, 2360 testing.\n",
      "=== Training Dual Path Model ===\n",
      "1. Extracting features...\n",
      "Extracting statistical features...\n",
      "Processing texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing:   1%|▏         | 74/5506 [00:21<36:04,  2.51it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Integrating Dual Path Fusion Models into Existing AI Text Detection Processes\n",
    "\"\"\"\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "\n",
    "class EnhancedAIDetector:\n",
    "    \"\"\"\n",
    "    MODIFIED AI Detector: Integrates the dual-path model.\n",
    "    The __init__ method is updated to accept a model name for the dual-path detector.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: Optional[DetectionConfig] = None, dual_path_model_name: str = \"distilbert-base-uncased\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            config: Configuration for the base detector (feature extraction).\n",
    "            dual_path_model_name: The name of the transformer to use in the DualPathFusionModel.\n",
    "        \"\"\"\n",
    "        self.config = config or DetectionConfig()\n",
    "        self.base_detector = AITextDetector(self.config)\n",
    "        # Pass the dynamic model name to the dual-path detector\n",
    "        self.dual_path_detector = DualPathDetector(model_name=dual_path_model_name)\n",
    "        print(f\"Enhanced AI Detector initialized with Dual Path Model: {dual_path_model_name}\")\n",
    "\n",
    "    \n",
    "    async def extract_all_features(self, texts: List[str]) -> pd.DataFrame:\n",
    "        \n",
    "        print(\"Extracting statistical features...\")\n",
    "        results = await self.base_detector.detect_batch(texts)\n",
    "        features_array = results['features']\n",
    "     \n",
    "        all_possible_features = [\n",
    "            'semantic_similarity', 'word_overlap_ratio', 'vocabulary_change_ratio',\n",
    "            'unique_words_ratio', 'stopword_ratio_change', 'sentence_count_ratio',\n",
    "            'avg_sentence_length_change', 'pos_NN_ratio_change', 'pos_VB_ratio_change',\n",
    "            'pos_JJ_ratio_change', 'pos_RB_ratio_change', 'punctuation_ratio_change',\n",
    "            'capitalization_ratio_change', 'avg_word_length_change', 'char_level_similarity',\n",
    "            'word_level_similarity', 'length_ratio'\n",
    "        ]\n",
    "        core_features = all_possible_features[:13] \n",
    "        features_df = pd.DataFrame(features_array[:, :13], columns=core_features)\n",
    "        return features_df\n",
    "\n",
    "    async def train_dual_path_model(self, train_df: pd.DataFrame, num_epochs: int, batch_size: int, save_path: str):\n",
    "        print(\"=== Training Dual Path Model ===\")\n",
    "        texts = train_df[\"text\"].tolist()\n",
    "        labels = train_df[\"label\"].tolist()\n",
    "        print(\"1. Extracting features...\")\n",
    "        features_df = await self.extract_all_features(texts)\n",
    "        print(\"2. Preparing datasets...\")\n",
    "        train_dataset, val_dataset = self.dual_path_detector.prepare_data(texts, features_df, labels)\n",
    "        print(\"3. Training model...\")\n",
    "        self.dual_path_detector.train(train_dataset, val_dataset, num_epochs=num_epochs, batch_size=batch_size, save_path=save_path)\n",
    "        print(\"\\n Training completed!\")\n",
    "\n",
    "    async def detect_with_dual_path(self, texts: List[str]) -> Dict:\n",
    "        features_df = await self.extract_all_features(texts)\n",
    "        predictions, probabilities = self.dual_path_detector.predict(texts, features_df)\n",
    "        return {\"predictions\": predictions, \"probabilities\": probabilities, \"features\": features_df}\n",
    "\n",
    "def setup_logging(output_dir: str = \"./result\"):\n",
    "    \"\"\"\n",
    "    Sets up logging to save all print output to a file and also show it on the console.\n",
    "    \"\"\"\n",
    "    log_file_path = os.path.join(output_dir, 'output.log')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Get the root logger\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    # Clear existing handlers to avoid duplicate logs in interactive environments\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='w', encoding='utf-8')\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_formatter = logging.Formatter('%(message)s')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    logging.info(f\"Logging setup complete. All output will be saved to {log_file_path}\")\n",
    "\n",
    "def plot_roc_curves(results: Dict, output_dir: str = \"./result\"):\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        fpr, tpr, _ = roc_curve(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        auc = roc_auc_score(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        plt.plot(fpr, tpr, label=f\"{name} (AUC = {auc:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"roc_curves_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curves(results: Dict, output_dir: str = \"./result\"):\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for name, result_data in results.items():\n",
    "        precision, recall, _ = precision_recall_curve(result_data[\"test_labels\"], result_data[\"test_results\"][\"probabilities\"])\n",
    "        plt.plot(recall, precision, label=f\"{name}\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curves Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, \"precision_recall_curves_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrices(results: Dict, output_dir: str = \"./result\"):\n",
    "    \n",
    "    n_results = len(results)\n",
    "    fig, axes = plt.subplots(1, n_results, figsize=(6 * n_results, 5), squeeze=False)\n",
    "    for ax, (name, result_data) in zip(axes.flatten(), results.items()):\n",
    "        cm = confusion_matrix(result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"])\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax, cbar=False, xticklabels=[\"Human\", \"AI\"], yticklabels=[\"Human\", \"AI\"])\n",
    "        ax.set_title(f\"Confusion Matrix: {name}\")\n",
    "        ax.set_xlabel(\"Predicted Label\")\n",
    "        ax.set_ylabel(\"True Label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"confusion_matrices_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "def plot_classification_metrics(results: Dict, output_dir: str = \"./result\"):\n",
    "    \n",
    "    metrics_data = []\n",
    "    for name, result_data in results.items():\n",
    "        report = classification_report(result_data[\"test_labels\"], result_data[\"test_results\"][\"predictions\"], target_names=[\"Human\", \"AI\"], output_dict=True)\n",
    "        ai_metrics = report[\"AI\"]\n",
    "        metrics_data.append({\"Configuration\": name, \"Precision\": ai_metrics[\"precision\"], \"Recall\": ai_metrics[\"recall\"], \"F1-Score\": ai_metrics[\"f1-score\"]})\n",
    "    df = pd.DataFrame(metrics_data)\n",
    "    df_melted = df.melt(id_vars=\"Configuration\", var_name=\"Metric\", value_name=\"Score\")\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    sns.barplot(data=df_melted, x=\"Configuration\", y=\"Score\", hue=\"Metric\")\n",
    "    plt.title('Classification Metrics for \"AI\" Class')\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.savefig(os.path.join(output_dir, \"classification_metrics_comparison.png\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "async def main_workflow_for_comparison(\n",
    "    data_path: str,\n",
    "    max_samples: Optional[int],\n",
    "    use_cache: bool,\n",
    "    config: DetectionConfig,\n",
    "    dual_path_model_name: str,\n",
    "    save_path: str \n",
    "):\n",
    "    \"\"\"\n",
    "    MODIFIED Main Workflow: This function now uses logging instead of print.\n",
    "    \"\"\"\n",
    "    SEED = 42\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "    detector = EnhancedAIDetector(config=config, dual_path_model_name=dual_path_model_name)\n",
    "\n",
    "    cache_path = f\"enhanced_cache_{config.revision_model}.json\"\n",
    "    if use_cache and os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, \"r\") as f:\n",
    "                detector.base_detector.cache = json.load(f)\n",
    "            logging.info(f\"Successfully loaded cache with {len(detector.base_detector.cache)} entries from {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Could not load cache from {cache_path}: {e}\")\n",
    "    elif use_cache:\n",
    "        logging.info(f\"Cache file not found at {cache_path}. A new cache will be created.\")\n",
    "\n",
    "    loader = CustomDataLoader()\n",
    "    df_full = loader.load_data(data_path)\n",
    "    df = df_full.head(max_samples) if max_samples else df_full\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.3, random_state=SEED, stratify=df['label'])\n",
    "    logging.info(f\"Data prepared: {len(train_df)} training, {len(test_df)} testing.\")\n",
    "\n",
    "    await detector.train_dual_path_model(train_df=train_df, num_epochs=5, batch_size=16, save_path=save_path)\n",
    "\n",
    "    test_texts = test_df[\"text\"].tolist()\n",
    "    test_labels = test_df[\"label\"].tolist()\n",
    "    test_results = await detector.detect_with_dual_path(test_texts)\n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, test_results['predictions'])\n",
    "    auc = roc_auc_score(test_labels, test_results['probabilities'])\n",
    "    report_dict = classification_report(test_labels, test_results['predictions'], output_dict=True)\n",
    "    f1_score = report_dict['weighted avg']['f1-score']\n",
    "\n",
    "    logging.info(f\"\\n--- Test Results for {dual_path_model_name} ---\")\n",
    "    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "    logging.info(f\"AUC: {auc:.4f}\")\n",
    "    logging.info(f\"Weighted F1-Score: {f1_score:.4f}\")\n",
    "    logging.info(\"\\nClassification Report:\")\n",
    "    logging.info(f\"\\n{classification_report(test_labels, test_results['predictions'], target_names=['Human', 'AI'], digits=4)}\")\n",
    "\n",
    "    return {\"detector\": detector, \"test_results\": test_results, \"test_labels\": test_labels, \"accuracy\": accuracy, \"auc\": auc, \"f1_score\": f1_score}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def run_transformer_comparison(data_path: str = \"finance\", max_samples: Optional[int] = 1000):\n",
    "    \"\"\"\n",
    "    Runs a comparison experiment between different transformer models for both\n",
    "    feature extraction and deep feature representation.\n",
    "    \"\"\"\n",
    "    setup_logging() # Setup logging to file and console\n",
    "    logging.info(\"=== Transformer Model Comparison Experiment ===\")\n",
    "    results = {}\n",
    "\n",
    "    # Define the combinations of models you want to test\n",
    "    configurations = {\n",
    "        # \"BERT\": {\n",
    "        #     \"embedding_model\": \"bert-base-uncased\",\n",
    "        #     \"transformer_name\": \"bert-base-uncased\"\n",
    "        # },\n",
    "        # \"DistilBERT\": {\n",
    "        #     \"embedding_model\": \"distilbert-base-uncased\",\n",
    "        #     \"transformer_name\": \"distilbert-base-uncased\"\n",
    "        # },\n",
    "        # \"fine-tune model\": {\n",
    "        #     \"embedding_model\": \"roberta-base\",\n",
    "        #     \"transformer_name\": \"roberta-base\"\n",
    "        # },\n",
    "        # \"RoBERTa\":{\n",
    "        #     \"embedding_model\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "        #     \"transformer_name\": \"sentence-transformers/paraphrase-MiniLM-L6-v2\",\n",
    "        # },\n",
    "        # \"RoBERTa\":{\n",
    "        #     \"embedding_model\": \"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "        #     \"transformer_name\": \"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "        # },\n",
    "        # \"T5\": {\n",
    "        #     \"embedding_model\": \"t5-small\",\n",
    "        #     \"transformer_name\": \"t5-small\"\n",
    "        # },\n",
    "        # \"GPT-2\": {\n",
    "        #     \"embedding_model\": \"gpt2\",\n",
    "        #     \"transformer_name\": \"gpt2\"\n",
    "        # },\n",
    "        \"fine-tune model\": {\n",
    "            \"embedding_model\": \"./models/paraphrase-MiniLM-L6-v2-ai-detector-incomplete\",\n",
    "            \"transformer_name\": \"roberta-base\"\n",
    "        },\n",
    "        \n",
    "    }\n",
    "\n",
    "   \n",
    "    for name, model_config in configurations.items():\n",
    "        logging.info(f\"\\n{'='*25} Running Experiment: {name} {'='*25}\")\n",
    "        try:\n",
    "            model_save_path = f\"best_model_{name.replace(' ', '_').replace('/', '-')}.pt\"\n",
    "\n",
    "            detection_config = DetectionConfig(\n",
    "                revision_model=\"gpt-3.5-turbo\",\n",
    "                embedding_model=model_config[\"embedding_model\"],\n",
    "                perturbation_rate=0.15,\n",
    "                use_ml_classifier=True,\n",
    "            )\n",
    "            \n",
    "            results[name] = await main_workflow_for_comparison(\n",
    "                data_path=data_path,\n",
    "                max_samples=max_samples,\n",
    "                use_cache=True,\n",
    "                config=detection_config,\n",
    "                dual_path_model_name=model_config[\"transformer_name\"],\n",
    "                save_path=model_save_path \n",
    "            )\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            logging.error(f\"\\n!!!!!! Experiment '{name}' FAILED: {e} !!!!!!\")\n",
    "            logging.error(traceback.format_exc())\n",
    "\n",
    "    logging.info(\"\\n\\n\" + \"=\"*25 + \" Final Comparison Summary \" + \"=\"*25)\n",
    "    summary_df = pd.DataFrame([\n",
    "        {\"Configuration\": name, \"Accuracy\": res[\"accuracy\"], \"AUC\": res[\"auc\"], \"F1-Score\": res[\"f1_score\"]}\n",
    "        for name, res in results.items()\n",
    "    ])\n",
    "    summary_df['Accuracy'] = summary_df['Accuracy'].map('{:.4f}'.format)\n",
    "    summary_df['AUC'] = summary_df['AUC'].map('{:.4f}'.format)\n",
    "    summary_df['F1-Score'] = summary_df['F1-Score'].map('{:.4f}'.format)\n",
    "    logging.info(f\"\\n{summary_df.to_string(index=False)}\")\n",
    "\n",
    "    logging.info(\"\\n\\nGenerating visualizations...\")\n",
    "    if results:\n",
    "        plot_roc_curves(results)\n",
    "        plot_precision_recall_curves(results)\n",
    "        plot_confusion_matrices(results)\n",
    "        plot_classification_metrics(results)\n",
    "    else:\n",
    "        logging.warning(\"No successful experiments were completed to visualize.\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    await run_transformer_comparison(data_path=\"test\", max_samples=None)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7745407,
     "sourceId": 12289600,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
